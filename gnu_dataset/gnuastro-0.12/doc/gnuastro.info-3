This is gnuastro.info, produced by makeinfo version 6.7 from
gnuastro.texi.

This book documents version 0.12 of the GNU Astronomy Utilities
(Gnuastro).  Gnuastro provides various programs and libraries for
astronomical data manipulation and analysis.

   Copyright © 2015-2020, Free Software Foundation, Inc.

     Permission is granted to copy, distribute and/or modify this
     document under the terms of the GNU Free Documentation License,
     Version 1.3 or any later version published by the Free Software
     Foundation; with no Invariant Sections, no Front-Cover Texts, and
     no Back-Cover Texts.  A copy of the license is included in the
     section entitled “GNU Free Documentation License”.
INFO-DIR-SECTION Astronomy
START-INFO-DIR-ENTRY
* Gnuastro: (gnuastro).       GNU Astronomy Utilities.
* libgnuastro: (gnuastro)Gnuastro library. Full Gnuastro library doc.

* help-gnuastro: (gnuastro)help-gnuastro mailing list. Getting help.

* bug-gnuastro: (gnuastro)Report a bug. How to report bugs

* Arithmetic: (gnuastro)Arithmetic. Arithmetic operations on pixels.
* astarithmetic: (gnuastro)Invoking astarithmetic. Options to Arithmetic.

* BuildProgram: (gnuastro)BuildProgram. Compile and run programs using Gnuastro’s library.
* astbuildprog: (gnuastro)Invoking astbuildprog. Options to BuildProgram.

* ConvertType: (gnuastro)ConvertType. Convert different file types.
* astconvertt: (gnuastro)Invoking astconvertt. Options to ConvertType.

* Convolve: (gnuastro)Convolve. Convolve an input file with kernel.
* astconvolve: (gnuastro)Invoking astconvolve. Options to Convolve.

* CosmicCalculator: (gnuastro)CosmicCalculator. For cosmological params.
* astcosmiccal: (gnuastro)Invoking astcosmiccal. Options to CosmicCalculator.

* Crop: (gnuastro)Crop. Crop region(s) from image(s).
* astcrop: (gnuastro)Invoking astcrop. Options to Crop.

* Fits: (gnuastro)Fits. View and manipulate FITS extensions and keywords.
* astfits: (gnuastro)Invoking astfits. Options to Fits.

* MakeCatalog: (gnuastro)MakeCatalog. Make a catalog from labeled image.
* astmkcatalog: (gnuastro)Invoking astmkcatalog. Options to MakeCatalog.

* MakeNoise: (gnuastro)MakeNoise. Make (add) noise to an image.
* astmknoise: (gnuastro)Invoking astmknoise. Options to MakeNoise.

* MakeProfiles: (gnuastro)MakeProfiles. Make mock profiles.
* astmkprof: (gnuastro)Invoking astmkprof. Options to MakeProfiles.

* Match: (gnuastro)Match. Match two separate catalogs.
* astmatch: (gnuastro)Invoking astmatch. Options to Match.

* NoiseChisel: (gnuastro)NoiseChisel. Detect signal in noise.
* astnoisechisel: (gnuastro)Invoking astnoisechisel. Options to NoiseChisel.

* Segment: (gnuastro)Segment. Segment detections based on signal structure.
* astsegment: (gnuastro)Invoking astsegment. Options to Segment.

* Statistics: (gnuastro)Statistics. Get image Statistics.
* aststatistics: (gnuastro)Invoking aststatistics. Options to Statistics.

* Table: (gnuastro)Table. Read and write FITS binary or ASCII tables.
* asttable: (gnuastro)Invoking asttable. Options to Table.

* Warp: (gnuastro)Warp. Warp a dataset to a new grid.
* astwarp: (gnuastro)Invoking astwarp. Options to Warp.

* astscript-sort-by-night: (gnuastro)Invoking astscript-sort-by-night. Options to this script

END-INFO-DIR-ENTRY


File: gnuastro.info,  Node: Blank pixels,  Next: Invoking astcrop,  Prev: Crop section syntax,  Up: Crop

6.1.3 Blank pixels
------------------

The cropped box can potentially include pixels that are beyond the image
range.  For example when a target in the input catalog was very near the
edge of the input image.  The parts of the cropped image that were not
in the input image will be filled with the following two values
depending on the data type of the image.  In both cases, SAO ds9 will
not color code those pixels.
   • If the data type of the image is a floating point type (float or
     double), IEEE NaN (Not a number) will be used.
   • For integer types, pixels out of the image will be filled with the
     value of the ‘BLANK’ keyword in the cropped image header.  The
     value assigned to it is the lowest value possible for that type, so
     you will probably never need it any way.  Only for the unsigned
     character type (‘BITPIX=8’ in the FITS header), the maximum value
     is used because it is unsigned, the smallest value is zero which is
     often meaningful.
   You can ask for such blank regions to not be included in the output
crop image using the ‘--noblank’ option.  In such cases, there is no
guarantee that the image size of your outputs are what you asked for.

   In some survey images, unfortunately they do not use the ‘BLANK’ FITS
keyword.  Instead they just give all pixels outside of the survey area a
value of zero.  So by default, when dealing with float or double image
types, any values that are 0.0 are also regarded as blank regions.  This
can be turned off with the ‘--zeroisnotblank’ option.


File: gnuastro.info,  Node: Invoking astcrop,  Prev: Blank pixels,  Up: Crop

6.1.4 Invoking Crop
-------------------

Crop will crop a region from an image.  If in WCS mode, it will also
stitch parts from separate images in the input files.  The executable
name is ‘astcrop’ with the following general template

     $ astcrop [OPTION...] [ASCIIcatalog] ASTRdata ...

One line examples:

     ## Crop all objects in cat.txt from image.fits:
     $ astcrop --catalog=cat.txt image.fits

     ## Crop all options in catalog (with RA,DEC) from all the files
     ## ending in `_drz.fits' in `/mnt/data/COSMOS/':
     $ astcrop --mode=wcs --catalog=cat.txt /mnt/data/COSMOS/*_drz.fits

     ## Crop the outer 10 border pixels of the input image:
     $ astcrop --section=10:*-10,10:*-10 --hdu=2 image.fits

     ## Crop region around RA and Dec of (189.16704, 62.218203):
     $ astcrop --mode=wcs --center=189.16704,62.218203 goodsnorth.fits

     ## Crop region around pixel coordinate (568.342, 2091.719):
     $ astcrop --mode=img --center=568.342,2091.719 --width=201 image.fits

Crop has one mandatory argument which is the input image name(s), shown
above with ‘ASTRdata ...’.  You can use shell expansions, for example
‘*’ for this if you have lots of images in WCS mode.  If the crop box
centers are in a catalog, you can use the ‘--catalog’ option.  In other
cases, you have to provide the single cropped output parameters must be
given with command-line options.  See *note Crop output:: for how the
output file name(s) can be specified.  For the full list of general
options to all Gnuastro programs (including Crop), please see *note
Common options::.

   Floating point numbers can be used to specify the crop region (except
the ‘--section’ option, see *note Crop section syntax::).  In such
cases, the floating point values will be used to find the desired
integer pixel indices based on the FITS standard.  Hence, Crop
ultimately doesn’t do any sub-pixel cropping (in other words, it doesn’t
change pixel values).  If you need such crops, you can use *note Warp::
to first warp the image to the a new pixel grid, then crop from that.
For example, let’s assume you want a crop from pixels 12.982 to 80.982
along the first dimension.  You should first translate the image by
$-0.482$ (note that the edge of a pixel is at integer multiples of
$0.5$).  So you should run Warp with ‘--translate=-0.482,0’ and then
crop the warped image with ‘--section=13:81’.

   There are two ways to define the cropped region: with its center or
its vertices.  See *note Crop modes:: for a full description.  In the
former case, Crop can check if the central region of the cropped image
is indeed filled with data or is blank (see *note Blank pixels::), and
not produce any output when the center is blank, see the description
under ‘--checkcenter’ for more.

   When in catalog mode, Crop will run in parallel unless you set
‘--numthreads=1’, see *note Multi-threaded operations::.  Note that when
multiple outputs are created with threads, the outputs will not be
created in the same order.  This is because the threads are asynchronous
and thus not started in order.  This has no effect on each output, see
*note Finding reddest clumps and visual inspection:: for a tutorial on
effectively using this feature.

* Menu:

* Crop options::                A list of all the options with explanation.
* Crop output::                 The outputs of Crop.


File: gnuastro.info,  Node: Crop options,  Next: Crop output,  Prev: Invoking astcrop,  Up: Invoking astcrop

6.1.4.1 Crop options
....................

The options can be classified into the following contexts: Input, Output
and operating mode options.  Options that are common to all Gnuastro
program are listed in *note Common options:: and will not be repeated
here.

   When you are specifying the crop vertices your self (through
‘--section’, or ‘--polygon’) on relatively small regions (depending on
the resolution of your images) the outputs from image and WCS mode can
be approximately equivalent.  However, as the crop sizes get large, the
curved nature of the WCS coordinates have to be considered.  For
example, when using ‘--section’, the right ascension of the bottom left
and top left corners will not be equal.  If you only want regions within
a given right ascension, use ‘--polygon’ in WCS mode.

Input image parameters:

‘--hstartwcs=INT’
     Specify the first keyword card (line number) to start finding the
     input image world coordinate system information.  Distortions were
     only recently included in WCSLIB (from version 5).  Therefore until
     now, different telescope would apply their own specific set of WCS
     keywords and put them into the image header along with those that
     WCSLIB does recognize.  So now that WCSLIB recognizes most of the
     standard distortion parameters, they will get confused with the old
     ones and give completely wrong results.  For example in the
     CANDELS-GOODS South images(1).

     The two ‘--hstartwcs’ and ‘--hendwcs’ are thus provided so when
     using older datasets, you can specify what region in the FITS
     headers you want to use to read the WCS keywords.  Note that this
     is only relevant for reading the WCS information, basic data
     information like the image size are read separately.  These two
     options will only be considered when the value to ‘--hendwcs’ is
     larger than that of ‘--hstartwcs’.  So if they are equal or
     ‘--hstartwcs’ is larger than ‘--hendwcs’, then all the input
     keywords will be parsed to get the WCS information of the image.

‘--hendwcs=INT’
     Specify the last keyword card to read for specifying the image
     world coordinate system on the input images.  See ‘--hstartwcs’

Crop box parameters:

‘-c FLT[,FLT[,...]]’
‘--center=FLT[,FLT[,...]]’
     The central position of the crop in the input image.  The positions
     along each dimension must be separated by a comma (<,>) and
     fractions are also acceptable.  The number of values given to this
     option must be the same as the dimensions of the input dataset.
     The width of the crop should be set with ‘--width’.  The units of
     the coordinates are read based on the value to the ‘--mode’ option,
     see below.

‘-w FLT[,FLT[,...]]’
‘--width=FLT[,FLT[,...]]’
     Width of the cropped region about its center.  ‘--width’ may take
     either a single value (to be used for all dimensions) or multiple
     values (a specific value for each dimension).  If in WCS mode,
     value(s) given to this option will be read in the same units as the
     dataset’s WCS information along this dimension.  The final output
     will have an odd number of pixels to allow easy identification of
     the pixel which keeps your requested coordinate (from ‘--center’ or
     ‘--catalog’).

     The ‘--width’ option also accepts fractions.  For example if you
     want the width of your crop to be 3 by 5 arcseconds along RA and
     Dec respectively, you can call it with: ‘--width=3/3600,5/3600’.

     If you want an even sided crop, you can run Crop afterwards with
     ‘--section=":*-1,:*-1"’ or ‘--section=2:,2:’ (depending on which
     side you don’t need), see *note Crop section syntax::.

‘-l STR’
‘--polygon=STR’
     String of vertices to define a polygon to crop.  The vertices are
     used to define the polygon in the same order given to this option.
     When the vertices are not necessarily ordered in the proper order
     (for example one vertice in a square comes after its diagonal
     opposite), you can add the ‘--polygonsort’ option which will
     attempt to sort the vertices before cropping.  Note that for
     concave polygons, sorting is not recommended because there is no
     unique solution, for more, see the description under
     ‘--polygonsort’.

     This option can be used both in the image and WCS modes, see *note
     Crop modes::.  The cropped image will be the size of the
     rectangular region that completely encompasses the polygon.  By
     default all the pixels that are outside of the polygon will be set
     as blank values (see *note Blank pixels::).  However, if
     ‘--polygonout’ is called all pixels internal to the vertices will
     be set to blank.  In WCS-mode, you may provide many FITS
     images/tiles: Crop will stitch them to produce this cropped region,
     then apply the polygon.

     The syntax for the polygon vertices is similar to, and simpler
     than, that for ‘--section’.  In short, the dimensions of each
     coordinate are separated by a comma (<,>) and each vertex is
     separated by a colon (<:>).  You can define as many vertices as you
     like.  If you would like to use space characters between the
     dimensions and vertices to make them more human-readable, then you
     have to put the value to this option in double quotation marks.

     For example, let’s assume you want to work on the deepest part of
     the WFC3/IR images of Hubble Space Telescope eXtreme Deep Field
     (HST-XDF). According to the webpage
     (https://archive.stsci.edu/prepds/xdf/)(2) the deepest part is
     contained within the coordinates:

          [ (53.187414,-27.779152), (53.159507,-27.759633),
            (53.134517,-27.787144), (53.161906,-27.807208) ]

     They have provided mask images with only these pixels in the
     WFC3/IR images, but what if you also need to work on the same
     region in the full resolution ACS images?  Also what if you want to
     use the CANDELS data for the shallow region?  Running Crop with
     ‘--polygon’ will easily pull out this region of the image for you,
     irrespective of the resolution.  If you have set the operating mode
     to WCS mode in your nearest configuration file (see *note
     Configuration files::), there is no need to call ‘--mode=wcs’ on
     the command line.

          $ astcrop --mode=wcs desired-filter-image(s).fits           \
             --polygon="53.187414,-27.779152 : 53.159507,-27.759633 : \
                        53.134517,-27.787144 : 53.161906,-27.807208"
     In other cases, you have an image and want to define the polygon
     yourself (it isn’t already published like the example above).  As
     the number of vertices increases, checking the vertex coordinates
     on a FITS viewer (for example SAO ds9) and typing them in one by
     one can be very tedious and prone to typo errors.

     You can take the following steps to avoid the frustration and
     possible typos: Open the image with ds9 and activate its “region”
     mode with Edit→Region.  Then define the region as a polygon with
     Region→Shape→Polygon.  Click on the approximate center of the
     region you want and a small square will appear.  By clicking on the
     vertices of the square you can shrink or expand it, clicking and
     dragging anywhere on the edges will enable you to define a new
     vertex.  After the region has been nicely defined, save it as a
     file with Region→Save Regions.  You can then select the name and
     address of the output file, keep the format as ‘REG’ and press
     “OK”.  In the next window, keep format as “ds9” and “Coordinate
     System” as “fk5”.  A plain text file (let’s call it ‘ds9.reg’) is
     now created.

     You can now convert this plain text file to Crop’s polygon format
     with this command (when typing on the command-line, ignore the
     “<\>” at the end of the first and second lines along with the extra
     spaces, these are only for nice printing):

          $ v=$(awk 'NR==4' ds9.reg | sed -e's/polygon(//'        \
                     -e's/\([^,]*,[^,]*\),/\1:/g' -e's/)//' )
          $ astcrop --mode=wcs image.fits --polygon=$v

‘--polygonout’
     Keep all the regions outside the polygon and mask the inner ones
     with blank pixels (see *note Blank pixels::).  This is practically
     the inverse of the default mode of treating polygons.  Note that
     this option only works when you have only provided one input image.
     If multiple images are given (in WCS mode), then the full area
     covered by all the images has to be shown and the polygon excluded.
     This can lead to a very large area if large surveys like COSMOS are
     used.  So Crop will abort and notify you.  In such cases, it is
     best to crop out the larger region you want, then mask the smaller
     region with this option.

‘--polygonsort’
     Sort the given set of vertices to the ‘--polygon’ option.  For a
     concave polygon it will sort the vertices correctly, however for a
     convex polygon it there is no unique sorting, so be careful because
     the crop may not be what you expected.

     Polygons come in two classes: convex and concave (or generally,
     non-convex!), see below for a demonstration.  Convex polygons are
     those where all inner angles are less than 180 degrees.  By
     contrast, a convex polygon is one where an inner angle may be more
     than 180 degrees.

                      Concave Polygon        Convex Polygon

                       D --------C          D------------- C
                        \        |        E /              |
                         \E      |          \              |
                         /       |           \             |
                        A--------B             A ----------B

‘-s STR’
‘--section=STR’
     Section of the input image which you want to be cropped.  See *note
     Crop section syntax:: for a complete explanation on the syntax
     required for this input.

‘-x STR/INT’
‘--coordcol=STR/INT’
     The column in a catalog to read as a coordinate.  The value can be
     either the column number (starting from 1), or a match/search in
     the table meta-data, see *note Selecting table columns::.  This
     option must be called multiple times, depending on the number of
     dimensions in the input dataset.  If it is called more than
     necessary, the extra columns (later calls to this option on the
     command-line or configuration files) will be ignored, see *note
     Configuration file precedence::.

‘-n STR/INT’
‘--namecol=STR/INT’
     Column selection of crop file name.  The value can be either the
     column number (starting from 1), or a match/search in the table
     meta-data, see *note Selecting table columns::.  This option can be
     used both in Image and WCS modes, and not a mandatory.  When a
     column is given to this option, the final crop base file name will
     be taken from the contents of this column.  The directory will be
     determined by the ‘--output’ option (current directory if not
     given) and the value to ‘--suffix’ will be appended.  When this
     column isn’t given, the row number will be used instead.

Output options:

‘-c FLT/INT’
‘--checkcenter=FLT/INT’
     Square box width of region in the center of the image to check for
     blank values.  If any of the pixels in this central region of a
     crop (defined by its center) are blank, then it will not be stored
     in an output file.  If the value to this option is zero, no
     checking is done.  This check is only applied when the cropped
     region(s) are defined by their center (not by the vertices, see
     *note Crop modes::).

     The units of the value are interpreted based on the ‘--mode’ value
     (in WCS or pixel units).  The ultimate checked region size (in
     pixels) will be an odd integer around the center (converted from
     WCS, or when an even number of pixels are given to this option).
     In WCS mode, the value can be given as fractions, for example if
     the WCS units are in degrees, ‘0.1/3600’ will correspond to a check
     size of 0.1 arcseconds.

     Because survey regions don’t often have a clean square or rectangle
     shape, some of the pixels on the sides of the survey FITS image
     don’t commonly have any data and are blank (see *note Blank
     pixels::).  So when the catalog was not generated from the input
     image, it often happens that the image does not have data over some
     of the points.

     When the given center of a crop falls in such regions or outside
     the dataset, and this option has a non-zero value, no crop will be
     created.  Therefore with this option, you can specify a width of a
     small box (3 pixels is often good enough) around the central pixel
     of the cropped image.  You can check which crops were created and
     which weren’t from the command-line (if ‘--quiet’ was not called,
     see *note Operating mode options::), or in Crop’s log file (see
     *note Crop output::).

‘-p STR’
‘--suffix=STR’
     The suffix (or post-fix) of the output files for when you want all
     the cropped images to have a special ending.  One case where this
     might be helpful is when besides the science images, you want the
     weight images (or exposure maps, which are also distributed with
     survey images) of the cropped regions too.  So in one run, you can
     set the input images to the science images and ‘--suffix=_s.fits’.
     In the next run you can set the weight images as input and
     ‘--suffix=_w.fits’.

‘-b’
‘--noblank’
     Pixels outside of the input image that are in the crop box will not
     be used.  By default they are filled with blank values (depending
     on type), see *note Blank pixels::.  This option only applies only
     in Image mode, see *note Crop modes::.

‘-z’
‘--zeroisnotblank’
     In float or double images, it is common to give the value of zero
     to blank pixels.  If the input image type is one of these two
     types, such pixels will also be considered as blank.  You can
     disable this behavior with this option, see *note Blank pixels::.

Operating mode options:

‘-O STR’
‘--mode=STR’
     Operate in Image mode or WCS mode when the input coordinates can be
     both image or WCS. The value must either be ‘img’ or ‘wcs’, see
     *note Crop modes:: for a full description.

   ---------- Footnotes ----------

   (1) <https://archive.stsci.edu/pub/hlsp/candels/goods-s/gs-tot/v1.0/>

   (2) <https://archive.stsci.edu/prepds/xdf/>


File: gnuastro.info,  Node: Crop output,  Prev: Crop options,  Up: Invoking astcrop

6.1.4.2 Crop output
...................

The string given to ‘--output’ option will be interpreted depending on
how many crops were requested, see *note Crop modes:::

   • When a catalog is given, the value of the ‘--output’ (see *note
     Common options::) will be read as the directory to store the output
     cropped images.  Hence if it doesn’t already exist, Crop will abort
     with an error of a “No such file or directory” error.

     The crop file names will consist of two parts: a variable part (the
     row number of each target starting from 1) along with a fixed
     string which you can set with the ‘--suffix’ option.  Optionally,
     you may also use the ‘--namecol’ option to define a column in the
     input catalog to use as the file name instead of numbers.

   • When only one crop is desired, the value to ‘--output’ will be read
     as a file name.  If no output is specified or if it is a directory,
     the output file name will follow the automatic output names of
     Gnuastro, see *note Automatic output::: The string given to
     ‘--suffix’ will be replaced with the ‘.fits’ suffix of the input.

   The header of each output cropped image will contain the names of the
input image(s) it was cut from.  If a name is longer than the 70
character space that the FITS standard allows for header keyword values,
the name will be cut into several keywords from the nearest slash (</>).
The keywords have the following format: ‘ICFn_m’ (for Crop File).  Where
‘n’ is the number of the image used in this crop and ‘m’ is the part of
the name (it can be broken into multiple keywords).  Following the name
is another keyword named ‘ICFnPIX’ which shows the pixel range from that
input image in the same syntax as *note Crop section syntax::.  So this
string can be directly given to the ‘--section’ option later.

   Once done, a log file can be created in the current directory with
the ‘--log’ option.  This file will have three columns and the same
number of rows as the number of cropped images.  There are also comments
on the top of the log file explaining basic information about the run
and descriptions for the columns.  A short description of the columns is
also given below:

  1. The cropped image file name for that row.
  2. The number of input images that were used to create that image.
  3. A ‘0’ if the central few pixels (value to the ‘--checkcenter’
     option) are blank and ‘1’ if they aren’t.  When the crop was not
     defined by its center (see *note Crop modes::), or ‘--checkcenter’
     was given a value of 0 (see *note Invoking astcrop::), the center
     will not be checked and this column will be given a value of ‘-1’.


File: gnuastro.info,  Node: Arithmetic,  Next: Convolve,  Prev: Crop,  Up: Data manipulation

6.2 Arithmetic
==============

It is commonly necessary to do operations on some or all of the elements
of a dataset independently (pixels in an image).  For example, in the
reduction of raw data it is necessary to subtract the Sky value (*note
Sky value::) from each image image.  Later (once the images as warped
into a single grid using Warp for example, see *note Warp::), the images
are co-added (the output pixel grid is the average of the pixels of the
individual input images).  Arithmetic is Gnuastro’s program for such
operations on your datasets directly from the command-line.  It
currently uses the reverse polish or post-fix notation, see *note
Reverse polish notation:: and will work on the native data types of the
input images/data to reduce CPU and RAM resources, see *note Numeric
data types::.  For more information on how to run Arithmetic, please see
*note Invoking astarithmetic::.

* Menu:

* Reverse polish notation::     The current notation style for Arithmetic
* Arithmetic operators::        List of operators known to Arithmetic
* Invoking astarithmetic::      How to run Arithmetic: options and output


File: gnuastro.info,  Node: Reverse polish notation,  Next: Arithmetic operators,  Prev: Arithmetic,  Up: Arithmetic

6.2.1 Reverse polish notation
-----------------------------

The most common notation for arithmetic operations is the infix notation
(https://en.wikipedia.org/wiki/Infix_notation) where the operator goes
between the two operands, for example $4+5$.  While the infix notation
is the preferred way in most programming languages, currently the
Gnuastro’s program (in particular Arithmetic and Table, when doing
column arithmetic) do not use it.  This is because it will require
parenthesis which can complicate the implementation of the code.  In the
near future we do plan to also allow this notation(1), but for the time
being (due to time constraints on the developers), arithmetic operations
can only be done in the post-fix notation (also known as reverse polish
notation (https://en.wikipedia.org/wiki/Reverse_Polish_notation)).  The
Wikipedia article provides some excellent explanation on this notation
but here we will give a short summary here for self-sufficiency.

   In the post-fix notation, the operator is placed after the operands,
as we will see below this removes the need to define parenthesis for
most ordinary operators.  For example, instead of writing ‘5+6’, we
write ‘5 6 +’.  To easily understand how this notation works, you can
think of each operand as a node in a “last-in-first-out” stack.  Every
time an operator is confronted, the operator pops the number of operands
it needs from the top of the stack (so they don’t exist in the stack any
more), does its operation and pushes the result back on top of the
stack.  So if you want the average of 5 and 6, you would write: ‘5 6 + 2
/’.  The operations that are done are:

  1. ‘5’ is an operand, so it is pushed to the top of the stack (which
     is initially empty).
  2. ‘6’ is an operand, so it is pushed to the top of the stack.
  3. ‘+’ is a _binary_ operator, so it will pop the top two elements of
     the stack out of it, and perform addition on them (the order is
     $5+6$ in the example above).  The result is ‘11’ which is pushed to
     the top of the stack.
  4. ‘2’ is an operand so push it onto the top of the stack.
  5. ‘/’ is a binary operator, so pull out the top two elements of the
     stack (top-most is ‘2’, then ‘11’) and divide the second one by the
     first.

   In the Arithmetic program, the operands can be FITS images or numbers
(see *note Invoking astarithmetic::).  In Table’s column arithmetic,
they can be any column or a number (see *note Column arithmetic::).

   With this notation, very complicated procedures can be created
without the need for parenthesis or worrying about precedence.  Even
functions which take an arbitrary number of arguments can be defined in
this notation.  This is a very powerful notation and is used in
languages like Postscript (2) which produces PDF files when compiled.

   ---------- Footnotes ----------

   (1) <https://savannah.gnu.org/task/index.php?13867>

   (2) See the EPS and PDF part of *note Recognized file formats:: for a
little more on the Postscript language.


File: gnuastro.info,  Node: Arithmetic operators,  Next: Invoking astarithmetic,  Prev: Reverse polish notation,  Up: Arithmetic

6.2.2 Arithmetic operators
--------------------------

The recognized operators in Arithmetic are listed below.  See *note
Reverse polish notation:: for more on how the operators and operands
should be ordered on the command-line.  The operands to all operators
can be a data array (for example a FITS image) or a number, the output
will be an array or number according to the inputs.  For example a
number multiplied by an array will produce an array.  The conditional
operators will return pixel, or numerical values of 0 (false) or 1
(true) and stored in an ‘unsigned char’ data type (see *note Numeric
data types::).

‘+’
     Addition, so “‘4 5 +’” is equivalent to $4+5$.

‘-’
     Subtraction, so “‘4 5 -’” is equivalent to $4-5$.

‘x’
     Multiplication, so “‘4 5 x’” is equivalent to $4\times5$.

‘/’
     Division, so “‘4 5 /’” is equivalent to $4/5$.

‘%’
     Modulo (remainder), so “‘3 2 %’” is equivalent to $1$.  Note that
     the modulo operator only works on integer types.

‘abs’
     Absolute value of first operand, so “‘4 abs’” is equivalent to
     $|4|$.

‘pow’
     First operand to the power of the second, so “‘4.3 5f pow’” is
     equivalent to $4.3^{5}$.  Currently ‘pow’ will only work on single
     or double precision floating point numbers or images.  To be sure
     that a number is read as a floating point (even if it doesn’t have
     any non-zero decimals) put an ‘f’ after it.

‘sqrt’
     The square root of the first operand, so “‘5 sqrt’” is equivalent
     to $\sqrt{5}$.  The output will have a floating point type, but its
     precision is determined from the input: if the input is a 64-bit
     floating point, the output will also be 64-bit.  Otherwise, the
     output will be 32-bit floating point (see *note Numeric data
     types:: for the respective precision).  Therefore if you require
     64-bit precision in estimating the square root, convert the input
     to 64-bit floating point first, for example with ‘5 float64 sqrt’.

‘log’
     Natural logarithm of first operand, so “‘4 log’” is equivalent to
     $ln(4)$.  The output type is determined from the input, see the
     explanation under ‘sqrt’ for more.

‘log10’
     Base-10 logarithm of first operand, so “‘4 log10’” is equivalent to
     $\log(4)$.  The output type is determined from the input, see the
     explanation under ‘sqrt’ for more.

‘minvalue’
     Minimum (non-blank) value in the top operand on the stack, so
     “‘a.fits minvalue’” will push the minimum pixel value in this image
     onto the stack.  Therefore this operator is mainly intended for
     data (for example images), if the top operand is a number, this
     operator just returns it without any change.  So note that when
     this operator acts on a single image, the output will no longer be
     an image, but a number.  The output of this operand is in the same
     type as the input.

‘maxvalue’
     Maximum (non-blank) value of first operand in the same type,
     similar to ‘minvalue’.

‘numbervalue’
     Number of non-blank elements in first operand in the ‘uint64’ type,
     similar to ‘minvalue’.

‘sumvalue’
     Sum of non-blank elements in first operand in the ‘float32’ type,
     similar to ‘minvalue’.

‘meanvalue’
     Mean value of non-blank elements in first operand in the ‘float32’
     type, similar to ‘minvalue’.

‘stdvalue’
     Standard deviation of non-blank elements in first operand in the
     ‘float32’ type, similar to ‘minvalue’.

‘medianvalue’
     Median of non-blank elements in first operand with the same type,
     similar to ‘minvalue’.

‘min’
     For each pixel, find the minimum value in all given datasets.  The
     output will have the same type as the input.

     The first popped operand to this operator must be a positive
     integer number which specifies how many further operands should be
     popped from the stack.  All the subsequently popped operands must
     have the same type and size.  This operator (and all the
     variable-operand operators similar to it that are discussed below)
     will work in multi-threaded mode unless Arithmetic is called with
     the ‘--numthreads=1’ option, see *note Multi-threaded operations::.

     Each pixel of the output of the ‘min’ operator will be given the
     minimum value of the same pixel from all the popped
     operands/images.  For example the following command will produce an
     image with the same size and type as the three inputs, but each
     output pixel value will be the minimum of the same pixel’s values
     in all three input images.

          $ astarithmetic a.fits b.fits c.fits 3 min

     Important notes:

        • NaN/blank pixels will be ignored, see *note Blank pixels::.

        • The output will have the same type as the inputs.  This is
          natural for the ‘min’ and ‘max’ operators, but for other
          similar operators (for example ‘sum’, or ‘average’) the
          per-pixel operations will be done in double precision floating
          point and then stored back in the input type.  Therefore, if
          the input was an integer, C’s internal type conversion will be
          used.

‘max’
     For each pixel, find the maximum value in all given datasets.  The
     output will have the same type as the input.  This operator is
     called similar to the ‘min’ operator, please see there for more.

‘number’
     For each pixel count the number of non-blank pixels in all given
     datasets.  The output will be an unsigned 32-bit integer datatype
     (see *note Numeric data types::).  This operator is called similar
     to the ‘min’ operator, please see there for more.

‘sum’
     For each pixel, calculate the sum in all given datasets.  The
     output will have the a single-precision (32-bit) floating point
     type.  This operator is called similar to the ‘min’ operator,
     please see there for more.

‘mean’
     For each pixel, calculate the mean in all given datasets.  The
     output will have the a single-precision (32-bit) floating point
     type.  This operator is called similar to the ‘min’ operator,
     please see there for more.

‘std’
     For each pixel, find the standard deviation in all given datasets.
     The output will have the a single-precision (32-bit) floating point
     type.  This operator is called similar to the ‘min’ operator,
     please see there for more.

‘median’
     For each pixel, find the median in all given datasets.  The output
     will have the a single-precision (32-bit) floating point type.
     This operator is called similar to the ‘min’ operator, please see
     there for more.

‘quantile’
     For each pixel, find the quantile from all given datasets.  The
     output will have the same numeric data type and size as the input
     datasets.  Besides the input datasets, the quantile operator also
     needs a single parameter (the requested quantile).  The parameter
     should be the first popped operand, with a value between (and
     including) 0 and 1.  The second popped operand must be the number
     of datasets to use.

     In the example below, the first-popped operand (‘0.7’) is the
     quantile, the second-popped operand (‘3’) is the number of datasets
     to pop.

          astarithmetic a.fits b.fits c.fits 3 0.7 quantile

‘sigclip-number’
     For each pixel, find the sigma-clipped number (after removing
     outliers) in all given datasets.  The output will have the an
     unsigned 32-bit integer type (see *note Numeric data types::).

     This operator will combine the specified number of inputs into a
     single output that contains the number of remaining elements after
     $\sigma$-clipping on each element/pixel (for more on
     $\sigma$-clipping, see *note Sigma clipping::).  This operator is
     very similar to ‘min’, with the exception that it expects two
     operands (parameters for sigma-clipping) before the total number of
     inputs.  The first popped operand is the termination criteria and
     the second is the multiple of $\sigma$.

     For example in the command below, the first popped operand (‘0.2’)
     is the sigma clipping termination criteria.  If the termination
     criteria is larger than, or equal to, 1 it is interpreted as the
     number of clips to do.  But if it is between 0 and 1, then it is
     the tolerance level on the standard deviation (see *note Sigma
     clipping::).  The second popped operand (‘5’) is the multiple of
     sigma to use in sigma-clipping.  The third popped operand (‘10’) is
     number of datasets that will be used (similar to the first popped
     operand to ‘min’).

          astarithmetic a.fits b.fits c.fits 3 5 0.2 sigclip-number

‘sigclip-median’
     For each pixel, find the sigma-clipped median in all given
     datasets.  The output will have the a single-precision (32-bit)
     floating point type.  This operator is called similar to the
     ‘sigclip-number’ operator, please see there for more.

‘sigclip-mean’
     For each pixel, find the sigma-clipped mean in all given datasets.
     The output will have the a single-precision (32-bit) floating point
     type.  This operator is called similar to the ‘sigclip-number’
     operator, please see there for more.

‘sigclip-std’
     For each pixel, find the sigma-clipped standard deviation in all
     given datasets.  The output will have the a single-precision
     (32-bit) floating point type.  This operator is called similar to
     the ‘sigclip-number’ operator, please see there for more.

‘filter-mean’
     Apply mean filtering (or moving average
     (https://en.wikipedia.org/wiki/Moving_average)) on the input
     dataset.  During mean filtering, each pixel (data element) is
     replaced by the mean value of all its surrounding pixels (excluding
     blank values).  The number of surrounding pixels in each dimension
     (to calculate the mean) is determined through the earlier operands
     that have been pushed onto the stack prior to the input dataset.
     The number of necessary operands is determined by the dimensions of
     the input dataset (first popped operand).  The order of the
     dimensions on the command-line is the order in FITS format.  Here
     is one example:

          $ astarithmetic 5 4 image.fits filter-mean

     In this example, each pixel is replaced by the mean of a 5 by 4 box
     around it.  The box is 5 pixels along the first FITS dimension
     (horizontal when viewed in ds9) and 4 pixels along the second FITS
     dimension (vertical).

     Each pixel will be placed in the center of the box that the mean is
     calculated on.  If the given width along a dimension is even, then
     the center is assumed to be between the pixels (not in the center
     of a pixel).  When the pixel is close to the edge, the pixels of
     the box that fall outside the image are ignored.  Therefore, on the
     edge, less points will be used in calculating the mean.

     The final effect of mean filtering is to smooth the input image, it
     is essentially a convolution with a kernel that has identical
     values for all its pixels (is flat), see *note Convolution
     process::.

     Note that blank pixels will also be affected by this operator: if
     there are any non-blank elements in the box surrounding a blank
     pixel, in the filtered image, it will have the mean of the
     non-blank elements, therefore it won’t be blank any more.  If blank
     elements are important for your analysis, you can use the ‘isblank’
     with the ‘where’ operator to set them back to blank after
     filtering.

‘filter-median’
     Apply median filtering
     (https://en.wikipedia.org/wiki/Median_filter) on the input dataset.
     This is very similar to ‘filter-mean’, except that instead of the
     mean value of the box pixels, the median value is used to replace a
     pixel value.  For more on how to use this operator, please see
     ‘filter-mean’.

     The median is less susceptible to outliers compared to the mean.
     As a result, after median filtering, the pixel values will be more
     discontinuous than mean filtering.

‘filter-sigclip-mean’
     Apply a $\sigma$-clipped mean filtering onto the input dataset.
     This is very similar to ‘filter-mean’, except that all outliers
     (identified by the $\sigma$-clipping algorithm) have been removed,
     see *note Sigma clipping:: for more on the basics of this
     algorithm.  As described there, two extra input parameters are
     necessary for $\sigma$-clipping: the multiple of $\sigma$ and the
     termination criteria.  ‘filter-sigclip-mean’ therefore needs to pop
     two other operands from the stack after the dimensions of the box.

     For example the line below uses the same box size as the example of
     ‘filter-mean’.  However, all elements in the box that are
     iteratively beyond $3\sigma$ of the distribution’s median are
     removed from the final calculation of the mean until the change in
     $\sigma$ is less than $0.2$.

          $ astarithmetic 3 0.2 5 4 image.fits filter-sigclip-mean

     The median (which needs a sorted dataset) is necessary for
     $\sigma$-clipping, therefore ‘filter-sigclip-mean’ can be
     significantly slower than ‘filter-mean’.  However, if there are
     strong outliers in the dataset that you want to ignore (for example
     emission lines on a spectrum when finding the continuum), this is a
     much better solution.

‘filter-sigclip-median’
     Apply a $\sigma$-clipped median filtering onto the input dataset.
     This operator and its necessary operands are almost identical to
     ‘filter-sigclip-mean’, except that after $\sigma$-clipping, the
     median value (which is less affected by outliers than the mean) is
     added back to the stack.

‘interpolate-medianngb’
     Interpolate all the blank elements of the second popped operand
     with the median of its nearest non-blank neighbors.  The number of
     the nearest non-blank neighbors used to calculate the median is
     given by the first popped operand.  Note that the distance of the
     nearest non-blank neighbors is irrelevant in this interpolation.

‘collapse-sum’
     Collapse the given dataset (second popped operand), by summing all
     elements along the first popped operand (a dimension in FITS
     standard: counting from one, from fastest dimension).  The returned
     dataset has one dimension less compared to the input.

     The output will have a double-precision floating point type
     irrespective of the input dataset’s type.  Doing the operation in
     double-precision (64-bit) floating point will help the collapse
     (summation) be affected less by floating point errors.  But
     afterwards, single-precision floating points are usually enough in
     real (noisy) datasets.  So depending on the type of the input and
     its nature, it is recommended to use one of the type conversion
     operators on the returned dataset.

     If any WCS is present, the returned dataset will also lack the
     respective dimension in its WCS matrix.  Therefore, when the WCS is
     important for later processing, be sure that the input is aligned
     with the respective axes: all non-diagonal elements in the WCS
     matrix are zero.

     One common application of this operator is the creation of pseudo
     broad-band or narrow-band 2D images from 3D data cubes.  For
     example integral field unit (IFU) data products that have two
     spatial dimensions (first two FITS dimensions) and one spectral
     dimension (third FITS dimension).  The command below will collapse
     the whole third dimension into a 2D array the size of the first two
     dimensions, and then convert the output to single-precision
     floating point (as discussed above).

          $ astarithmetic cube.fits 3 collapse-sum float32

‘collapse-mean’
     Similar to ‘collapse-sum’, but the returned dataset will be the
     mean value along the collapsed dimension, not the sum.

‘collapse-number’
     Similar to ‘collapse-sum’, but the returned dataset will be the
     number of non-blank values along the collapsed dimension.  The
     output will have a 32-bit signed integer type.  If the input
     dataset doesn’t have blank values, all the elements in the returned
     dataset will have a single value (the length of the collapsed
     dimension).  Therefore this is mostly relevant when there are blank
     values in the dataset.

‘collapse-min’
     Similar to ‘collapse-sum’, but the returned dataset will have the
     same numeric type as the input and will contain the minimum value
     for each pixel along the collapsed dimension.

‘collapse-max’
     Similar to ‘collapse-sum’, but the returned dataset will have the
     same numeric type as the input and will contain the maximum value
     for each pixel along the collapsed dimension.

‘add-dimension’
     Build a higher-dimensional dataset from all the input datasets
     stacked after one another (along the slowest dimension).  The first
     popped operand has to be a single number.  It is used by the
     operator to know how many operands it should pop from the stack
     (and the size of the output in the new dimension).  The rest of the
     operands must have the same size and numerical data type.  This
     operator currently only works for 2D input operands, please contact
     us if you want inputs to have different dimensions.

     The output’s WCS (which should have a different dimensionality
     compared to the inputs) can be read from another file with the
     ‘--wcsfile’ option.  If no file is specified for the WCS, the first
     dataset’s WCS will be used, you can later add/change the necessary
     WCS keywords with the FITS keyword modification features of the
     Fits program (see *note Fits::).

     If your datasets don’t have the same type, you can use the type
     transformation operators of Arithmetic that are discussed below.
     Just beware of overflow if you are transforming to a smaller type,
     see *note Numeric data types::.

     For example if you want to put the three ‘img1.fits’, ‘img2.fits’
     and ‘img3.fits’ images (each a 2D dataset) into one 3D datacube,
     you can use this command:

          $ astarithmetic img1.fits img2.fits img3.fits 3 add-dimension

‘unique’
     Remove all duplicate (and blank) elements from the first popped
     operand.  The unique elements of the dataset will be stored in a
     single-dimensional dataset.

     Recall that by default, single-dimensional datasets are stored as a
     table column in the output.  But you can use ‘--onedasimage’ or
     ‘--onedonstdout’ to respectively store them as a single-dimensional
     FITS array/image, or to print them on the standard output.

‘erode’
     Erode the foreground pixels (with value ‘1’) of the input dataset
     (second popped operand).  The first popped operand is the
     connectivity (see description in ‘connected-components’).  Erosion
     is simply a flipping of all foreground pixels (to background; with
     value ‘0’) that are “touching” background pixels.  “Touching” is
     defined by the connectivity.  In effect, this carves off the outer
     borders of the foreground, making them thinner.  This operator
     assumes a binary dataset (all pixels are ‘0’ and ‘1’).

‘dilate’
     Dilate the foreground pixels (with value ‘1’) of the input dataset
     (second popped operand).  The first popped operand is the
     connectivity (see description in ‘connected-components’).  Erosion
     is simply a flipping of all background pixels (with value ‘0’) to
     foreground that are “touching” foreground pixels.  “Touching” is
     defined by the connectivity.  In effect, this expands the outer
     borders of the foreground.  This operator assumes a binary dataset
     (all pixels are ‘0’ and ‘1’).

‘connected-components’
     Find the connected components in the input dataset (second popped
     operand).  The first popped is the connectivity used in the
     connected components algorithm.  The second popped operand is the
     dataset where connected components are to be found.  It is assumed
     to be a binary image (with values of 0 or 1).  It must have an
     8-bit unsigned integer type which is the format produced by
     conditional operators.  This operator will return a labeled dataset
     where the non-zero pixels in the input will be labeled with a
     counter (starting from 1).

     The connectivity is a number between 1 and the number of dimensions
     in the dataset (inclusive).  1 corresponds to the weakest
     (symmetric) connectivity between elements and the number of
     dimensions the strongest.  For example on a 2D image, a
     connectivity of 1 corresponds to 4-connected neighbors and 2
     corresponds to 8-connected neighbors.

     One example usage of this operator can be the identification of
     regions above a certain threshold, as in the command below.  With
     this command, Arithmetic will first separate all pixels greater
     than 100 into a binary image (where pixels with a value of 1 are
     above that value).  Afterwards, it will label all those that are
     connected.

          $ astarithmetic in.fits 100 gt 2 connected-components

     If your input dataset doesn’t have a binary type, but you know all
     its values are 0 or 1, you can use the ‘uint8’ operator (below) to
     convert it to binary.

‘fill-holes’
     Flip background (0) pixels surrounded by foreground (1) in a binary
     dataset.  This operator takes two operands (similar to
     ‘connected-components’): the first popped operand is the
     connectivity (to define a hole) and the second is the binary (0 or
     1 valued) dataset to fill holes in.

‘invert’
     Invert an unsigned integer dataset.  This is the only operator that
     ignores blank values (which are set to be the maximum values in the
     unsigned integer types).

     This is useful in cases where the target(s) has(have) been imaged
     in absorption as raw formats (which are unsigned integer types).
     With this option, the maximum value for the given type will be
     subtracted from each pixel value, thus “inverting” the image, so
     the target(s) can be treated as emission.  This can be useful when
     the higher-level analysis methods/tools only work on emission
     (positive skew in the noise, not negative).

‘lt’
     Less than: If the second popped (or left operand in infix notation,
     see *note Reverse polish notation::) value is smaller than the
     first popped operand, then this function will return a value of 1,
     otherwise it will return a value of 0.  If both operands are
     images, then all the pixels will be compared with their
     counterparts in the other image.  If only one operand is an image,
     then all the pixels will be compared with the single value (number)
     of the other operand.  Finally if both are numbers, then the output
     is also just one number (0 or 1).  When the output is not a single
     number, it will be stored as an ‘unsigned char’ type.

‘le’
     Less or equal: similar to ‘lt’ (‘less than’ operator), but
     returning 1 when the second popped operand is smaller or equal to
     the first.

‘gt’
     Greater than: similar to ‘lt’ (‘less than’ operator), but returning
     1 when the second popped operand is greater than the first.

‘ge’
     Greater or equal: similar to ‘lt’ (‘less than’ operator), but
     returning 1 when the second popped operand is larger or equal to
     the first.

‘eq’
     Equality: similar to ‘lt’ (‘less than’ operator), but returning 1
     when the two popped operands are equal (to double precision
     floating point accuracy).

‘ne’
     Non-Equality: similar to ‘lt’ (‘less than’ operator), but returning
     1 when the two popped operands are _not_ equal (to double precision
     floating point accuracy).

‘and’
     Logical AND: returns 1 if both operands have a non-zero value and 0
     if both are zero.  Both operands have to be the same kind: either
     both images or both numbers.

‘or’
     Logical OR: returns 1 if either one of the operands is non-zero and
     0 only when both operators are zero.  Both operands have to be the
     same kind: either both images or both numbers.

‘not’
     Logical NOT: returns 1 when the operand is zero and 0 when the
     operand is non-zero.  The operand can be an image or number, for an
     image, it is applied to each pixel separately.

‘isblank’
     Test for a blank value (see *note Blank pixels::).  In essence,
     this is very similar to the conditional operators: the output is
     either 1 or 0 (see the ‘less than’ operator above).  The difference
     is that it only needs one operand.  Because of the definition of a
     blank pixel, a blank value is not even equal to itself, so you
     cannot use the equal operator above to select blank pixels.  See
     the “Blank pixels” box below for more on Blank pixels in
     Arithmetic.

‘where’
     Change the input (pixel) value _where_/if a certain condition
     holds.  The conditional operators above can be used to define the
     condition.  Three operands are required for ‘where’.  The input
     format is demonstrated in this simplified example:

          $ astarithmetic modify.fits binary.fits if-true.fits where

     The value of any pixel in ‘modify.fits’ that corresponds to a
     non-zero _and_ non-blank pixel of ‘binary.fits’ will be changed to
     the value of the same pixel in ‘if-true.fits’ (this may also be a
     number).  The 3rd and 2nd popped operands (‘modify.fits’ and
     ‘binary.fits’ respectively, see *note Reverse polish notation::)
     have to have the same dimensions/size.  ‘if-true.fits’ can be
     either a number, or have the same dimension/size as the other two.

     The 2nd popped operand (‘binary.fits’) has to have ‘uint8’ (or
     ‘unsigned char’ in standard C) type (see *note Numeric data
     types::).  It is treated as a binary dataset (with only two values:
     zero and non-zero, hence the name ‘binary.fits’ in this example).
     However, commonly you won’t be dealing with an actual FITS file of
     a condition/binary image.  You will probably define the condition
     in the same run based on some other reference image and use the
     conditional and logical operators above to make a true/false (or
     one/zero) image for you internally.  For example the case below:

          $ astarithmetic in.fits reference.fits 100 gt new.fits where

     In the example above, any of the ‘in.fits’ pixels that has a value
     in ‘reference.fits’ greater than ‘100’, will be replaced with the
     corresponding pixel in ‘new.fits’.  Effectively the ‘reference.fits
     100 gt’ part created the condition/binary image which was added to
     the stack (in memory) and later used by ‘where’.  The command above
     is thus equivalent to these two commands:

          $ astarithmetic reference.fits 100 gt --output=binary.fits
          $ astarithmetic in.fits binary.fits new.fits where

     Finally, the input operands are read and used independently, so you
     can use the same file more than once as any of the operands.

     When the 1st popped operand to ‘where’ (‘if-true.fits’) is a single
     number, it may be a NaN value (or any blank value, depending on its
     type) like the example below (see *note Blank pixels::).  When the
     number is blank, it will be converted to the blank value of the
     type of the 3rd popped operand (‘in.fits’).  Hence, in the example
     below, all the pixels in ‘reference.fits’ that have a value greater
     than 100, will become blank in the natural data type of ‘in.fits’
     (even though NaN values are only defined for floating point types).

          $ astarithmetic in.fits reference.fits 100 gt nan where

‘bitand’
     Bitwise AND operator: only bits with values of 1 in both popped
     operands will get the value of 1, the rest will be set to 0.  For
     example (assuming numbers can be written as bit strings on the
     command-line): ‘00101000 00100010 bitand’ will give ‘00100000’.
     Note that the bitwise operators only work on integer type datasets.

‘bitor’
     Bitwise inclusive OR operator: The bits where at least one of the
     two popped operands has a 1 value get a value of 1, the others 0.
     For example (assuming numbers can be written as bit strings on the
     command-line): ‘00101000 00100010 bitand’ will give ‘00101010’.
     Note that the bitwise operators only work on integer type datasets.

‘bitxor’
     Bitwise exclusive OR operator: A bit will be 1 if it differs
     between the two popped operands.  For example (assuming numbers can
     be written as bit strings on the command-line): ‘00101000 00100010
     bitand’ will give ‘00001010’.  Note that the bitwise operators only
     work on integer type datasets.

‘lshift’
     Bitwise left shift operator: shift all the bits of the first
     operand to the left by a number of times given by the second
     operand.  For example (assuming numbers can be written as bit
     strings on the command-line): ‘00101000 2 lshift’ will give
     ‘10100000’.  This is equivalent to multiplication by 4.  Note that
     the bitwise operators only work on integer type datasets.

‘rshift’
     Bitwise right shift operator: shift all the bits of the first
     operand to the right by a number of times given by the second
     operand.  For example (assuming numbers can be written as bit
     strings on the command-line): ‘00101000 2 rshift’ will give
     ‘00001010’.  Note that the bitwise operators only work on integer
     type datasets.

‘bitnot’
     Bitwise not (more formally known as one’s complement) operator:
     flip all the bits of the popped operand (note that this is the only
     unary, or single operand, bitwise operator).  In other words, any
     bit with a value of ‘0’ is changed to ‘1’ and vice-versa.  For
     example (assuming numbers can be written as bit strings on the
     command-line): ‘00101000 bitnot’ will give ‘11010111’.  Note that
     the bitwise operators only work on integer type datasets/numbers.

‘uint8’
     Convert the type of the popped operand to 8-bit unsigned integer
     type (see *note Numeric data types::).  The internal conversion of
     C will be used.

‘int8’
     Convert the type of the popped operand to 8-bit signed integer type
     (see *note Numeric data types::).  The internal conversion of C
     will be used.

‘uint16’
     Convert the type of the popped operand to 16-bit unsigned integer
     type (see *note Numeric data types::).  The internal conversion of
     C will be used.

‘int16’
     Convert the type of the popped operand to 16-bit signed integer
     (see *note Numeric data types::).  The internal conversion of C
     will be used.

‘uint32’
     Convert the type of the popped operand to 32-bit unsigned integer
     type (see *note Numeric data types::).  The internal conversion of
     C will be used.

‘int32’
     Convert the type of the popped operand to 32-bit signed integer
     type (see *note Numeric data types::).  The internal conversion of
     C will be used.

‘uint64’
     Convert the type of the popped operand to 64-bit unsigned integer
     (see *note Numeric data types::).  The internal conversion of C
     will be used.

‘float32’
     Convert the type of the popped operand to 32-bit (single precision)
     floating point (see *note Numeric data types::).  The internal
     conversion of C will be used.

‘float64’
     Convert the type of the popped operand to 64-bit (double precision)
     floating point (see *note Numeric data types::).  The internal
     conversion of C will be used.

‘size’
     Size of the dataset along a given FITS/Fortran dimension (counting
     from 1).  The desired dimension should be the first popped operand
     and the dataset must be the second popped operand.  The output will
     be a single unsigned integer (dimensions cannot be negative).  For
     example, the following command will produce the size of the first
     extension/HDU (the default HDU) of ‘a.fits’ along the second FITS
     axis.

          astarithmetic a.fits 2 size

‘set-AAA’
     Set the characters after the dash (‘AAA’ in the case shown here) as
     a name for the first popped operand on the stack.  The named
     dataset will be freed from memory as soon as it is no longer
     needed, or if the name is reset to refer to another dataset later
     in the command.  This operator thus enables re-usability of a
     dataset without having to re-read it from a file every time it is
     necessary during a process.  When a dataset is necessary more than
     once, this operator can thus help simplify reading/writing on the
     command-line (thus avoiding potential bugs), while also speeding up
     the processing.

     Like all operators, this operator pops the top operand off of the
     main processing stack, but unlike other operands, it won’t add
     anything back to the stack immediately.  It will keep the popped
     dataset in memory through a separate list of named datasets (not on
     the main stack).  That list will be used to add/copy any requested
     dataset to the main processing stack when the name is called.

     The name to give the popped dataset is part of the operator’s name.
     For example the ‘set-a’ operator of the command below, gives the
     name “‘a’” to the contents of ‘image.fits’.  This name is then used
     instead of the actual filename to multiply the dataset by two.

          $ astarithmetic image.fits set-a a 2 x

     The name can be any string, but avoid strings ending with standard
     filename suffixes (for example ‘.fits’)(1).

     One example of the usefulness of this operator is in the ‘where’
     operator.  For example, let’s assume you want to mask all pixels
     larger than ‘5’ in ‘image.fits’ (extension number 1) with a NaN
     value.  Without setting a name for the dataset, you have to read
     the file two times from memory in a command like this:

          $ astarithmetic image.fits image.fits 5 gt nan where -g1

     But with this operator you can simply give ‘image.fits’ the name
     ‘i’ and simplify the command above to the more readable one below
     (which greatly helps when the filename is long):

          $ astarithmetic image.fits set-i   i i 5 gt nan where

‘tofile-AAA’
     Write the top operand on the operands stack into a file called
     ‘AAA’ (can be any FITS file name) without changing the operands
     stack.  If you don’t need the dataset any more and would like to
     free it, see the ‘tofilefree’ operator below.

     By default, any file that is given to this operator is deleted
     before Arithmetic actually starts working on the input datasets.
     The deletion can be deactivated with the ‘--dontdelete’ option (as
     in all Gnuastro programs, see *note Input output options::).  If
     the same FITS file is given to this operator multiple times, it
     will contain multiple extensions (in the same order that it was
     called.

     For example the operator ‘tofile-check.fits’ will write the top
     operand to ‘check.fits’.  Since it doesn’t modify the operands
     stack, this operator is very convenient when you want to debug, or
     understanding, a string of operators and operands given to
     Arithmetic: simply put ‘tofile-AAA’ anywhere in the process to see
     what is happening behind the scenes without modifying the overall
     process.

‘tofilefree-AAA’
     Similar to the ‘tofile’ operator, with the only difference that the
     dataset that is written to a file is popped from the operand stack
     and freed from memory (cannot be used any more).

*Blank pixels in Arithmetic:* Blank pixels in the image (see *note Blank
pixels::) will be stored based on the data type.  When the input is
floating point type, blank values are NaN. One aspect of NaN values is
that by definition they will fail on _any_ comparison.  Hence both equal
and not-equal operators will fail when both their operands are NaN!
Therefore, the only way to guarantee selection of blank pixels is
through the ‘isblank’ operator explained above.

   One way you can exploit this property of the NaN value to your
advantage is when you want a fully zero-valued image (even over the
blank pixels) based on an already existing image (with same size and
world coordinate system settings).  The following command will produce
this for you:

     $ astarithmetic input.fits nan eq --output=all-zeros.fits

Note that on the command-line you can write NaN in any case (for example
‘NaN’, or ‘NAN’ are also acceptable).  Reading NaN as a floating point
number in Gnuastro isn’t case-sensitive.

   ---------- Footnotes ----------

   (1) A dataset name like ‘a.fits’ (which can be set with ‘set-a.fits’)
will cause confusion in the initial parser of Arithmetic.  It will
assume this name is a FITS file, and if it is used multiple times,
Arithmetic will abort, complaining that you haven’t provided enough
HDUs.


File: gnuastro.info,  Node: Invoking astarithmetic,  Prev: Arithmetic operators,  Up: Arithmetic

6.2.3 Invoking Arithmetic
-------------------------

Arithmetic will do pixel to pixel arithmetic operations on the
individual pixels of input data and/or numbers.  For the full list of
operators with explanations, please see *note Arithmetic operators::.
Any operand that only has a single element (number, or single pixel FITS
image) will be read as a number, the rest of the inputs must have the
same dimensions.  The general template is:

     $ astarithmetic [OPTION...] ASTRdata1 [ASTRdata2] OPERATOR ...

One line examples:

     ## Calculate (10.32-3.84)^2.7 quietly (will just print 155.329):
     $ astarithmetic -q 10.32 3.84 - 2.7 pow

     ## Inverse the input image (1/pixel):
     $ astarithmetic 1 image.fits / --out=inverse.fits

     ## Multiply each pixel in image by -1:
     $ astarithmetic image.fits -1 x --out=negative.fits

     ## Subtract extension 4 from extension 1 (counting from zero):
     $ astarithmetic image.fits image.fits - --out=skysub.fits           \
                     --hdu=1 --hdu=4

     ## Add two images, then divide them by 2 (2 is read as floating point):
     $ astarithmetic image1.fits image2.fits + 2f / --out=average.fits

     ## Use Arithmetic's average operator:
     $ astarithmetic image1.fits image2.fits average --out=average.fits

     ## Calculate the median of three images in three separate extensions:
     $ astarithmetic img1.fits img2.fits img3.fits median                \
                     -h0 -h1 -h2 --out=median.fits

   Arithmetic’s notation for giving operands to operators is fully
described in *note Reverse polish notation::.  The output dataset is
last remaining operand on the stack.  When the output dataset a single
number, it will be printed on the command-line.  When the output is an
array, it will be stored as a file.

   The name of the final file can be specified with the ‘--output’
option, but if its not given, Arithmetic will use “automatic output” on
the name of the first FITS image encountered to generate an output file
name, see *note Automatic output::.  By default, if the output file
already exists, it will be deleted before Arithmetic starts operation.
However, this can be disabled with the ‘--dontdelete’ option (see
below).  At any point during Arithmetic’s operation, you can also write
the top operand on the stack to a file, using the ‘tofile’ or
‘tofilefree’ operators, see *note Arithmetic operators::.

   By default, the world coordinate system (WCS) information of the
output dataset will be taken from the first input image (that contains a
WCS) on the command-line.  This can be modified with the ‘--wcsfile’ and
‘--wcshdu’ options described below.  When the ‘--quiet’ option isn’t
given, the name and extension of the dataset used for the output’s WCS
is printed on the command-line.

   Through operators like those starting with ‘collapse-’, the
dimensionality of the inputs may not be the same as the outputs.  By
default, when the output is 1D, Arithmetic will write it as a table, not
an image/array.  The format of the output table (plain text or FITS
ASCII or binary) can be set with the ‘--tableformat’ option, see *note
Input output options::).  You can disable this feature (write 1D arrays
as FITS images/arrays, or to the standard output) with the
‘--onedasimage’ or ‘--onedonstdout’ options.

   See *note Common options:: for a review of the options in all
Gnuastro programs.  Arithmetic just redefines the ‘--hdu’ and
‘--dontdelete’ options as explained below.

‘-h INT/STR’
‘--hdu INT/STR’
     The header data unit of the input FITS images, see *note Input
     output options::.  Unlike most options in Gnuastro (which will
     ultimately only have one value for this option), Arithmetic allows
     ‘--hdu’ to be called multiple times and the value of each
     invocation will be stored separately (for the unlimited number of
     input images you would like to use).  Recall that for other
     programs this (common) option only takes a single value.  So in
     other programs, if you specify it multiple times on the
     command-line, only the last value will be used and in the
     configuration files, it will be ignored if it already has a value.

     The order of the values to ‘--hdu’ has to be in the same order as
     input FITS images.  Options are first read from the command-line
     (from left to right), then top-down in each configuration file, see
     *note Configuration file precedence::.

     If the number of HDUs is less than the number of input images,
     Arithmetic will abort and notify you.  However, if there are more
     HDUs than FITS images, there is no problem: they will be used in
     the given order (every time a FITS image comes up on the stack) and
     the extra HDUs will be ignored in the end.  So there is no problem
     with having extra HDUs in the configuration files and by default
     several HDUs with a value of ‘0’ are kept in the system-wide
     configuration file when you install Gnuastro.

‘-g INT/STR’
‘--globalhdu INT/STR’
     Use the value to this option as the HDU of all input FITS files.
     This option is very convenient when you have many input files and
     the dataset of interest is in the same HDU of all the files.  When
     this option is called, any values given to the ‘--hdu’ option
     (explained above) are ignored and will not be used.

‘-w STR’
‘--wcsfile STR’
     FITS Filename containing the WCS structure that must be written to
     the output.  The HDU/extension should be specified with ‘--wcshdu’.

     When this option is used, the respective WCS will be read before
     any processing is done on the command-line and directly used in the
     final output.  If the given file doesn’t have any WCS, then the
     default WCS (first file on the command-line with WCS) will be used
     in the output.

     This option will mostly be used when the default file (first of the
     set of inputs) is not the one containing your desired WCS. But with
     this option, you can also use Arithmetic to rewrite/change the WCS
     of an existing FITS dataset from another file:

          $ astarithmetic data.fits --wcsfile=other.fits -ofinal.fits

‘-W STR’
‘--wcshdu STR’
     HDU/extension to read the WCS within the file given to ‘--wcsfile’.
     For more, see the description of ‘--wcsfile’.

‘-O’
‘--onedasimage’
     When final dataset to write as output only has one dimension, write
     it as a FITS image/array.  By default, if the output is 1D, it will
     be written as a table, see above.

‘-s’
‘--onedonstdout’
     When final dataset to write as output only has one dimension, print
     it on the standard output, not in a file.  By default, if the
     output is 1D, it will be written as a table, see above.

‘-D’
‘--dontdelete’
     Don’t delete the output file, or files given to the ‘tofile’ or
     ‘tofilefree’ operators, if they already exist.  Instead append the
     desired datasets to the extensions that already exist in the
     respective file.  Note it doesn’t matter if the final output file
     name is given with the ‘--output’ option, or determined
     automatically.

     Arithmetic treats this option differently from its default
     operation in other Gnuastro programs (see *note Input output
     options::).  If the output file exists, when other Gnuastro
     programs are called with ‘--dontdelete’, they simply complain and
     abort.  But when Arithmetic is called with ‘--dontdelete’, it will
     appended the dataset(s) to the existing extension(s) in the file.

   Arithmetic accepts two kinds of input: images and numbers.  Images
are considered to be any of the inputs that is a file name of a
recognized type (see *note Arguments::) and has more than one
element/pixel.  Numbers on the command-line will be read into the
smallest type (see *note Numeric data types::) that can store them, so
‘-2’ will be read as a ‘char’ type (which is signed on most systems and
can thus keep negative values), ‘2500’ will be read as an ‘unsigned
short’ (all positive numbers will be read as unsigned), while
‘3.1415926535897’ will be read as a ‘double’ and ‘3.14’ will be read as
a ‘float’.  To force a number to be read as float, add a ‘f’ after it,
so ‘5f’ will be added to the stack as ‘float’ (see *note Reverse polish
notation::).

   Unless otherwise stated (in *note Arithmetic operators::), the
operators can deal with numeric multiple data types (see *note Numeric
data types::).  For example in “‘a.fits b.fits +’”, the image types can
be ‘long’ and ‘float’.  In such cases, C’s internal type conversion will
be used.  The output type will be set to the higher-ranking type of the
two inputs.  Unsigned integer types have smaller ranking than their
signed counterparts and floating point types have higher ranking than
the integer types.  So the internal C type conversions done in the
example above are equivalent to this piece of C:

     size_t i;
     long a[100];
     float b[100], out[100];
     for(i=0;i<100;++i) out[i]=a[i]+b[i];

Relying on the default C type conversion significantly speeds up the
processing and also requires less RAM (when using very large images).

   Some operators can only work on integer types (of any length, for
example bitwise operators) while others only work on floating point
types, (currently only the ‘pow’ operator).  In such cases, if the
operand type(s) are different, an error will be printed.  Arithmetic
also comes with internal type conversion operators which you can use to
convert the data into the appropriate type, see *note Arithmetic
operators::.

   The hyphen (‘-’) can be used both to specify options (see *note
Options::) and also to specify a negative number which might be
necessary in your arithmetic.  In order to enable you to do this,
Arithmetic will first parse all the input strings and if the first
character after a hyphen is a digit, then that hyphen is temporarily
replaced by the vertical tab character which is not commonly used.  The
arguments are then parsed and these strings will not be specified as an
option.  Then the given arguments are parsed and any vertical tabs are
replaced back with a hyphen so they can be read as negative numbers.
Therefore, as long as the names of the files you want to work on, don’t
start with a vertical tab followed by a digit, there is no problem.  An
important consequence of this implementation is that you should not
write negative fractions like this: ‘-.3’, instead write them as ‘-0.3’.

   Without any images, Arithmetic will act like a simple calculator and
print the resulting output number on the standard output like the first
example above.  If you really want such calculator operations on the
command-line, AWK (GNU AWK is the most common implementation) is much
faster, easier and much more powerful.  For example, the numerical
one-line example above can be done with the following command.  In
general AWK is a fantastic tool and GNU AWK has a wonderful manual
(<https://www.gnu.org/software/gawk/manual/>).  So if you often confront
situations like this, or have to work with large text tables/catalogs,
be sure to checkout AWK and simplify your life.

     $ echo "" | awk '{print (10.32-3.84)^2.7}'
     155.329


File: gnuastro.info,  Node: Convolve,  Next: Warp,  Prev: Arithmetic,  Up: Data manipulation

6.3 Convolve
============

On an image, convolution can be thought of as a process to blur or
remove the contrast in an image.  If you are already familiar with the
concept and just want to run Convolve, you can jump to *note Convolution
kernel:: and *note Invoking astconvolve:: and skip the lengthy
introduction on the basic definitions and concepts of convolution.

   There are generally two methods to convolve an image.  The first and
more intuitive one is in the “spatial domain” or using the actual image
pixel values, see *note Spatial domain convolution::.  The second method
is when we manipulate the “frequency domain”, or work on the magnitudes
of the different frequencies that constitute the image, see *note
Frequency domain and Fourier operations::.  Understanding convolution in
the spatial domain is more intuitive and thus recommended if you are
just starting to learn about convolution.  However, getting a good grasp
of the frequency domain is a little more involved and needs some
concentration and some mathematical proofs.  However, its reward is a
faster operation and more importantly a very fundamental understanding
of this very important operation.

   Convolution of an image will generally result in blurring the image
because it mixes pixel values.  In other words, if the image has sharp
differences in neighboring pixel values(1), those sharp differences will
become smoother.  This has very good consequences in detection of signal
in noise for example.  In an actual observed image, the variation in
neighboring pixel values due to noise can be very high.  But after
convolution, those variations will decrease and we have a better hope in
detecting the possible underlying signal.  Another case where
convolution is extensively used is in mock images and modeling in
general, convolution can be used to simulate the effect of the
atmosphere or the optical system on the mock profiles that we create,
see *note PSF::.  Convolution is a very interesting and important topic
in any form of signal analysis (including astronomical observations).
So we have thoroughly(2) explained the concepts behind it in the
following sub-sections.

* Menu:

* Spatial domain convolution::  Only using the input image values.
* Frequency domain and Fourier operations::  Using frequencies in input.
* Spatial vs. Frequency domain::  When to use which?
* Convolution kernel::          How to specify the convolution kernel.
* Invoking astconvolve::        Options and argument to Convolve.

   ---------- Footnotes ----------

   (1) In astronomy, the only major time we confront such sharp borders
in signal are cosmic rays.  All other sources of signal in an image are
already blurred by the atmosphere or the optics of the instrument.

   (2) A mathematician will certainly consider this explanation is
incomplete and inaccurate.  However this text is written for an
understanding on the operations that are done on a real (not complex,
discrete and noisy) astronomical image, not any general form of abstract
function


File: gnuastro.info,  Node: Spatial domain convolution,  Next: Frequency domain and Fourier operations,  Prev: Convolve,  Up: Convolve

6.3.1 Spatial domain convolution
--------------------------------

The pixels in an input image represent different “spatial” positions,
therefore when convolution is done only using the actual input pixel
values, we name the process as being done in the “Spatial domain”.  In
particular this is in contrast to the “frequency domain” that we will
discuss later in *note Frequency domain and Fourier operations::.  In
the spatial domain (and in realistic situations where the image and the
convolution kernel don’t extend to infinity), convolution is the process
of changing the value of one pixel to the _weighted_ average of all the
pixels in its _neighborhood_.

   The ‘neighborhood’ of each pixel (how many pixels in which direction)
and the ‘weight’ function (how much each neighboring pixel should
contribute depending on its position) are given through a second image
which is known as a “kernel”(1).

* Menu:

* Convolution process::         More basic explanations.
* Edges in the spatial domain::  Dealing with the edges of an image.

   ---------- Footnotes ----------

   (1) Also known as filter, here we will use ‘kernel’.


File: gnuastro.info,  Node: Convolution process,  Next: Edges in the spatial domain,  Prev: Spatial domain convolution,  Up: Spatial domain convolution

6.3.1.1 Convolution process
...........................

In convolution, the kernel specifies the weight and positions of the
neighbors of each pixel.  To find the convolved value of a pixel, the
central pixel of the kernel is placed on that pixel.  The values of each
overlapping pixel in the kernel and image are multiplied by each other
and summed for all the kernel pixels.  To have one pixel in the center,
the sides of the convolution kernel have to be an odd number.  This
process effectively mixes the pixel values of each pixel with its
neighbors, resulting in a blurred image compared to the sharper input
image.

   Formally, convolution is one kind of linear ‘spatial filtering’ in
image processing texts.  If we assume that the kernel has $2a+1$ and
$2b+1$ pixels on each side, the convolved value of a pixel placed at $x$
and $y$ ($C_{x,y}$) can be calculated from the neighboring pixel values
in the input image ($I$) and the kernel ($K$) from

 $$C_{x,y}=\sum_{s=-a}^{a}\sum_{t=-b}^{b}K_{s,t}\times{}I_{x+s,y+t}.$$

   Any pixel coordinate that is outside of the image in the equation
above will be considered to be zero.  When the kernel is symmetric about
its center the blurred image has the same orientation as the original
image.  However, if the kernel is not symmetric, the image will be
affected in the opposite manner, this is a natural consequence of the
definition of spatial filtering.  In order to avoid this we can rotate
the kernel about its center by 180 degrees so the convolved output can
have the same original orientation.  Technically speaking, only if the
kernel is flipped the process is known _Convolution_.  If it isn’t it is
known as _Correlation_.

   To be a weighted average, the sum of the weights (the pixels in the
kernel) have to be unity.  This will have the consequence that the
convolved image of an object and unconvolved object will have the same
brightness (see *note Flux Brightness and magnitude::), which is
natural, because convolution should not eat up the object photons, it
only disperses them.


File: gnuastro.info,  Node: Edges in the spatial domain,  Prev: Convolution process,  Up: Spatial domain convolution

6.3.1.2 Edges in the spatial domain
...................................

In purely ‘linear’ spatial filtering (convolution), there are problems
on the edges of the input image.  Here we will explain the problem in
the spatial domain.  For a discussion of this problem from the frequency
domain perspective, see *note Edges in the frequency domain::.  The
problem originates from the fact that on the edges, in practice(1), the
sum of the weights we use on the actual image pixels is not unity.  For
example, as discussed above, a profile in the center of an image will
have the same brightness before and after convolution.  However, for
partially imaged profile on the edge of the image, the brightness (sum
of its pixel fluxes within the image, see *note Flux Brightness and
magnitude::) will not be equal, some of the flux is going to be ‘eaten’
by the edges.

   If you ran ‘$ make check’ on the source files of Gnuastro, you can
see the this effect by comparing the ‘convolve_frequency.fits’ with
‘convolve_spatial.fits’ in the ‘./tests/’ directory.  In the spatial
domain, by default, no assumption will be made about pixels outside of
the image or any blank pixels in the image.  The problem explained above
will also occur on the sides of blank regions (see *note Blank
pixels::).  The solution to this edge effect problem is only possible in
the spatial domain.  For pixels near the edge, we have to abandon the
assumption that the sum of the kernel pixels is unity during the
convolution process(2).  So taking $W$ as the sum of the kernel pixels
that overlapped with non-blank and in-image pixels, the equation in
*note Convolution process:: will become:

$$C_{x,y}= { \sum_{s=-a}^{a}\sum_{t=-b}^{b}K_{s,t}\times{}I_{x+s,y+t} \over W}.$$

In this manner, objects which are near the edges of the image or blank
pixels will also have the same brightness (within the image) before and
after convolution.  This correction is applied by default in Convolve
when convolving in the spatial domain.  To disable it, you can use the
‘--noedgecorrection’ option.  In the frequency domain, there is no way
to avoid this loss of flux near the edges of the image, see *note Edges
in the frequency domain:: for an interpretation from the frequency
domain perspective.

   Note that the edge effect discussed here is different from the one in
*note If convolving afterwards::.  In making mock images we want to
simulate a real observation.  In a real observation the images of the
galaxies on the sides of the CCD are first blurred by the atmosphere and
instrument, then imaged.  So light from the parts of a galaxy which are
immediately outside the CCD will affect the parts of the galaxy which
are covered by the CCD. Therefore in modeling the observation, we have
to convolve an image that is larger than the input image by exactly half
of the convolution kernel.  We can hence conclude that this correction
for the edges is only useful when working on actual observed images
(where we don’t have any more data on the edges) and not in modeling.

   ---------- Footnotes ----------

   (1) Because we assumed the overlapping pixels outside the input image
have a value of zero.

   (2) ofcourse the sum of the kernel pixels still have to be unity in
general.


File: gnuastro.info,  Node: Frequency domain and Fourier operations,  Next: Spatial vs. Frequency domain,  Prev: Spatial domain convolution,  Up: Convolve

6.3.2 Frequency domain and Fourier operations
---------------------------------------------

Getting a good grip on the frequency domain is usually not an easy job!
So we have decided to give the issue a complete review here.
Convolution in the frequency domain (see *note Convolution theorem::)
heavily relies on the concepts of Fourier transform (*note Fourier
transform::) and Fourier series (*note Fourier series::) so we will be
investigating these important operations first.  It has become something
of a cliché for people to say that the Fourier series “is a way to
represent a (wave-like) function as the sum of simple sine waves” (from
Wikipedia).  However, sines themselves are abstract functions, so this
statement really adds no extra layer of physical insight.

   Before jumping head-first into the equations and proofs, we will
begin with a historical background to see how the importance of
frequencies actually roots in our ancient desire to see everything in
terms of circles.  A short review of how the complex plane should be
interpreted is then given.  Having paved the way with these two basics,
we define the Fourier series and subsequently the Fourier transform.
The final aim is to explain discrete Fourier transform, however some
very important concepts need to be solidified first: The Dirac comb,
convolution theorem and sampling theorem.  So each of these topics are
explained in their own separate sub-sub-section before going on to the
discrete Fourier transform.  Finally we revisit (after *note Edges in
the spatial domain::) the problem of convolution on the edges, but this
time in the frequency domain.  Understanding the sampling theorem and
the discrete Fourier transform is very important in order to be able to
pull out valuable science from the discrete image pixels.  Therefore we
have included the mathematical proofs and figures so you can have a
clear understanding of these very important concepts.

* Menu:

* Fourier series historical background::  Historical background.
* Circles and the complex plane::  Interpreting complex numbers.
* Fourier series::              Fourier Series definition.
* Fourier transform::           Fourier Transform definition.
* Dirac delta and comb::        Dirac delta and Dirac comb.
* Convolution theorem::         Derivation of Convolution theorem.
* Sampling theorem::            Sampling theorem (Nyquist frequency).
* Discrete Fourier transform::  Derivation and explanation of DFT.
* Fourier operations in two dimensions::  Extend to 2D images.
* Edges in the frequency domain::  Interpretation of edge effects.


File: gnuastro.info,  Node: Fourier series historical background,  Next: Circles and the complex plane,  Prev: Frequency domain and Fourier operations,  Up: Frequency domain and Fourier operations

6.3.2.1 Fourier series historical background
............................................

Ever since the ancient times, the circle has been (and still is) the
simplest shape for abstract comprehension.  All you need is a center
point and a radius and you are done.  All the points on a circle are at
a fixed distance from the center.  However, the moment you try to
connect this elegantly simple and beautiful abstract construct (the
circle) with the real world (for example compute its area or its
circumference), things become really hard (ideally, impossible) because
the irrational number $\pi$ gets involved.

   The key to understanding the Fourier series (thus the Fourier
transform and finally the Discrete Fourier Transform) is our ancient
desire to express everything in terms of circles or the most
exceptionally simple and elegant abstract human construct.  Most people
prefer to say the same thing in a more ahistorical manner: to break a
function into sines and cosines.  As the term “ancient” in the previous
sentence implies, Jean-Baptiste Joseph Fourier (1768 – 1830 A.D.) was
not the first person to do this.  The main reason we know this process
by his name today is that he came up with an ingenious method to find
the necessary coefficients (radius of) and frequencies (“speed” of
rotation on) the circles for any generic (integrable) function.

 [image src="gnuastro-figures/epicycles.png" alt="Middle ages epicycles along with two demonstrations of breaking a generic function using epicycles." text="../gnuastro-figures//epicycles.png" ]

Figure 6.1: Epicycles and the Fourier series.  Left: A demonstration of
Mercury’s epicycles relative to the “center of the world” by Qutb al-Din
al-Shirazi (1236 – 1311 A.D.) retrieved from Wikipedia
(https://commons.wikimedia.org/wiki/File:Ghotb2.jpg).  Middle
(https://commons.wikimedia.org/wiki/File:Fourier_series_square_wave_circles_animation.gif)
and Right: How adding more epicycles (or terms in the Fourier series)
will approximate functions.  The right
(https://commons.wikimedia.org/wiki/File:Fourier_series_sawtooth_wave_circles_animation.gif)
animation is also available.

   Like most aspects of mathematics, this process of interpreting
everything in terms of circles, began for astronomical purposes.  When
astronomers noticed that the orbit of Mars and other outer planets, did
not appear to be a simple circle (as everything should have been in the
heavens).  At some point during their orbit, the revolution of these
planets would become slower, stop, go back a little (in what is known as
the retrograde motion) and then continue going forward again.

   The correction proposed by Ptolemy (90 – 168 A.D.) was the most
agreed upon.  He put the planets on Epicycles or circles whose center
itself rotates on a circle whose center is the earth.  Eventually, as
observations became more and more precise, it was necessary to add more
and more epicycles in order to explain the complex motions of the
planets(1).  *note Figure 6.1: epicycle.(Left) shows an example
depiction of the epicycles of Mercury in the late 13th century.

   Of course we now know that if they had abdicated the Earth from its
throne in the center of the heavens and allowed the Sun to take its
place, everything would become much simpler and true.  But there wasn’t
enough observational evidence for changing the “professional consensus”
of the time to this radical view suggested by a small minority(2).  So
the pre-Galilean astronomers chose to keep Earth in the center and find
a correction to the models (while keeping the heavens a purely
“circular” order).

   The main reason we are giving this historical background which might
appear off topic is to give historical evidence that while such
“approximations” do work and are very useful for pragmatic reasons (like
measuring the calendar from the movement of astronomical bodies).  They
offer no physical insight.  The astronomers who were involved with the
Ptolemaic world view had to add a huge number of epicycles during the
centuries after Ptolemy in order to explain more accurate observations.
Finally the death knell of this world-view was Galileo’s observations
with his new instrument (the telescope).  So the physical insight, which
is what Astronomers and Physicists are interested in (as opposed to
Mathematicians and Engineers who just like proving and optimizing or
calculating!)  comes from being creative and not limiting our selves to
such approximations.  Even when they work.

   ---------- Footnotes ----------

   (1) See the Wikipedia page on “Deferent and epicycle” for a more
complete historical review.

   (2) Aristarchus of Samos (310 – 230 B.C.) appears to be one of the
first people to suggest the Sun being in the center of the universe.
This approach to science (that the standard model is defined by
consensus) and the fact that this consensus might be completely wrong
still applies equally well to our models of particle physics and
cosmology today.


File: gnuastro.info,  Node: Circles and the complex plane,  Next: Fourier series,  Prev: Fourier series historical background,  Up: Frequency domain and Fourier operations

6.3.2.2 Circles and the complex plane
.....................................

Before going onto the derivation, it is also useful to review how the
complex numbers and their plane relate to the circles we talked about
above.  The two schematics in the middle and right of *note Figure 6.1:
epicycle. show how a 1D function of time can be made using the 2D real
and imaginary surface.  Seeing the animation in Wikipedia will really
help in understanding this important concept.  At each point in time, we
take the vertical coordinate of the point and use it to find the value
of the function at that point in time.  *note Figure 6.2: iandtime.
shows this relation with the axes marked.

   Leonhard Euler(1) (1707 – 1783 A.D.) showed that the complex
exponential ($e^{iv}$ where $v$ is real) is periodic and can be written
as: $e^{iv}=\cos{v}+isin{v}$.  Therefore $e^{iv+2\pi}=e^{iv}$.  Later,
Caspar Wessel (mathematician and cartographer 1745 – 1818 A.D.) showed
how complex numbers can be displayed as vectors on a plane.  Euler’s
identity might seem counter intuitive at first, so we will try to
explain it geometrically (for deeper physical insight).  On the
real-imaginary 2D plane (like the left hand plot in each box of *note
Figure 6.2: iandtime.), multiplying a number by $i$ can be interpreted
as rotating the point by $90$ degrees (for example the value $3$ on the
real axis becomes $3i$ on the imaginary axis).  On the other hand,
$e\equiv\lim_{n\rightarrow\infty}(1+{1\over n})^n$, therefore, defining
$m\equiv nu$, we get:

    $$e^{u}=\lim_{n\rightarrow\infty}\left(1+{1\over n}\right)^{nu}
       =\lim_{n\rightarrow\infty}\left(1+{u\over nu}\right)^{nu}
       =\lim_{m\rightarrow\infty}\left(1+{u\over m}\right)^{m}$$

Taking $u\equiv iv$ the result can be written as a generic complex
number (a function of $v$):

          $$e^{iv}=\lim_{m\rightarrow\infty}\left(1+i{v\over
                      m}\right)^{m}=a(v)+ib(v)$$

For $v=\pi$, a nice geometric animation of going to the limit can be
seen on Wikipedia (https://commons.wikimedia.org/wiki/File:ExpIPi.gif).
We see that $\lim_{m\rightarrow\infty}a(\pi)=-1$, while
$\lim_{m\rightarrow\infty}b(\pi)=0$, which gives the famous
$e^{i\pi}=-1$ equation.  The final value is the real number $-1$,
however the distance of the polygon points traversed as
$m\rightarrow\infty$ is half the circumference of a circle or $\pi$,
showing how $v$ in the equation above can be interpreted as an angle in
units of radians and therefore how $a(v)=cos(v)$ and $b(v)=sin(v)$.

   Since $e^{iv}$ is periodic (let’s assume with a period of $T$), it is
more clear to write it as $v\equiv{2{\pi}n\over T}t$ (where $n$ is an
integer), so $e^{iv}=e^{i{2{\pi}n\over T}t}$.  The advantage of this
notation is that the period ($T$) is clearly visible and the frequency
($2{\pi}n \over T$, in units of 1/cycle) is defined through the integer
$n$.  In this notation, $t$ is in units of “cycle”s.

   As we see from the examples in *note Figure 6.1: epicycle. and *note
Figure 6.2: iandtime, for each constituting frequency, we need a
respective ‘magnitude’ or the radius of the circle in order to
accurately approximate the desired 1D function.  The concepts of
“period” and “frequency” are relatively easy to grasp when using
temporal units like time because this is how we define them in every-day
life.  However, in an image (astronomical data), we are dealing with
spatial units like distance.  Therefore, by one “period” we mean the
_distance_ at which the signal is identical and frequency is defined as
the inverse of that spatial “period”.  The complex circle of *note
Figure 6.2: iandtime. can be thought of the Moon rotating about Earth
which is rotating around the Sun; so the “Real (signal)” axis shows the
Moon’s position as seen by a distant observer on the Sun as time goes
by.  Because of the scalar (not having any direction or vector) nature
of time, *note Figure 6.2: iandtime. is easier to understand in units of
time.  When thinking about spatial units, mentally replace the “Time
(sec)” axis with “Distance (meters)”.  Because length has direction and
is a vector, visualizing the rotation of the imaginary circle and the
advance along the “Distance (meters)” axis is not as simple as temporal
units like time.

 [image src="gnuastro-figures/iandtime.png" text="../gnuastro-figures//iandtime.eps" ]

Figure 6.2: Relation between the real (signal), imaginary
($i\equiv\sqrt{-1}$) and time axes at two snapshots of time.

   ---------- Footnotes ----------

   (1) Other forms of this equation were known before Euler.  For
example in 1707 A.D. (the year of Euler’s birth) Abraham de Moivre (1667
– 1754 A.D.) showed that $(\cos{x}+i\sin{x})^n=\cos(nx)+i\sin(nx)$.  In
1714 A.D., Roger Cotes (1682 – 1716 A.D. a colleague of Newton who
proofread the second edition of Principia) showed that:
$ix=\ln(\cos{x}+i\sin{x})$.


File: gnuastro.info,  Node: Fourier series,  Next: Fourier transform,  Prev: Circles and the complex plane,  Up: Frequency domain and Fourier operations

6.3.2.3 Fourier series
......................

In astronomical images, our variable (brightness, or number of
photo-electrons, or signal to be more generic) is recorded over the 2D
spatial surface of a camera pixel.  However to make things easier to
understand, here we will assume that the signal is recorded in 1D
(assume one row of the 2D image pixels).  Also for this section and the
next (*note Fourier transform::) we will be talking about the signal
before it is digitized or pixelated.  Let’s assume that we have the
continuous function $f(l)$ which is integrable in the interval $[l_0,
l_0+L]$ (always true in practical cases like images).  Take $l_0$ as the
position of the first pixel in the assumed row of the image and $L$ as
the width of the image along that row.  The units of $l_0$ and $L$ can
be in any spatial units (for example meters) or an angular unit (like
radians) multiplied by a fixed distance which is more common.

   To approximate $f(l)$ over this interval, we need to find a set of
frequencies and their corresponding ‘magnitude’s (see *note Circles and
the complex plane::).  Therefore our aim is to show $f(l)$ as the
following sum of periodic functions:

$$f(l)=\displaystyle\sum_{n=-\infty}^{\infty}c_ne^{i{2{\pi}n\over L}l} $$

Note that the different frequencies ($2{\pi}n/L$, in units of cycles per
meters for example) are not arbitrary.  They are all integer multiples
of the fundamental frequency of $\omega_0=2\pi/L$.  Recall that $L$ was
the length of the signal we want to model.  Therefore, we see that the
smallest possible frequency (or the frequency resolution) in the end,
depends on the length we observed the signal or $L$.  In the case of
each dimension on an image, this is the size of the image in the
respective dimension.  The frequencies have been defined in this
“harmonic” fashion to insure that the final sum is periodic outside of
the $[l_0, l_0+L]$ interval too.  At this point, you might be thinking
that the sky is not periodic with the same period as my camera’s view
angle.  You are absolutely right!  The important thing is that since
your camera’s observed region is the only region we are “observing” and
will be using, the rest of the sky is irrelevant; so we can safely
assume the sky is periodic outside of it.  However, this working
assumption will haunt us later in *note Edges in the frequency domain::.

   The frequencies are thus determined by definition.  So all we need to
do is to find the coefficients ($c_n$), or magnitudes, or radii of the
circles for each frequency which is identified with the integer $n$.
Fourier’s approach was to multiply both sides with a fixed term:

$$f(l)e^{-i{2{\pi}m\over L}l}=\displaystyle\sum_{n=-\infty}^{\infty}c_ne^{i{2{\pi}(n-m)\over L}l}
                                  $$

where $m>0$(1).  We can then integrate both sides over the observation
period:

           $$\int_{l_0}^{l_0+L}f(l)e^{-i{2{\pi}m\over L}l}dl
=\int_{l_0}^{l_0+L}\displaystyle\sum_{n=-\infty}^{\infty}c_ne^{i{2{\pi}(n-m)\over L}l}dl=\displaystyle\sum_{n=-\infty}^{\infty}c_n\int_{l_0}^{l_0+L}e^{i{2{\pi}(n-m)\over L}l}dl
                                  $$

Both $n$ and $m$ are positive integers.  Also, we know that a complex
exponential is periodic so after one period ($L$) it comes back to its
starting point.  Therefore $\int_{l_0}^{l_0+L}e^{2{\pi}k/L}dl=0$ for any
$k>0$.  However, when $k=0$, this integral becomes:
$\int_{l_0}^{l_0+T}e^0dt=\int_{l_0}^{l_0+T}dt=T$.  Hence since the
integral will be zero for all $n{\neq}m$, we get:

$$\displaystyle\sum_{n=-\infty}^{\infty}c_n\int_{l_0}^{l_0+T}e^{i{2{\pi}(n-m)\over L}l}dl=Lc_m $$

The origin of the axis is fundamentally an arbitrary position.  So let’s
set it to the start of the image such that $l_0=0$.  So we can find the
“magnitude” of the frequency $2{\pi}m/L$ within $f(l)$ through the
relation:

     $$c_m={1\over L}\int_{0}^{L}f(l)e^{-i{2{\pi}m\over L}l}dl $$

   ---------- Footnotes ----------

   (1) We could have assumed $m<0$ and set the exponential to positive,
but this is more clear.


File: gnuastro.info,  Node: Fourier transform,  Next: Dirac delta and comb,  Prev: Fourier series,  Up: Frequency domain and Fourier operations

6.3.2.4 Fourier transform
.........................

In *note Fourier series::, we had to assume that the function is
periodic outside of the desired interval with a period of $L$.
Therefore, assuming that $L\rightarrow\infty$ will allow us to work with
any function.  However, with this approximation, the fundamental
frequency ($\omega_0$) or the frequency resolution that we discussed in
*note Fourier series:: will tend to zero: $\omega_0\rightarrow0$.  In
the equation to find $c_m$, every $m$ represented a frequency (multiple
of $\omega_0$) and the integration on $l$ removes the dependence of the
right side of the equation on $l$, making it only a function of $m$ or
frequency.  Let’s define the following two variables:

             $$\omega{\equiv}m\omega_0={2{\pi}m\over L}$$

                       $$F(\omega){\equiv}Lc_m$$

The equation to find the coefficients of each frequency in *note Fourier
series:: thus becomes:

      $$F(\omega)=\int_{-\infty}^{\infty}f(l)e^{-i{\omega}l}dl.$$

The function $F(\omega)$ is thus the _Fourier transform_ of $f(l)$ in
the frequency domain.  So through this transformation, we can find
(analyze) the magnitudes of the constituting frequencies or the value in
the frequency space(1) of our spatial input function.  The great thing
is that we can also do the reverse and later synthesize the input
function from its Fourier transform.  Let’s do it: with the
approximations above, multiply the right side of the definition of the
Fourier Series (*note Fourier series::) with
$1=L/L=({\omega_0}L)/(2\pi)$:

                            $$f(l)={1\over
   2\pi}\displaystyle\sum_{n=-\infty}^{\infty}Lc_ne^{{2{\pi}in\over
                         L}l}\omega_0={1\over
2\pi}\displaystyle\sum_{n=-\infty}^{\infty}F(\omega)e^{i{\omega}l}\Delta\omega
                                  $$

To find the right most side of this equation, we renamed $\omega_0$ as
$\Delta\omega$ because it was our resolution, $2{\pi}n/L$ was written as
$\omega$ and finally, $Lc_n$ was written as $F(\omega)$ as we defined
above.  Now, as $L\rightarrow\infty$, $\Delta\omega\rightarrow0$ so we
can write:

                            $$f(l)={1\over
     2\pi}\int_{-\infty}^{\infty}F(\omega)e^{i{\omega}l}d\omega $$

   Together, these two equations provide us with a very powerful set of
tools that we can use to process (analyze) and recreate (synthesize) the
input signal.  Through the first equation, we can break up our input
function into its constituent frequencies and analyze it, hence it is
also known as _analysis_.  Using the second equation, we can synthesize
or make the input function from the known frequencies and their
magnitudes.  Thus it is known as _synthesis_.  Here, we symbolize the
Fourier transform (analysis) and its inverse (synthesis) of a function
$f(l)$ and its Fourier Transform $F(\omega)$ as ${\cal F}[f]$ and ${\cal
F}^{-1}[F]$.

   ---------- Footnotes ----------

   (1) As we discussed before, this ‘magnitude’ can be interpreted as
the radius of the circle rotating at this frequency in the epicyclic
interpretation of the Fourier series, see *note Figure 6.1: epicycle.
and *note Figure 6.2: iandtime.


File: gnuastro.info,  Node: Dirac delta and comb,  Next: Convolution theorem,  Prev: Fourier transform,  Up: Frequency domain and Fourier operations

6.3.2.5 Dirac delta and comb
............................

The Dirac $\delta$ (delta) function (also known as an impulse) is the
way that we convert a continuous function into a discrete one.  It is
defined to satisfy the following integral:

               $$\int_{-\infty}^{\infty}\delta(l)dl=1$$

When integrated with another function, it gives that function’s value at
$l=0$:

            $$\int_{-\infty}^{\infty}f(l)\delta(l)dt=f(0)$$

An impulse positioned at another point (say $l_0$) is written as
$\delta(l-l_0)$:

         $$\int_{-\infty}^{\infty}f(l)\delta(l-l_0)dt=f(l_0)$$

The Dirac $\delta$ function also operates similarly if we use summations
instead of integrals.  The Fourier transform of the delta function is:

$${\cal F}[\delta(l)]=\int_{-\infty}^{\infty}\delta(l)e^{-i{\omega}l}dl=e^{-i{\omega}0}=1$$

$${\cal F}[\delta(l-l_0)]=\int_{-\infty}^{\infty}\delta(l-l_0)e^{-i{\omega}l}dl=e^{-i{\omega}l_0}$$

From the definition of the Dirac $\delta$ we can also define a Dirac
comb (${\rm III}_P$) or an impulse train with infinite impulses
separated by $P$:

$${\rm III}_P(l)\equiv\displaystyle\sum_{k=-\infty}^{\infty}\delta(l-kP) $$

$P$ is chosen to represent “pixel width” later in *note Sampling
theorem::.  Therefore the Dirac comb is periodic with a period of $P$.
We have intentionally used a different name for the period of the Dirac
comb compared to the input signal’s length of observation that we showed
with $L$ in *note Fourier series::.  This difference is highlighted here
to avoid confusion later when these two periods are needed together in
*note Discrete Fourier transform::.  The Fourier transform of the Dirac
comb will be necessary in *note Sampling theorem::, so let’s derive it.
By its definition, it is periodic, with a period of $P$, so the Fourier
coefficients of its Fourier Series (*note Fourier series::) can be
calculated within one period:

$${\rm III}_P=\displaystyle\sum_{n=-\infty}^{\infty}c_ne^{i{2{\pi}n\over
                                P}l}$$

We can now find the $c_n$ from *note Fourier series:::

   $$c_n={1\over P}\int_{-P/2}^{P/2}\delta(l)e^{-i{2{\pi}n\over P}l}
             ={1\over P}\quad\quad \rightarrow \quad\quad
{\rm III}_P={1\over P}\displaystyle\sum_{n=-\infty}^{\infty}e^{i{2{\pi}n\over P}l}
                                  $$

So we can write the Fourier transform of the Dirac comb as:

$${\cal F}[{\rm III}_P]=\int_{-\infty}^{\infty}{\rm III}_Pe^{-i{\omega}l}dl
={1\over P}\displaystyle\sum_{n=-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-i(\omega-{2{\pi}n\over P})l}dl={1\over P}\displaystyle\sum_{n=-\infty}^{\infty}\delta\left(\omega-{2{\pi}n\over P}\right)
                                  $$

In the last step, we used the fact that the complex exponential is a
periodic function, that $n$ is an integer and that as we defined in
*note Fourier transform::, $\omega{\equiv}m\omega_0$, where $m$ was an
integer.  The integral will be zero for any $\omega$ that is not equal
to $2{\pi}n/P$, a more complete explanation can be seen in *note Fourier
series::.  Therefore, while in the spatial domain the impulses had
spacing of $P$ (meters for example), in the frequency space, the spacing
between the different impulses are $2\pi/P$ cycles per meters.


File: gnuastro.info,  Node: Convolution theorem,  Next: Sampling theorem,  Prev: Dirac delta and comb,  Up: Frequency domain and Fourier operations

6.3.2.6 Convolution theorem
...........................

The convolution (shown with the $\ast$ operator) of the two functions
$f(l)$ and $h(l)$ is defined as:

$$c(l)\equiv[f{\ast}h](l)=\int_{-\infty}^{\infty}f(\tau)h(l-\tau)d\tau
                                  $$

See *note Convolution process:: for a more detailed physical (pixel
based) interpretation of this definition.  The Fourier transform of
convolution ($C(\omega)$) can be written as:

  $$ C(\omega)=\int_{-\infty}^{\infty}[f{\ast}h](l)e^{-i{\omega}l}dl=
\int_{-\infty}^{\infty}f(\tau)\left[\int_{-\infty}^{\infty}h(l-\tau)e^{-i{\omega}l}dl\right]d\tau
                                  $$

To solve the inner integral, let’s define $s{\equiv}l-\tau$, so that
$ds=dl$ and $l=s+\tau$ then the inner integral becomes:

         $$\int_{-\infty}^{\infty}h(l-\tau)e^{-i{\omega}l}dl=
\int_{-\infty}^{\infty}h(s)e^{-i{\omega}(s+\tau)}ds=e^{-i{\omega}\tau}\int_{-\infty}^{\infty}h(s)e^{-i{\omega}s}ds=H(\omega)e^{-i{\omega}\tau}
                                  $$

where $H(\omega)$ is the Fourier transform of $h(l)$.  Substituting this
result for the inner integral above, we get:

$$C(\omega)=H(\omega)\int_{-\infty}^{\infty}f(\tau)e^{-i{\omega}\tau}d\tau=H(\omega)F(\omega)=F(\omega)H(\omega)
                                  $$

where $F(\omega)$ is the Fourier transform of $f(l)$.  So multiplying
the Fourier transform of two functions individually, we get the Fourier
transform of their convolution.  The convolution theorem also proves a
relation between the convolutions in the frequency space.  Let’s define:

             $$D(\omega){\equiv}F(\omega){\ast}H(\omega)$$

Applying the inverse Fourier Transform or synthesis equation (*note
Fourier transform::) to both sides and following the same steps above,
we get:

                           $$d(l)=f(l)h(l)$$

Where $d(l)$ is the inverse Fourier transform of $D(\omega)$.  We can
therefore re-write the two equations above formally as the convolution
theorem:

             $$ {\cal F}[f{\ast}h]={\cal F}[f]{\cal F}[h]
                                  $$

              $$ {\cal F}[fh]={\cal F}[f]\ast{\cal F}[h]
                                  $$

   Besides its usefulness in blurring an image by convolving it with a
given kernel, the convolution theorem also enables us to do another very
useful operation in data analysis: to match the blur (or PSF) between
two images taken with different telescopes/cameras or under different
atmospheric conditions.  This process is also known as de-convolution.
Let’s take $f(l)$ as the image with a narrower PSF (less blurry) and
$c(l)$ as the image with a wider PSF which appears more blurred.  Also
let’s take $h(l)$ to represent the kernel that should be convolved with
the sharper image to create the more blurry image.  Above, we proved the
relation between these three images through the convolution theorem.
But there, we assumed that $f(l)$ and $h(l)$ are known (given) and the
convolved image is desired.

   In de-convolution, we have $f(l)$ –the sharper image– and $f*h(l)$
–the more blurry image– and we want to find the kernel $h(l)$.  The
solution is a direct result of the convolution theorem:

         $$ {\cal F}[h]={{\cal F}[f{\ast}h]\over {\cal F}[f]}
                              \quad\quad
                               {\rm or}
                              \quad\quad
 h(l)={\cal F}^{-1}\left[{{\cal F}[f{\ast}h]\over {\cal F}[f]}\right]
                                  $$

   While this works really nice, it has two problems:

   • If ${\cal F}[f]$ has any zero values, then the inverse Fourier
     transform will not be a number!

   • If there is significant noise in the image, then the high
     frequencies of the noise are going to significantly reduce the
     quality of the final result.

   A standard solution to both these problems is the Weiner
de-convolution algorithm(1).

   ---------- Footnotes ----------

   (1) <https://en.wikipedia.org/wiki/Wiener_deconvolution>


File: gnuastro.info,  Node: Sampling theorem,  Next: Discrete Fourier transform,  Prev: Convolution theorem,  Up: Frequency domain and Fourier operations

6.3.2.7 Sampling theorem
........................

Our mathematical functions are continuous, however, our data collecting
and measuring tools are discrete.  Here we want to give a mathematical
formulation for digitizing the continuous mathematical functions so that
later, we can retrieve the continuous function from the digitized
recorded input.  Assuming that we have a continuous function $f(l)$,
then we can define $f_s(l)$ as the ‘sampled’ $f(l)$ through the Dirac
comb (see *note Dirac delta and comb::):

$$f_s(l)=f(l){\rm III}_P=\displaystyle\sum_{n=-\infty}^{\infty}f(l)\delta(l-nP)
                                  $$

The discrete data-element $f_k$ (for example, a pixel in an image),
where $k$ is an integer, can thus be represented as:

$$f_k=\int_{-\infty}^{\infty}f_s(l)dl=\int_{-\infty}^{\infty}f(l)\delta(l-kP)dt=f(kP)$$

   Note that in practice, our discrete data points are not found in this
fashion.  Each detector pixel (in an image for example) has an area and
averages the signal it receives over that area, not a mathematical point
as the Dirac $\delta$ function defines.  However, as long as the
variation in the signal over one detector pixel is not significant, this
can be a good approximation.  Having put this issue to the side, we can
now try to find the relation between the Fourier transforms of the
un-sampled $f(l)$ and the sampled $f_s(l)$.  For a more clear notation,
let’s define:

                  $$F_s(\omega)\equiv{\cal F}[f_s]$$

               $$D(\omega)\equiv{\cal F}[{\rm III}_P]$$

Then using the Convolution theorem (see *note Convolution theorem::),
$F_s(\omega)$ can be written as:

  $$F_s(\omega)={\cal F}[f(l){\rm III}_P]=F(\omega){\ast}D(\omega)$$

Finally, from the definition of convolution and the Fourier transform of
the Dirac comb (see *note Dirac delta and comb::), we get:

                              $$\eqalign{
 F_s(\omega) &= \int_{-\infty}^{\infty}F(\omega)D(\omega-\mu)d\mu \cr
&= {1\over P}\displaystyle\sum_{n=-\infty}^{\infty}\int_{-\infty}^{\infty}F(\omega)\delta\left(\omega-\mu-{2{\pi}n\over P}\right)d\mu \cr
      &= {1\over P}\displaystyle\sum_{n=-\infty}^{\infty}F\left(
                 \omega-{2{\pi}n\over P}\right).\cr }
                                  $$

   $F(\omega)$ was only a simple function, see *note Figure 6.3:
samplingfreq.(left).  However, from the sampled Fourier transform
function we see that $F_s(\omega)$ is the superposition of infinite
copies of $F(\omega)$ that have been shifted, see *note Figure 6.3:
samplingfreq.(right).  From the equation, it is clear that the shift in
each copy is $2\pi/P$.

 [image src="gnuastro-figures/samplingfreq.png" text="../gnuastro-figures//samplingfreq.eps" ]

Figure 6.3: Sampling causes infinite repetition in the frequency domain.
FT is an abbreviation for ‘Fourier transform’.  $\omega_m$ represents
the maximum frequency present in the input.  $F(\omega)$ is only
symmetric on both sides of 0 when the input is real (not complex).  In
general $F(\omega)$ is complex and thus cannot be simply plotted like
this.  Here we have assumed a real Gaussian $f(t)$ which has produced a
Gaussian $F(\omega)$.

   The input $f(l)$ can have any distribution of frequencies in it.  In
the example of *note Figure 6.3: samplingfreq.(left), the input
consisted of a range of frequencies equal to $\Delta\omega=2\omega_m$.
Fortunately as *note Figure 6.3: samplingfreq.(right) shows, the assumed
pixel size ($P$) we used to sample this hypothetical function was such
that $2\pi/P>\Delta\omega$.  The consequence is that each copy of
$F(\omega)$ has become completely separate from the surrounding copies.
Such a digitized (sampled) data set is thus called _over-sampled_.  When
$2\pi/P=\Delta\omega$, $P$ is just small enough to finely separate even
the largest frequencies in the input signal and thus it is known as
_critically-sampled_.  Finally if $2\pi/P<\Delta\omega$ we are dealing
with an _under-sampled_ data set.  In an under-sampled data set, the
separate copies of $F(\omega)$ are going to overlap and this will
deprive us of recovering high constituent frequencies of $f(l)$.  The
effects of under-sampling in an image with high rates of change (for
example a brick wall imaged from a distance) can clearly be visually
seen and is known as _aliasing_.

   When the input $f(l)$ is composed of a finite range of frequencies,
$f(l)$ is known as a _band-limited_ function.  The example in *note
Figure 6.3: samplingfreq.(left) was a nice demonstration of such a case:
for all $\omega<-\omega_m$ or $\omega>\omega_m$, we have $F(\omega)=0$.
Therefore, when the input function is band-limited and our detector’s
pixels are placed such that we have critically (or over-) sampled it,
then we can exactly reproduce the continuous $f(l)$ from the discrete or
digitized samples.  To do that, we just have to isolate one copy of
$F(\omega)$ from the infinite copies and take its inverse Fourier
transform.

   This ability to exactly reproduce the continuous input from the
sampled or digitized data leads us to the _sampling theorem_ which
connects the inherent property of the continuous signal (its maximum
frequency) to that of the detector (the spacing between its pixels).
The sampling theorem states that the full (continuous) signal can be
recovered when the pixel size ($P$) and the maximum constituent
frequency in the signal ($\omega_m$) have the following relation(1):

                      $${2\pi\over P}>2\omega_m$$

This relation was first formulated by Harry Nyquist (1889 – 1976 A.D.)
in 1928 and formally proved in 1949 by Claude E. Shannon (1916 – 2001
A.D.) in what is now known as the Nyquist-Shannon sampling theorem.  In
signal processing, the signal is produced (synthesized) by a transmitter
and is received and de-coded (analyzed) by a receiver.  Therefore
producing a band-limited signal is necessary.

   In astronomy, we do not produce the shapes of our targets, we are
only observers.  Galaxies can have any shape and size, therefore
ideally, our signal is not band-limited.  However, since we are always
confined to observing through an aperture, the aperture will cause a
point source (for which $\omega_m=\infty$) to be spread over several
pixels.  This spread is quantitatively known as the point spread
function or PSF. This spread does blur the image which is undesirable;
however, for this analysis it produces the positive outcome that there
will be a finite $\omega_m$.  Though we should caution that any detector
will have noise which will add lots of very high frequency (ideally
infinite) changes between the pixels.  However, the coefficients of
those noise frequencies are usually exceedingly small.

   ---------- Footnotes ----------

   (1) This equation is also shown in some places without the $2\pi$.
Whether $2\pi$ is included or not depends on how you define the
frequency


File: gnuastro.info,  Node: Discrete Fourier transform,  Next: Fourier operations in two dimensions,  Prev: Sampling theorem,  Up: Frequency domain and Fourier operations

6.3.2.8 Discrete Fourier transform
..................................

As we have stated several times so far, the input image is a digitized,
pixelated or discrete array of values ($f_s(l)$, see *note Sampling
theorem::).  The input is not a continuous function.  Also, all our
numerical calculations can only be done on a sampled, or discrete
Fourier transform.  Note that $F_s(\omega)$ is not discrete, it is
continuous.  One way would be to find the analytic $F_s(\omega)$, then
sample it at any desired “freq-pixel”(1) spacing.  However, this process
would involve two steps of operations and computers in particular are
not too good at analytic operations for the first step.  So here, we
will derive a method to directly find the ‘freq-pixel’ated $F_s(\omega)$
from the pixelated $f_s(l)$.  Let’s start with the definition of the
Fourier transform (see *note Fourier transform::):

    $$F_s(\omega)=\int_{-\infty}^{\infty}f_s(l)e^{-i{\omega}l}dl $$

From the definition of $f_s(\omega)$ (using $x$ instead of $n$) we get:

                              $$\eqalign{
         F_s(\omega) &= \displaystyle\sum_{x=-\infty}^{\infty}
     \int_{-\infty}^{\infty}f(l)\delta(l-xP)e^{-i{\omega}l}dl \cr
               &= \displaystyle\sum_{x=-\infty}^{\infty}
                          f_xe^{-i{\omega}xP}
                                   }
                                  $$

Where $f_x$ is the value of $f(l)$ on the point $x$ or the value of the
$x$th pixel.  As shown in *note Sampling theorem:: this function is
infinitely periodic with a period of $2\pi/P$.  So all we need is the
values within one period: $0<\omega<2\pi/P$, see *note Figure 6.3:
samplingfreq.  We want $X$ samples within this interval, so the
frequency difference between each frequency sample or freq-pixel is
$1/XP$.  Hence we will evaluate the equation above on the points at:

        $$\omega={u\over XP} \quad\quad u = 0, 1, 2, ..., X-1$$

Therefore the value of the freq-pixel $u$ in the frequency domain is:

      $$F_u=\displaystyle\sum_{x=0}^{X-1} f_xe^{-i{ux\over X}} $$

Therefore, we see that for each freq-pixel in the frequency domain, we
are going to need all the pixels in the spatial domain(2).  If the input
(spatial) pixel row is also $X$ pixels wide, then we can exactly recover
the $x$th pixel with the following summation:

 $$f_x={1\over X}\displaystyle\sum_{u=0}^{X-1} F_ue^{i{ux\over X}} $$

   When the input pixel row (we are still only working on 1D data) has
$X$ pixels, then it is $L=XP$ spatial units wide.  $L$, or the length of
the input data was defined in *note Fourier series:: and $P$ or the
space between the pixels in the input was defined in *note Dirac delta
and comb::.  As we saw in *note Sampling theorem::, the input (spatial)
pixel spacing ($P$) specifies the range of frequencies that can be
studied and in *note Fourier series:: we saw that the length of the
(spatial) input, ($L$) determines the resolution (or size of the
freq-pixels) in our discrete Fourier transformed image.  Both result
from the fact that the frequency domain is the inverse of the spatial
domain.

   ---------- Footnotes ----------

   (1) We are using the made-up word “freq-pixel” so they are not
confused with spatial domain “pixels”.

   (2) So even if one pixel is a blank pixel (see *note Blank pixels::),
all the pixels in the frequency domain will also be blank.


File: gnuastro.info,  Node: Fourier operations in two dimensions,  Next: Edges in the frequency domain,  Prev: Discrete Fourier transform,  Up: Frequency domain and Fourier operations

6.3.2.9 Fourier operations in two dimensions
............................................

Once all the relations in the previous sections have been clearly
understood in one dimension, it is very easy to generalize them to two
or even more dimensions since each dimension is by definition
independent.  Previously we defined $l$ as the continuous variable in 1D
and the inverse of the period in its direction to be $\omega$.  Let’s
show the second spatial direction with $m$ the inverse of the period in
the second dimension with $\nu$.  The Fourier transform in 2D (see *note
Fourier transform::) can be written as:

    $$F(\omega, \nu)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
                  f(l, m)e^{-i({\omega}l+{\nu}m)}dl$$

       $$f(l, m)=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
               F(\omega, \nu)e^{i({\omega}l+{\nu}m)}dl$$

   The 2D Dirac $\delta(l,m)$ is non-zero only when $l=m=0$.  The 2D
Dirac comb (or Dirac brush!  See *note Dirac delta and comb::) can be
written in units of the 2D Dirac $\delta$.  For most image detectors,
the sides of a pixel are equal in both dimensions.  So $P$ remains
unchanged, if a specific device is used which has non-square pixels,
then for each dimension a different value should be used.

    $${\rm III}_P(l, m)\equiv\displaystyle\sum_{j=-\infty}^{\infty}
                \displaystyle\sum_{k=-\infty}^{\infty}
                         \delta(l-jP, m-kP) $$

   The Two dimensional Sampling theorem (see *note Sampling theorem::)
is thus very easily derived as before since the frequencies in each
dimension are independent.  Let’s take $\nu_m$ as the maximum frequency
along the second dimension.  Therefore the two dimensional sampling
theorem says that a 2D band-limited function can be recovered when the
following conditions hold(1):

         $${2\pi\over P} > 2\omega_m \quad\quad\quad {\rm and}
               \quad\quad\quad {2\pi\over P} > 2\nu_m$$

   Finally, let’s represent the pixel counter on the second dimension in
the spatial and frequency domains with $y$ and $v$ respectively.  Also
let’s assume that the input image has $Y$ pixels on the second
dimension.  Then the two dimensional discrete Fourier transform and its
inverse (see *note Discrete Fourier transform::) can be written as:

 $$F_{u,v}=\displaystyle\sum_{x=0}^{X-1}\displaystyle\sum_{y=0}^{Y-1}
               f_{x,y}e^{-i({ux\over X}+{vy\over Y})} $$

$$f_{x,y}={1\over XY}\displaystyle\sum_{u=0}^{X-1}\displaystyle\sum_{v=0}^{Y-1}
               F_{u,v}e^{i({ux\over X}+{vy\over Y})} $$

   ---------- Footnotes ----------

   (1) If the pixels are not a square, then each dimension has to use
the respective pixel size, but since most detectors have square pixels,
we assume so here too


File: gnuastro.info,  Node: Edges in the frequency domain,  Prev: Fourier operations in two dimensions,  Up: Frequency domain and Fourier operations

6.3.2.10 Edges in the frequency domain
......................................

With a good grasp of the frequency domain, we can revisit the problem of
convolution on the image edges, see *note Edges in the spatial domain::.
When we apply the convolution theorem (see *note Convolution theorem::)
to convolve an image, we first take the discrete Fourier transforms
(DFT, *note Discrete Fourier transform::) of both the input image and
the kernel, then we multiply them with each other and then take the
inverse DFT to construct the convolved image.  Of course, in order to
multiply them with each other in the frequency domain, the two images
have to be the same size, so let’s assume that we pad the kernel (it is
usually smaller than the input image) with zero valued pixels in both
dimensions so it becomes the same size as the input image before the
DFT.

   Having multiplied the two DFTs, we now apply the inverse DFT which is
where the problem is usually created.  If the DFT of the kernel only had
values of 1 (unrealistic condition!)  then there would be no problem and
the inverse DFT of the multiplication would be identical with the input.
However in real situations, the kernel’s DFT has a maximum of 1 (because
the sum of the kernel has to be one, see *note Convolution process::)
and decreases something like the hypothetical profile of *note Figure
6.3: samplingfreq.  So when multiplied with the input image’s DFT, the
coefficients or magnitudes (see *note Circles and the complex plane::)
of the smallest frequency (or the sum of the input image pixels) remains
unchanged, while the magnitudes of the higher frequencies are
significantly reduced.

   As we saw in *note Sampling theorem::, the Fourier transform of a
discrete input will be infinitely repeated.  In the final inverse DFT
step, the input is in the frequency domain (the multiplied DFT of the
input image and the kernel DFT). So the result (our output convolved
image) will be infinitely repeated in the spatial domain.  In order to
accurately reconstruct the input image, we need all the frequencies with
the correct magnitudes.  However, when the magnitudes of higher
frequencies are decreased, longer periods (shorter frequencies) will
dominate in the reconstructed pixel values.  Therefore, when
constructing a pixel on the edge of the image, the newly empowered
longer periods will look beyond the input image edges and will find the
repeated input image there.  So if you convolve an image in this fashion
using the convolution theorem, when a bright object exists on one edge
of the image, its blurred wings will be present on the other side of the
convolved image.  This is often termed as circular convolution or cyclic
convolution.

   So, as long as we are dealing with convolution in the frequency
domain, there is nothing we can do about the image edges.  The least we
can do is to eliminate the ghosts of the other side of the image.  So,
we add zero valued pixels to both the input image and the kernel in both
dimensions so the image that will be convolved has a size equal to the
sum of both images in each dimension.  Of course, the effect of this
zero-padding is that the sides of the output convolved image will become
dark.  To put it another way, the edges are going to drain the flux from
nearby objects.  But at least it is consistent across all the edges of
the image and is predictable.  In Convolve, you can see the padded
images when inspecting the frequency domain convolution steps with the
‘--viewfreqsteps’ option.


File: gnuastro.info,  Node: Spatial vs. Frequency domain,  Next: Convolution kernel,  Prev: Frequency domain and Fourier operations,  Up: Convolve

6.3.3 Spatial vs. Frequency domain
----------------------------------

With the discussions above it might not be clear when to choose the
spatial domain and when to choose the frequency domain.  Here we will
try to list the benefits of each.

The spatial domain,
   • Can correct for the edge effects of convolution, see *note Edges in
     the spatial domain::.

   • Can operate on blank pixels.

   • Can be faster than frequency domain when the kernel is small (in
     terms of the number of pixels on the sides).

The frequency domain,
   • Will be much faster when the image and kernel are both large.

As a general rule of thumb, when working on an image of modeled profiles
use the frequency domain and when working on an image of real (observed)
objects use the spatial domain (corrected for the edges).  The reason is
that if you apply a frequency domain convolution to a real image, you
are going to loose information on the edges and generally you don’t want
large kernels.  But when you have made the profiles in the image
yourself, you can just make a larger input image and crop the central
parts to completely remove the edge effect, see *note If convolving
afterwards::.  Also due to oversampling, both the kernels and the images
can become very large and the speed boost of frequency domain
convolution will significantly improve the processing time, see *note
Oversampling::.


File: gnuastro.info,  Node: Convolution kernel,  Next: Invoking astconvolve,  Prev: Spatial vs. Frequency domain,  Up: Convolve

6.3.4 Convolution kernel
------------------------

All the programs that need convolution will need to be given a
convolution kernel file and extension.  In most cases (other than
Convolve, see *note Convolve::) the kernel file name is optional.
However, the extension is necessary and must be specified either on the
command-line or at least one of the configuration files (see *note
Configuration files::).  Within Gnuastro, there are two ways to create a
kernel image:

   • MakeProfiles: You can use MakeProfiles to create a parametric
     (based on a radial function) kernel, see *note MakeProfiles::.  By
     default MakeProfiles will make the Gaussian and Moffat profiles in
     a separate file so you can feed it into any of the programs.

   • ConvertType: You can write your own desired kernel into a text file
     table and convert it to a FITS file with ConvertType, see *note
     ConvertType::.  Just be careful that the kernel has to have an odd
     number of pixels along its two axes, see *note Convolution
     process::.  All the programs that do convolution will normalize the
     kernel internally, so if you choose this option, you don’t have to
     worry about normalizing the kernel.  Only within Convolve, there is
     an option to disable normalization, see *note Invoking
     astconvolve::.

The two options to specify a kernel file name and its extension are
shown below.  These are common between all the programs that will do
convolution.
‘-k STR’
‘--kernel=STR’
     The convolution kernel file name.  The ‘BITPIX’ (data type) value
     of this file can be any standard type and it does not necessarily
     have to be normalized.  Several operations will be done on the
     kernel image prior to the program’s processing:

        • It will be converted to floating point type.

        • All blank pixels (see *note Blank pixels::) will be set to
          zero.

        • It will be normalized so the sum of its pixels equal unity.

        • It will be flipped so the convolved image has the same
          orientation.  This is only relevant if the kernel is not
          circular.  See *note Convolution process::.

‘-U STR’
‘--khdu=STR’
     The convolution kernel HDU. Although the kernel file name is
     optional, before running any of the programs, they need to have a
     value for ‘--khdu’ even if the default kernel is to be used.  So be
     sure to keep its value in at least one of the configuration files
     (see *note Configuration files::).  By default, the system
     configuration file has a value.


File: gnuastro.info,  Node: Invoking astconvolve,  Prev: Convolution kernel,  Up: Convolve

6.3.5 Invoking Convolve
-----------------------

Convolve an input dataset (2D image or 1D spectrum for example) with a
known kernel, or make the kernel necessary to match two PSFs.  The
general template for Convolve is:

     $ astconvolve [OPTION...] ASTRdata

One line examples:

     ## Convolve mockimg.fits with psf.fits:
     $ astconvolve --kernel=psf.fits mockimg.fits

     ## Convolve in the spatial domain:
     $ astconvolve observedimg.fits --kernel=psf.fits --domain=spatial

     ## Find the kernel to match sharper and blurry PSF images:
     $ astconvolve --kernel=sharperimage.fits --makekernel=10           \
                   blurryimage.fits

     ## Convolve a Spectrum (column 14 in the FITS table below) with a
     ## custom kernel (the kernel will be normalized internally, so only
     ## the ratios are important). Sed is used to replace the spaces with
     ## new line characters so Convolve sees them as values in one column.
     $ echo "1 3 10 3 1" | sed 's/ /\n/g' | astconvolve spectra.fits -c14

   The only argument accepted by Convolve is an input image file.  Some
of the options are the same between Convolve and some other Gnuastro
programs.  Therefore, to avoid repetition, they will not be repeated
here.  For the full list of options shared by all Gnuastro programs,
please see *note Common options::.  In particular, in the spatial
domain, on a multi-dimensional datasets, convolve uses Gnuastro’s
tessellation to speed up the run, see *note Tessellation::.  Common
options related to tessellation are described in in *note Processing
options::.

   1-dimensional datasets (for example spectra) are only read as columns
within a table (see *note Tables:: for more on how Gnuastro programs
read tables).  Note that currently 1D convolution is only implemented in
the spatial domain and thus kernel-matching is also not supported.

   Here we will only explain the options particular to Convolve.  Run
Convolve with ‘--help’ in order to see the full list of options Convolve
accepts, irrespective of where they are explained in this book.

‘--kernelcolumn’
     Column containing the 1D kernel.  When the input dataset is a
     1-dimensional column, and the host table has more than one column,
     use this option to specify which column should be used.

‘--nokernelflip’
     Do not flip the kernel after reading it the spatial domain
     convolution.  This can be useful if the flipping has already been
     applied to the kernel.

‘--nokernelnormx’
     Do not normalize the kernel after reading it, such that the sum of
     its pixels is unity.

‘-d STR’
‘--domain=STR’
     The domain to use for the convolution.  The acceptable values are
     ‘‘spatial’’ and ‘‘frequency’’, corresponding to the respective
     domain.

     For large images, the frequency domain process will be more
     efficient than convolving in the spatial domain.  However, the
     edges of the image will loose some flux (see *note Edges in the
     spatial domain::) and the image must not contain any blank pixels,
     see *note Spatial vs. Frequency domain::.

‘--checkfreqsteps’
     With this option a file with the initial name of the output file
     will be created that is suffixed with ‘_freqsteps.fits’, all the
     steps done to arrive at the final convolved image are saved as
     extensions in this file.  The extensions in order are:

       1. The padded input image.  In frequency domain convolution the
          two images (input and convolved) have to be the same size and
          both should be padded by zeros.

       2. The padded kernel, similar to the above.

       3. The Fourier spectrum of the forward Fourier transform of the
          input image.  Note that the Fourier transform is a complex
          operation (and not view able in one image!)  So we either have
          to show the ‘Fourier spectrum’ or the ‘Phase angle’.  For the
          complex number $a+ib$, the Fourier spectrum is defined as
          $\sqrt{a^2+b^2}$ while the phase angle is defined as
          $\arctan(b/a)$.

       4. The Fourier spectrum of the forward Fourier transform of the
          kernel image.

       5. The Fourier spectrum of the multiplied (through complex
          arithmetic) transformed images.

       6. The inverse Fourier transform of the multiplied image.  If you
          open it, you will see that the convolved image is now in the
          center, not on one side of the image as it started with (in
          the padded image of the first extension).  If you are working
          on a mock image which originally had pixels of precisely 0.0,
          you will notice that in those parts that your convolved
          profile(s) did not convert, the values are now $\sim10^{-18}$,
          this is due to floating-point round off errors.  Therefore in
          the final step (when cropping the central parts of the image),
          we also remove any pixel with a value less than $10^{-17}$.

‘--noedgecorrection’
     Do not correct the edge effect in spatial domain convolution.  For
     a full discussion, please see *note Edges in the spatial domain::.

‘-m INT’
‘--makekernel=INT’
     (‘=INT’) If this option is called, Convolve will do de-convolution
     (see *note Convolution theorem::).  The image specified by the
     ‘--kernel’ option is assumed to be the sharper (less blurry) image
     and the input image is assumed to be the more blurry image.  The
     value given to this option will be used as the maximum radius of
     the kernel.  Any pixel in the final kernel that is larger than this
     distance from the center will be set to zero.  The two images must
     have the same size.

     Noise has large frequencies which can make the result less reliable
     for the higher frequencies of the final result.  So all the
     frequencies which have a spectrum smaller than the value given to
     the ‘minsharpspec’ option in the sharper input image are set to
     zero and not divided.  This will cause the wings of the final
     kernel to be flatter than they would ideally be which will make the
     convolved image result unreliable if it is too high.  Some notes to
     take into account for a good result:

        • Choose a bright (unsaturated) star and use a region box (with
          Crop for example, see *note Crop::) that is sufficiently above
          the noise.

        • Use Warp (see *note Warp::) to warp the pixel grid so the
          star’s center is exactly on the center of the central pixel in
          the cropped image.  This will certainly slightly degrade the
          result, however, it is necessary.  If there are multiple good
          stars, you can shift all of them, then normalize them (so the
          sum of each star’s pixels is one) and then take their average
          to decrease this effect.

        • The shifting might move the center of the star by one pixel in
          any direction, so crop the central pixel of the warped image
          to have a clean image for the de-convolution.

     Note that this feature is not yet supported in 1-dimensional
     datasets.

‘-c’
‘--minsharpspec’
     (‘=FLT’) The minimum frequency spectrum (or coefficient, or pixel
     value in the frequency domain image) to use in deconvolution, see
     the explanations under the ‘--makekernel’ option for more
     information.


File: gnuastro.info,  Node: Warp,  Prev: Convolve,  Up: Data manipulation

6.4 Warp
========

Image warping is the process of mapping the pixels of one image onto a
new pixel grid.  This process is sometimes known as transformation,
however following the discussion of Heckbert 1989(1) we will not be
using that term because it can be confused with only pixel value or flux
transformations.  Here we specifically mean the pixel grid
transformation which is better conveyed with ‘warp’.

   Image wrapping is a very important step in astronomy, both in
observational data analysis and in simulating modeled images.  In
modeling, warping an image is necessary when we want to apply grid
transformations to the initial models, for example in simulating
gravitational lensing (Radial warpings are not yet included in Warp).
Observational reasons for warping an image are listed below:

   • *Noise:* Most scientifically interesting targets are inherently
     faint (have a very low Signal to noise ratio).  Therefore one short
     exposure is not enough to detect such objects that are drowned
     deeply in the noise.  We need multiple exposures so we can add them
     together and increase the objects’ signal to noise ratio.  Keeping
     the telescope fixed on one field of the sky is practically
     impossible.  Therefore very deep observations have to put into the
     same grid before adding them.

   • *Resolution:* If we have multiple images of one patch of the sky
     (hopefully at multiple orientations) we can warp them to the same
     grid.  The multiple orientations will allow us to ‘guess’ the
     values of pixels on an output pixel grid that has smaller pixel
     sizes and thus increase the resolution of the output.  This process
     of merging multiple observations is known as Mosaicing.

   • *Cosmic rays:* Cosmic rays can randomly fall on any part of an
     image.  If they collide vertically with the camera, they are going
     to create a very sharp and bright spot that in most cases can be
     separated easily(2).  However, depending on the depth of the camera
     pixels, and the angle that a cosmic rays collides with it, it can
     cover a line-like larger area on the CCD which makes the detection
     using their sharp edges very hard and error prone.  One of the best
     methods to remove cosmic rays is to compare multiple images of the
     same field.  To do that, we need all the images to be on the same
     pixel grid.

   • *Optical distortion:* (Not yet included in Warp) In wide field
     images, the optical distortion that occurs on the outer parts of
     the focal plane will make accurate comparison of the objects at
     various locations impossible.  It is therefore necessary to warp
     the image and correct for those distortions prior to the analysis.

   • *Detector not on focal plane:* In some cases (like the Hubble Space
     Telescope ACS and WFC3 cameras), the CCD might be tilted compared
     to the focal plane, therefore the recorded CCD pixels have to be
     projected onto the focal plane before further analysis.

* Menu:

* Warping basics::              Basics of coordinate transformation.
* Merging multiple warpings::   How to merge multiple matrices.
* Resampling::                  Warping an image is re-sampling it.
* Invoking astwarp::            Arguments and options for Warp.

   ---------- Footnotes ----------

   (1) Paul S. Heckbert.  1989.  _Fundamentals of Texture mapping and
Image Warping_, Master’s thesis at University of California, Berkeley.

   (2) All astronomical targets are blurred with the PSF, see *note
PSF::, however a cosmic ray is not and so it is very sharp (it suddenly
stops at one pixel).


File: gnuastro.info,  Node: Warping basics,  Next: Merging multiple warpings,  Prev: Warp,  Up: Warp

6.4.1 Warping basics
--------------------

Let’s take $\left[\matrix{u&v}\right]$ as the coordinates of a point in
the input image and $\left[\matrix{x&y}\right]$ as the coordinates of
that same point in the output image(1).  The simplest form of coordinate
transformation (or warping) is the scaling of the coordinates, let’s
assume we want to scale the first axis by $M$ and the second by $N$, the
output coordinates of that point can be calculated by

                    $$\left[\matrix{x\cr y}\right]=
                    \left[\matrix{Mu\cr Nv}\right]=
     \left[\matrix{M&0\cr0&N}\right]\left[\matrix{u\cr v}\right]$$

Note that these are matrix multiplications.  We thus see that we can
represent any such grid warping as a matrix.  Another thing we can do
with this $2\times2$ matrix is to rotate the output coordinate around
the common center of both coordinates.  If the output is rotated
anticlockwise by $\theta$ degrees from the positive (to the right)
horizontal axis, then the warping matrix should become:

                    $$\left[\matrix{x\cr y}\right]=
 \left[\matrix{ucos\theta-vsin\theta\cr usin\theta+vcos\theta}\right]=
   \left[\matrix{cos\theta&-sin\theta\cr sin\theta&cos\theta}\right]
                     \left[\matrix{u\cr v}\right]
                                  $$

We can also flip the coordinates around the first axis, the second axis
and the coordinate center with the following three matrices
respectively:

             $$\left[\matrix{1&0\cr0&-1}\right]\quad\quad
              \left[\matrix{-1&0\cr0&1}\right]\quad\quad
                  \left[\matrix{-1&0\cr0&-1}\right]$$

The final thing we can do with this definition of a $2\times2$ warping
matrix is shear.  If we want the output to be sheared along the first
axis with $A$ and along the second with $B$, then we can use the matrix:

                 $$\left[\matrix{1&A\cr B&1}\right]$$

To have one matrix representing any combination of these steps, you use
matrix multiplication, see *note Merging multiple warpings::.  So any
combinations of these transformations can be displayed with one
$2\times2$ matrix:

                 $$\left[\matrix{a&b\cr c&d}\right]$$

   The transformations above can cover a lot of the needs of most
coordinate transformations.  However they are limited to mapping the
point $[\matrix{0&0}]$ to $[\matrix{0&0}]$.  Therefore they are useless
if you want one coordinate to be shifted compared to the other one.
They are also space invariant, meaning that all the coordinates in the
image will receive the same transformation.  In other words, all the
pixels in the output image will have the same area if placed over the
input image.  So transformations which require varying output pixel
sizes like projections cannot be applied through this $2\times2$ matrix
either (for example for the tilted ACS and WFC3 camera detectors on
board the Hubble space telescope).

   To add these further capabilities, namely translation and projection,
we use the homogeneous coordinates.  They were defined about 200 years
ago by August Ferdinand Möbius (1790 – 1868).  For simplicity, we will
only discuss points on a 2D plane and avoid the complexities of higher
dimensions.  We cannot provide a deep mathematical introduction here,
interested readers can get a more detailed explanation from Wikipedia(2)
and the references therein.

   By adding an extra coordinate to a point we can add the flexibility
we need.  The point $[\matrix{x&y}]$ can be represented as
$[\matrix{xZ&yZ&Z}]$ in homogeneous coordinates.  Therefore multiplying
all the coordinates of a point in the homogeneous coordinates with a
constant will give the same point.  Put another way, the point
$[\matrix{x&y&Z}]$ corresponds to the point $[\matrix{x/Z&y/Z}]$ on the
constant $Z$ plane.  Setting $Z=1$, we get the input image plane, so
$[\matrix{u&v&1}]$ corresponds to $[\matrix{u&v}]$.  With this
definition, the transformations above can be generally written as:

                 $$\left[\matrix{x\cr y\cr 1}\right]=
             \left[\matrix{a&b&0\cr c&d&0\cr 0&0&1}\right]
                  \left[\matrix{u\cr v\cr 1}\right]$$

We thus acquired 4 extra degrees of freedom.  By giving non-zero values
to the zero valued elements of the last column we can have translation
(try the matrix multiplication!).  In general, any coordinate
transformation that is represented by the matrix below is known as an
affine transformation(3):

           $$\left[\matrix{a&b&c\cr d&e&f\cr 0&0&1}\right]$$

   We can now consider translation, but the affine transform is still
spatially invariant.  Giving non-zero values to the other two elements
in the matrix above gives us the projective transformation or
Homography(4) which is the most general type of transformation with the
$3\times3$ matrix:

                $$\left[\matrix{x'\cr y'\cr w}\right]=
             \left[\matrix{a&b&c\cr d&e&f\cr g&h&1}\right]
                  \left[\matrix{u\cr v\cr 1}\right]$$

So the output coordinates can be calculated from:

     $$x={x' \over w}={au+bv+c \over gu+hv+1}\quad\quad\quad\quad
               y={y' \over w}={du+ev+f \over gu+hv+1}$$

   Thus with Homography we can change the sizes of the output pixels on
the input plane, giving a ‘perspective’-like visual impression.  This
can be quantitatively seen in the two equations above.  When $g=h=0$,
the denominator is independent of $u$ or $v$ and thus we have spatial
invariance.  Homography preserves lines at all orientations.  A very
useful fact about Homography is that its inverse is also a Homography.
These two properties play a very important role in the implementation of
this transformation.  A short but instructive and illustrated review of
affine, projective and also bi-linear mappings is provided in Heckbert
1989(5).

   ---------- Footnotes ----------

   (1) These can be any real number, we are not necessarily talking
about integer pixels here.

   (2) <http://en.wikipedia.org/wiki/Homogeneous_coordinates>

   (3) <http://en.wikipedia.org/wiki/Affine_transformation>

   (4) <http://en.wikipedia.org/wiki/Homography>

   (5) Paul S. Heckbert.  1989.  _Fundamentals of Texture mapping and
Image Warping_, Master’s thesis at University of California, Berkeley.
Note that since points are defined as row vectors there, the matrix is
the transpose of the one discussed here.


File: gnuastro.info,  Node: Merging multiple warpings,  Next: Resampling,  Prev: Warping basics,  Up: Warp

6.4.2 Merging multiple warpings
-------------------------------

In *note Warping basics:: we saw how a basic warp/transformation can be
represented with a matrix.  To make more complex warpings (for example
to define a translation, rotation and scale as one warp) the individual
matrices have to be multiplied through matrix multiplication.  However
matrix multiplication is not commutative, so the order of the set of
matrices you use for the multiplication is going to be very important.

   The first warping should be placed as the left-most matrix.  The
second warping to the right of that and so on.  The second
transformation is going to occur on the warped coordinates of the first.
As an example for merging a few transforms into one matrix, the
multiplication below represents the rotation of an image about a point
$[\matrix{U&V}]$ anticlockwise from the horizontal axis by an angle of
$\theta$.  To do this, first we take the origin to $[\matrix{U&V}]$
through translation.  Then we rotate the image, then we translate it
back to where it was initially.  These three operations can be merged in
one operation by calculating the matrix multiplication below:

            $$\left[\matrix{1&0&U\cr0&1&V\cr{}0&0&1}\right]
\left[\matrix{cos\theta&-sin\theta&0\cr sin\theta&cos\theta&0\cr 0&0&1}\right]
           \left[\matrix{1&0&-U\cr0&1&-V\cr{}0&0&1}\right]$$


File: gnuastro.info,  Node: Resampling,  Next: Invoking astwarp,  Prev: Merging multiple warpings,  Up: Warp

6.4.3 Resampling
----------------

A digital image is composed of discrete ‘picture elements’ or ‘pixels’.
When a real image is created from a camera or detector, each pixel’s
area is used to store the number of photo-electrons that were created
when incident photons collided with that pixel’s surface area.  This
process is called the ‘sampling’ of a continuous or analog data into
digital data.  When we change the pixel grid of an image or warp it as
we defined in *note Warping basics::, we have to ‘guess’ the flux value
of each pixel on the new grid based on the old grid, or re-sample it.
Because of the ‘guessing’, any form of warping on the data is going to
degrade the image and mix the original pixel values with each other.  So
if an analysis can be done on an unwarped data image, it is best to
leave the image untouched and pursue the analysis.  However as discussed
in *note Warp:: this is not possible most of the times, so we have to
accept the problem and re-sample the image.

   In most applications of image processing, it is sufficient to
consider each pixel to be a point and not an area.  This assumption can
significantly speed up the processing of an image and also the
simplicity of the code.  It is a fine assumption when the signal to
noise ratio of the objects are very large.  The question will then be
one of interpolation because you have multiple points distributed over
the output image and you want to find the values at the pixel centers.
To increase the accuracy, you might also sample more than one point from
within a pixel giving you more points for a more accurate interpolation
in the output grid.

   However, interpolation has several problems.  The first one is that
it will depend on the type of function you want to assume for the
interpolation.  For example you can choose a bi-linear or bi-cubic (the
‘bi’s are for the 2 dimensional nature of the data) interpolation
method.  For the latter there are various ways to set the constants(1).
Such functional interpolation functions can fail seriously on the edges
of an image.  They will also need normalization so that the flux of the
objects before and after the warpings are comparable.  The most basic
problem with such techniques is that they are based on a point while a
detector pixel is an area.  They add a level of subjectivity to the data
(make more assumptions through the functions than the data can handle).
For most applications this is fine, but in scientific applications where
detection of the faintest possible galaxies or fainter parts of bright
galaxies is our aim, we cannot afford this loss.  Because of these
reasons Warp will not use such interpolation techniques.

   Warp will do interpolation based on “pixel mixing”(2) or “area
resampling”.  This is also what the Hubble Space Telescope pipeline
calls “Drizzling”(3).  This technique requires no functions, it is thus
non-parametric.  It is also the closest we can get (make least
assumptions) to what actually happens on the detector pixels.  The basic
idea is that you reverse-transform each output pixel to find which
pixels of the input image it covers and what fraction of the area of the
input pixels are covered.  To find the output pixel value, you simply
sum the value of each input pixel weighted by the overlapfraction
(between 0 to 1) of the output pixel and that input pixel.  Through this
process, pixels are treated as an area not as a point (which is how
detectors create the image), also the brightness (see *note Flux
Brightness and magnitude::) of an object will be left completely
unchanged.

   If there are very high spatial-frequency signals in the image (for
example fringes) which vary on a scale smaller than your output image
pixel size, pixel mixing can cause ailiasing(4).  So if the input image
has fringes, they have to be calculated and removed separately (which
would naturally be done in any astronomical application).  Because of
the PSF no astronomical target has a sharpchange in the signal so this
issue is less important for astronomical applications, see *note PSF::.

   ---------- Footnotes ----------

   (1) see <http://entropymine.com/imageworsener/bicubic/> for a nice
introduction.

   (2) For a graphic demonstration see
<http://entropymine.com/imageworsener/pixelmixing/>.

   (3) <http://en.wikipedia.org/wiki/Drizzle_(image_processing)>

   (4) <http://en.wikipedia.org/wiki/Aliasing>


File: gnuastro.info,  Node: Invoking astwarp,  Prev: Resampling,  Up: Warp

6.4.4 Invoking Warp
-------------------

Warp an input dataset into a new grid.  Any homographic warp (for
example scaling, rotation, translation, projection) is acceptable, see
*note Warping basics:: for the definitions.  The general template for
invoking Warp is:

     $ astwarp [OPTIONS...] InputImage

One line examples:

     ## Rotate and then scale input image:
     $ astwarp --rotate=37.92 --scale=0.8 image.fits

     ## Scale, then translate the input image:
     $ astwarp --scale 8/3 --translate 2.1 image.fits

     ## Align raw image with celestial coordinates:
     $ astwarp --align rawimage.fits --output=aligned.fits

     ## Directly input a custom warping matrix (using fraction):
     $ astwarp --matrix=1/5,0,4/10,0,1/5,4/10,0,0,1 image.fits

     ## Directly input a custom warping matrix, with final numbers:
     $ astwarp --matrix="0.7071,-0.7071,  0.7071,0.7071" image.fits

   If any processing is to be done, Warp can accept one file as input.
As in all Gnuastro programs, when an output is not explicitly set with
the ‘--output’ option, the output filename will be set automatically
based on the operation, see *note Automatic output::.  For the full list
of general options to all Gnuastro programs (including Warp), please see
*note Common options::.

   To be the most accurate, the input image will be read as a 64-bit
double precision floating point dataset and all internal processing is
done in this format (including the raw output type).  You can use the
common ‘--type’ option to write the output in any type you want, see
*note Numeric data types::.

   Warps must be specified as command-line options, either as (possibly
multiple) modular warpings (for example ‘--rotate’, or ‘--scale’), or
directly as a single raw matrix (with ‘--matrix’).  If specified
together, the latter (direct matrix) will take precedence and all the
modular warpings will be ignored.  Any number of modular warpings can be
specified on the command-line and configuration files.  If more than one
modular warping is given, all will be merged to create one warping
matrix.  As described in *note Merging multiple warpings::, matrix
multiplication is not commutative, so the order of specifying the
modular warpings on the command-line, and/or configuration files makes a
difference (see *note Configuration file precedence::).  The full list
of modular warpings and the other options particular to Warp are
described below.

   The values to the warping options (modular warpings as well as
‘--matrix’), are a sequence of at least one number.  Each number in this
sequence is separated from the next by a comma (<,>).  Each number can
also be written as a single fraction (with a forward-slash </> between
the numerator and denominator).  Space and Tab characters arepermitted
between any two numbers, just don’t forget to quote the whole value.
Otherwise, the value will not be fully passed onto the option.  See the
examples above as a demonstration.

   Based on the FITS standard, integer values are assigned to the center
of a pixel and the coordinate [1.0, 1.0] is the center of the first
pixel (bottom left of the image when viewed in SAO ds9).  So the
coordinate center [0.0, 0.0] is half a pixel away (in each axis) from
the bottom left vertex of the first pixel.  The resampling that is done
in Warp (see *note Resampling::) is done on the coordinate axes and thus
directly depends on the coordinate center.  In some situations this if
fine, for example when rotating/aligning a real image, all the edge
pixels will be similarly affected.  But in other situations (for example
when scaling an over-sampled mock image to its intended resolution, this
is not desired: you want the center of the coordinates to be on the
corner of the pixel.  In such cases, you can use the ‘--centeroncorner’
option which will shift the center by $0.5$ before the main warp, then
shift it back by $-0.5$ after the main warp, see below.

‘-a’
‘--align’
     Align the image and celestial (WCS) axes given in the input.  After
     it, the vertical image direction (when viewed in SAO ds9)
     corresponds to thedeclination and the horizontal axis is the
     inverse of the Right Ascension (RA). The inverse of the RA is
     chosen so the image can correspond to what you would actually see
     on the sky and is common in most survey images.

     Align is internally treated just like a rotation (‘--rotation’),
     but uses the input image’s WCS to find the rotation angle.  Thus,
     if you have rotated the image before calling ‘--align’, you might
     get unexpected results (because the rotation is defined on the
     original WCS).

‘-r FLT’
‘--rotate=FLT’
     Rotate the input image by the given angle in degrees: $\theta$ in
     *note Warping basics::.  Note that commonly, the WCS structure of
     the image is set such that the RA is the inverse of the image
     horizontal axis which increases towards the right in the FITS
     standard and as viewed by SAO ds9.  So the default center for
     rotation is on the right of the image.  If you want to rotate about
     other points, you have to translate the warping center first (with
     ‘--translate’) then apply your rotation and then return the center
     back to the original position (with another call to ‘--translate’,
     see *note Merging multiple warpings::.

‘-s FLT[,FLT]’
‘--scale=FLT[,FLT]’
     Scale the input image by the given factor(s): $M$ and $N$ in *note
     Warping basics::.  If only one value is given, then both image axes
     will be scaled with the given value.  When two values are given
     (separated by a comma), the first will be used to scale the first
     axis and the second will be used for the second axis.  If you only
     need to scale one axis, use ‘1’ for the axis you don’t need to
     scale.  The value(s) can also be written (on the command-line or in
     configuration files) as a fraction.

‘-f FLT[,FLT]’
‘--flip=FLT[,FLT]’
     Flip the input image around the given axis(s).  If only one value
     is given, then both image axes are flipped.  When two values are
     given (separated by acomma), you can choose which axis to flip
     over.  ‘--flip’ only takes values ‘0’ (for no flip), or ‘1’ (for a
     flip).  Hence, if you want to flip by the second axis only, use
     ‘--flip=0,1’.

‘-e FLT[,FLT]’
‘--shear=FLT[,FLT]’
     Shear the input image by the given value(s): $A$ and $B$ in *note
     Warping basics::.  If only one value is given, then both image axes
     will be sheared with the given value.  When two values are given
     (separated by a comma), the first will be used to shear the first
     axis and the second will be used for the second axis.  If you only
     need to shear along one axis, use ‘0’ for the axis that must be
     untouched.  The value(s) can also be written (on the command-line
     or in configuration files) as a fraction.

‘-t FLT[,FLT]’
‘--translate=FLT[,FLT]’
     Translate (move the center of coordinates) the input image by the
     given value(s): $c$ and $f$ in *note Warping basics::.  If only one
     value is given, then both image axes will be translated by the
     given value.  When two values are given (separated by a comma), the
     first will be used to translate the first axis and the second will
     be used for the second axis.  If you only need to translate along
     one axis, use ‘0’ for the axis that must be untouched.  The
     value(s) can also be written (on the command-line or in
     configuration files) as a fraction.

‘-p FLT[,FLT]’
‘--project=FLT[,FLT]’
     Apply a projection to the input image by the given values(s): $g$
     and $h$ in *note Warping basics::.  If only one value is given,
     then projection will apply to both axes with the given value.  When
     two values are given (separated by a comma), the first will be used
     to project the first axis and the second will be used for the
     second axis.  If you only need to project along one axis, use ‘0’
     for the axis that must be untouched.  The value(s) can also be
     written (on the command-line or in configuration files) as a
     fraction.

‘-m STR’
‘--matrix=STR’
     The warp/transformation matrix.  All the elements in this matrix
     must be separated by comas(<,>) characters and as described above,
     you can also use fractions (a forward-slash between two numbers).
     The transformation matrix can be either a 2 by 2 (4 numbers), or a
     3 by 3 (9 numbers) array.  In the former case (if a 2 by 2 matrix
     is given), then it is put into a 3 by 3 matrix (see *note Warping
     basics::).

     The determinant of the matrix has to be non-zero and it must not
     contain any non-number values (for example infinities or NaNs).
     The elements of the matrix have to be written row by row.  So for
     the general Homography matrix of *note Warping basics::, it should
     be called with ‘--matrix=a,b,c,d,e,f,g,h,1’.

     The raw matrix takes precedence over all the modular warping
     options listed above, so if it is called with any number of modular
     warps, the latter are ignored.

‘-c’
‘--centeroncorer’
     Put the center of coordinates on the corner of the first
     (bottom-left when viewed in SAO ds9) pixel.  This option is applied
     after the final warping matrix has been finalized: either through
     modular warpings or the raw matrix.  See the explanation above for
     coordinates in the FITS standard to better understand this option
     and when it should be used.

‘--hstartwcs=INT’
     Specify the first header keyword number (line) that should be used
     to read the WCS information, see the full explanation in *note
     Invoking astcrop::.

‘--hendwcs=INT’
     Specify the last header keyword number (line) that should be used
     to read the WCS information, see the full explanation in *note
     Invoking astcrop::.

‘-k’
‘--keepwcs’
     Do not correct the WCS information of the input image and save it
     untouched to the output image.  By default the WCS (World
     Coordinate System) information of the input image is going to be
     corrected in the output image so the objects in the image are at
     the same WCS coordinates.  But in some cases it might be useful to
     keep it unchanged (for example to correct alignments).

‘-C FLT’
‘--coveredfrac=FLT’
     Depending on the warp, the output pixels that cover pixels on the
     edge of the input image, or blank pixels in the input image, are
     not going to be fully covered by input data.  With this option, you
     can specify the acceptable covered fraction of such pixels (any
     value between 0 and 1).  If you only want output pixels that are
     fully covered by the input image area (and are not blank), then you
     can set ‘--coveredfrac=1’.  Alternatively, a value of ‘0’ will keep
     output pixels that are even infinitesimally covered by the input(so
     the sum of the pixels in the input and output images will be the
     same).


File: gnuastro.info,  Node: Data analysis,  Next: Modeling and fittings,  Prev: Data manipulation,  Up: Top

7 Data analysis
***************

Astronomical datasets (images or tables) contain very valuable
information, the tools in this section can help in analyzing,
extracting, and quantifying that information.  For example getting
general or specific statistics of the dataset (with *note Statistics::),
detecting signal within a noisy dataset (with *note NoiseChisel::), or
creating a catalog from an input dataset (with *note MakeCatalog::).

* Menu:

* Statistics::                  Calculate dataset statistics.
* NoiseChisel::                 Detect objects in an image.
* Segment::                     Segment detections based on signal structure.
* MakeCatalog::                 Catalog from input and labeled images.
* Match::                       Match two datasets.


File: gnuastro.info,  Node: Statistics,  Next: NoiseChisel,  Prev: Data analysis,  Up: Data analysis

7.1 Statistics
==============

The distribution of values in a dataset can provide valuable information
about it.  For example, in an image, if it is a positively skewed
distribution, we can see that there is significant data in the image.
If the distribution is roughly symmetric, we can tell that there is no
significant data in the image.  In a table, when we need to select a
sample of objects, it is important to first get a general view of the
whole sample.

   On the other hand, you might need to know certain statistical
parameters of the dataset.  For example, if we have run a detection
algorithm on an image, and we want to see how accurate it was, one
method is to calculate the average of the undetected pixels and see how
reasonable it is (if detection is done correctly, the average of
undetected pixels should be approximately equal to the background value,
see *note Sky value::).  In a table, you might have calculated the
magnitudes of a certain class of objects and want to get some general
characteristics of the distribution immediately on the command-line
(very fast!), to possibly change some parameters.  The Statistics
program is designed for such situations.

* Menu:

* Histogram and Cumulative Frequency Plot::  Basic definitions.
* Sigma clipping::              Definition of $\sigma$-clipping.
* Sky value::                   Definition and derivation of the Sky value.
* Invoking aststatistics::      Arguments and options to Statistics.


File: gnuastro.info,  Node: Histogram and Cumulative Frequency Plot,  Next: Sigma clipping,  Prev: Statistics,  Up: Statistics

7.1.1 Histogram and Cumulative Frequency Plot
---------------------------------------------

Histograms and the cumulative frequency plots are both used to visually
study the distribution of a dataset.  A histogram shows the number of
data points which lie within pre-defined intervals (bins).  So on the
horizontal axis we have the bin centers and on the vertical, the number
of points that are in that bin.  You can use it to get a general view of
the distribution: which values have been repeated the most?  how
close/far are the most significant bins?  Are there more values in the
larger part of the range of the dataset, or in the lower part?
Similarly, many very important properties about the dataset can be
deduced from a visual inspection of the histogram.  In the Statistics
program, the histogram can be either output to a table to plot with your
favorite plotting program(1), or it can be shown with ASCII characters
on the command-line, which is very crude, but good enough for a fast and
on-the-go analysis, see the example in *note Invoking aststatistics::.

   The width of the bins is only necessary parameter for a histogram.
In the limiting case that the bin-widths tend to zero (while assuming
the number of points in the dataset tend to infinity), then the
histogram will tend to the probability density function
(https://en.wikipedia.org/wiki/Probability_density_function) of the
distribution.  When the absolute number of points in each bin is not
relevant to the study (only the shape of the histogram is important),
you can _normalize_ a histogram so like the probability density
function, the sum of all its bins will be one.

   In the cumulative frequency plot of a distribution, the horizontal
axis is the sorted data values and the y axis is the index of each data
in the sorted distribution.  Unlike a histogram, a cumulative frequency
plot does not involve intervals or bins.  This makes it less prone to
any sort of bias or error that a given bin-width would have on the
analysis.  When a larger number of the data points have roughly the same
value, then the cumulative frequency plot will become steep in that
vicinity.  This occurs because on the horizontal axis, there is little
change while on the vertical axis, the indexes constantly increase.
Normalizing a cumulative frequency plot means to divide each index (y
axis) by the total number of data points (or the last value).

   Unlike the histogram which has a limited number of bins, ideally the
cumulative frequency plot should have one point for every data element.
Even in small datasets (for example a $200\times200$ image) this will
result in an unreasonably large number of points to plot (40000)!  As a
result, for practical reasons, it is common to only store its value on a
certain number of points (intervals) in the input range rather than the
whole dataset, so you should determine the number of bins you want when
asking for a cumulative frequency plot.  In Gnuastro (and thus the
Statistics program), the number reported for each bin is the total
number of data points until the larger interval value for that bin.  You
can see an example histogram and cumulative frequency plot of a single
dataset under the ‘--asciihist’ and ‘--asciicfp’ options of *note
Invoking aststatistics::.

   So as a summary, both the histogram and cumulative frequency plot in
Statistics will work with bins.  Within each bin/interval, the lower
value is considered to be within then bin (it is inclusive), but its
larger value is not (it is exclusive).  Formally, an interval/bin
between a and b is represented by [a, b).  When the over-all range of
the dataset is specified (with the ‘--greaterequal’, ‘--lessthan’, or
‘--qrange’ options), the acceptable values of the dataset are also
defined with a similar inclusive-exclusive manner.  But when the range
is determined from the actual dataset (none of these options is called),
the last element in the dataset is included in the last bin’s count.

   ---------- Footnotes ----------

   (1) We recommend PGFPlots (http://pgfplots.sourceforge.net/) which
generates your plots directly within TeX (the same tool that generates
your document).


File: gnuastro.info,  Node: Sigma clipping,  Next: Sky value,  Prev: Histogram and Cumulative Frequency Plot,  Up: Statistics

7.1.2 Sigma clipping
--------------------

Let’s assume that you have pure noise (centered on zero) with a clear
Gaussian distribution
(https://en.wikipedia.org/wiki/Normal_distribution), or see *note Photon
counting noise::.  Now let’s assume you add very bright objects (signal)
on the image which have a very sharp boundary.  By a sharp boundary, we
mean that there is a clear cutoff (from the noise) at the pixels the
objects finish.  In other words, at their boundaries, the objects do not
fade away into the noise.  In such a case, when you plot the histogram
(see *note Histogram and Cumulative Frequency Plot::) of the
distribution, the pixels relating to those objects will be clearly
separate from pixels that belong to parts of the image that did not have
any signal (were just noise).  In the cumulative frequency plot, after a
steady rise (due to the noise), you would observe a long flat region
were for a certain range of data (horizontal axis), there is no increase
in the index (vertical axis).

   Outliers like the example above can significantly bias the
measurement of noise statistics.  $\sigma$-clipping is defined as a way
to avoid the effect of such outliers.  In astronomical applications,
cosmic rays (when they collide at a near normal incidence angle) are a
very good example of such outliers.  The tracks they leave behind in the
image are perfectly immune to the blurring caused by the atmosphere and
the aperture.  They are also very energetic and so their borders are
usually clearly separated from the surrounding noise.  So
$\sigma$-clipping is very useful in removing their effect on the data.
See Figure 15 in Akhlaghi and Ichikawa, 2015
(https://arxiv.org/abs/1505.01664).

   $\sigma$-clipping is defined as the very simple iteration below.  In
each iteration, the range of input data might decrease and so when the
outliers have the conditions above, the outliers will be removed through
this iteration.  The exit criteria will be discussed below.

  1. Calculate the standard deviation ($\sigma$) and median ($m$) of a
     distribution.
  2. Remove all points that are smaller or larger than
     $m\pm\alpha\sigma$.
  3. Go back to step 1, unless the selected exit criteria is reached.

The reason the median is used as a reference and not the mean is that
the mean is too significantly affected by the presence of outliers,
while the median is less affected, see *note Quantifying signal in a
tile::.  As you can tell from this algorithm, besides the condition
above (that the signal have clear high signal to noise boundaries)
$\sigma$-clipping is only useful when the signal does not cover more
than half of the full data set.  If they do, then the median will lie
over the outliers and $\sigma$-clipping might remove the pixels with no
signal.

   There are commonly two exit criteria to stop the $\sigma$-clipping
iteration:

   • When a certain number of iterations has taken place (second value
     to the ‘--sclipparams’ option is larger than 1).
   • When the new measured standard deviation is within a certain
     tolerance level of the old one (second value to the ‘--sclipparams’
     option is less than 1).  The tolerance level is defined by:

           $$\sigma_{old}-\sigma_{new} \over \sigma_{new}$$

     The standard deviation is used because it is heavily influenced by
     the presence of outliers.  Therefore the fact that it stops
     changing between two iterations is a sign that we have successfully
     removed outliers.  Note that in each clipping, the dispersion in
     the distribution is either less or equal.  So
     $\sigma_{old}\geq\sigma_{new}$.

When working on astronomical images, objects like galaxies and stars are
blurred by the atmosphere and the telescope aperture, therefore their
signal sinks into the noise very gradually.  Galaxies in particular do
not appear to have a clear high signal to noise cutoff at all.
Therefore $\sigma$-clipping will not be useful in removing their effect
on the data.

   To gauge if $\sigma$-clipping will be useful for your dataset, look
at the histogram (see *note Histogram and Cumulative Frequency Plot::).
The ASCII histogram that is printed on the command-line with
‘--asciihist’ is good enough in most cases.


File: gnuastro.info,  Node: Sky value,  Next: Invoking aststatistics,  Prev: Sigma clipping,  Up: Statistics

7.1.3 Sky value
---------------

One of the most important aspects of a dataset is its reference value:
the value of the dataset where there is no signal.  Without knowing, and
thus removing the effect of, this value it is impossible to compare the
derived results of many high-level analyses over the dataset with other
datasets (in the attempt to associate our results with the “real”
world).

   In astronomy, this reference value is known as the “Sky” value: the
value that noise fluctuates around: where there is no signal from
detectable objects or artifacts (for example galaxies, stars, planets or
comets, star spikes or internal optical ghost).  Depending on the
dataset, the Sky value maybe a fixed value over the whole dataset, or it
may vary based on location.  For an example of the latter case, see
Figure 11 in Akhlaghi and Ichikawa (2015)
(https://arxiv.org/abs/1505.01664).

   Because of the significance of the Sky value in astronomical data
analysis, we have devoted this subsection to it for a thorough review.
We start with a thorough discussion on its definition (*note Sky value
definition::).  In the astronomical literature, researchers use a
variety of methods to estimate the Sky value, so in *note Sky value
misconceptions::) we review those and discuss their biases.  From the
definition of the Sky value, the most accurate way to estimate the Sky
value is to run a detection algorithm (for example *note NoiseChisel::)
over the dataset and use the undetected pixels.  However, there is also
a more crude method that maybe useful when good direct detection is not
initially possible (for example due to too many cosmic rays in a shallow
image).  A more crude (but simpler method) that is usable in such
situations is discussed in *note Quantifying signal in a tile::.

* Menu:

* Sky value definition::        Definition of the Sky/reference value.
* Sky value misconceptions::    Wrong methods to estimate the Sky value.
* Quantifying signal in a tile::  Method to estimate the presence of signal.


File: gnuastro.info,  Node: Sky value definition,  Next: Sky value misconceptions,  Prev: Sky value,  Up: Sky value

7.1.3.1 Sky value definition
............................

This analysis is taken from Akhlaghi and Ichikawa (2015)
(https://arxiv.org/abs/1505.01664).  Let’s assume that all instrument
defects – bias, dark and flat – have been corrected and the brightness
(see *note Flux Brightness and magnitude::) of a detected object, $O$,
is desired.  The sources of flux on pixel(1) $i$ of the image can be
written as follows:

   • Contribution from the target object ($O_i$).
   • Contribution from other detected objects ($D_i$).
   • Undetected objects or the fainter undetected regions of bright
     objects ($U_i$).
   • A cosmic ray ($C_i$).
   • The background flux, which is defined to be the count if none of
     the others exists on that pixel ($B_i$).
The total flux in this pixel ($T_i$) can thus be written as:

                     $$T_i=B_i+D_i+U_i+C_i+O_i.$$

By definition, $D_i$ is detected and it can be assumed that it is
correctly estimated (deblended) and subtracted, we can thus set $D_i=0$.
There are also methods to detect and remove cosmic rays, for example the
method described in van Dokkum (2001)(2), or by comparing multiple
exposures.  This allows us to set $C_i=0$.  Note that in practice, $D_i$
and $U_i$ are correlated, because they both directly depend on the
detection algorithm and its input parameters.  Also note that no
detection or cosmic ray removal algorithm is perfect.  With these
limitations in mind, the observed Sky value for this pixel ($S_i$) can
be defined as

                        $$S_i\equiv{}B_i+U_i.$$

Therefore, as the detection process (algorithm and input parameters)
becomes more accurate, or $U_i\to0$, the Sky value will tend to the
background value or $S_i\to B_i$.  Hence, we see that while $B_i$ is an
inherent property of the data (pixel in an image), $S_i$ depends on the
detection process.  Over a group of pixels, for example in an image or
part of an image, this equation translates to the average of undetected
pixels (Sky$=\sum{S_i}$).  With this definition of Sky, the object flux
in the data can be calculated, per pixel, with

               $$T_{i}=S_{i}+O_{i} \quad\rightarrow\quad
                         O_{i}=T_{i}-S_{i}.$$

   In the fainter outskirts of an object, a very small fraction of the
photo-electrons in a pixel actually belongs to objects, the rest is
caused by random factors (noise), see Figure 1b in Akhlaghi and Ichikawa
(2015) (https://arxiv.org/abs/1505.01664).  Therefore even a small over
estimation of the Sky value will result in the loss of a very large
portion of most galaxies.  Besides the lost area/brightness, this will
also cause an over-estimation of the Sky value and thus even more
under-estimation of the object’s brightness.  It is thus very important
to detect the diffuse flux of a target, even if they are not your
primary target.

   In summary, the more accurately the Sky is measured, the more
accurately the brightness (sum of pixel values) of the target object can
be measured (photometry).  Any under/over-estimation in the Sky will
directly translate to an over/under-estimation of the measured object’s
brightness.

The *Sky value* is only correctly found when all the detected objects
($D_i$ and $C_i$) have been removed from the data.

   ---------- Footnotes ----------

   (1) For this analysis the dimension of the data (image) is
irrelevant.  So if the data is an image (2D) with width of $w$ pixels,
then a pixel located on column $x$ and row $y$ (where all counting
starts from zero and (0, 0) is located on the bottom left corner of the
image), would have an index: $i=x+y\times{}w$.

   (2) van Dokkum, P. G. (2001).  Publications of the Astronomical
Society of the Pacific.  113, 1420.


File: gnuastro.info,  Node: Sky value misconceptions,  Next: Quantifying signal in a tile,  Prev: Sky value definition,  Up: Sky value

7.1.3.2 Sky value misconceptions
................................

As defined in *note Sky value::, the sky value is only accurately
defined when the detection algorithm is not significantly reliant on the
sky value.  In particular its detection threshold.  However, most
signal-based detection tools(1) use the sky value as a reference to
define the detection threshold.  These older techniques therefore had to
rely on approximations based on other assumptions about the data.  A
review of those other techniques can be seen in Appendix A of Akhlaghi
and Ichikawa (2015) (https://arxiv.org/abs/1505.01664).

   These methods were extensively used in astronomical data analysis for
several decades, therefore they have given rise to a lot of
misconceptions, ambiguities and disagreements about the sky value and
how to measure it.  As a summary, the major methods used until now were
an approximation of the mode of the image pixel distribution and
$\sigma$-clipping.

   • To find the mode of a distribution those methods would either have
     to assume (or find) a certain probability density function (PDF) or
     use the histogram.  But astronomical datasets can have any
     distribution, making it almost impossible to define a generic
     function.  Also, histogram-based results are very inaccurate (there
     is a large dispersion) and it depends on the histogram bin-widths.
     Generally, the mode of a distribution also shifts as signal is
     added.  Therefore, even if it is accurately measured, the mode is a
     biased measure for the Sky value.

   • Another approach was to iteratively clip the brightest pixels in
     the image (which is known as $\sigma$-clipping).  See *note Sigma
     clipping:: for a complete explanation.  $\sigma$-clipping is useful
     when there are clear outliers (an object with a sharp edge in an
     image for example).  However, real astronomical objects have
     diffuse and faint wings that penetrate deeply into the noise, see
     Figure 1 in Akhlaghi and Ichikawa (2015)
     (https://arxiv.org/abs/1505.01664).

   As discussed in *note Sky value::, the sky value can only be
correctly defined as the average of undetected pixels.  Therefore all
such approaches that try to approximate the sky value prior to detection
are ultimately poor approximations.

   ---------- Footnotes ----------

   (1) According to Akhlaghi and Ichikawa (2015), signal-based detection
is a detection process that relies heavily on assumptions about the
to-be-detected objects.  This method was the most heavily used technique
prior to the introduction of NoiseChisel in that paper.


File: gnuastro.info,  Node: Quantifying signal in a tile,  Prev: Sky value misconceptions,  Up: Sky value

7.1.3.3 Quantifying signal in a tile
....................................

Put simply, noise can be characterized with a certain spread about the
measured value.  In the Gaussian distribution (most commonly used to
model noise) the spread is defined by the standard deviation about the
characteristic mean.

   Let’s start by clarifying some definitions first: _Data_ is defined
as the combination of signal and noise (so a noisy image is one
_data_set).  _Signal_ is defined as the mean of the noise on each
element.  We’ll also assume that the _background_ (see *note Sky value
definition::) is subtracted and is zero.

   When a data set doesn’t have any signal (only noise), the mean,
median and mode of the distribution are equal within statistical errors
and approximately equal to the background value.  Signal always has a
positive value and will never become negative, see Figure 1 in Akhlaghi
and Ichikawa (2015) (https://arxiv.org/abs/1505.01664).  Therefore, as
more signal is added, the mean, median and mode of the dataset shift to
the positive.  The mean’s shift is the largest.  The median shifts less,
since it is defined based on an ordered distribution and so is not
affected by a small number of outliers.  The distribution’s mode shifts
the least to the positive.

   Inverting the argument above gives us a robust method to quantify the
significance of signal in a dataset.  Namely, when the mean and median
of a distribution are approximately equal, or the mean’s quantile is
around 0.5, we can argue that there is no significant signal.

   To allow for gradients (which are commonly present in ground-based
images), we can consider the image to be made of a grid of tiles (see
*note Tessellation::(1)).  Hence, from the difference of the mean and
median on each tile, we can estimate the significance of signal in it.
The median of a distribution is defined to be the value of the
distribution’s middle point after sorting (or 0.5 quantile).  Thus, to
estimate the presence of signal, we’ll compare with the quantile of the
mean with 0.5.  If the absolute difference in a tile is larger than the
value given to the ‘--meanmedqdiff’ option, that tile will be ignored.
You can read this option as “mean-median-quantile-difference”.

   The raw dataset’s distribution is noisy, so using the argument above
on the raw input will give a noisy result.  To decrease the noise/error
in estimating the mode, we will use convolution (see *note Convolution
process::).  Convolution decreases the range of the dataset and enhances
its skewness, See Section 3.1.1 and Figure 4 in Akhlaghi and Ichikawa
(2015) (https://arxiv.org/abs/1505.01664).  This enhanced skewness can
be interpreted as an increase in the Signal to noise ratio of the
objects buried in the noise.  Therefore, to obtain an even better
measure of the presence of signal in a tile, the mean and median
discussed above are measured on the convolved image.

   Through the difference of the mean and median we have actually
‘detected’ data in the distribution.  However this “detection” was only
based on the total distribution of the data in each tile (a much lower
resolution).  This is the main limitation of this technique.  The best
approach is thus to do detection over the dataset, mask all the detected
pixels and use the undetected regions to estimate the sky and its
standard deviation (possibly over a tessellation).  This is how
NoiseChisel works: it uses the argument above to find tiles that are
used to find its thresholds.  Several higher-level steps are done on the
thresholded pixels to define the higher-level detections (see *note
NoiseChisel::).

   There is one final hurdle: raw astronomical datasets are commonly
peppered with Cosmic rays.  Images of Cosmic rays aren’t smoothed by the
atmosphere or telescope aperture, so they have sharp boundaries.  Also,
since they don’t occupy too many pixels, they don’t affect the mode and
median calculation.  But their very high values can greatly bias the
calculation of the mean (recall how the mean shifts the fastest in the
presence of outliers), for example see Figure 15 in Akhlaghi and
Ichikawa (2015) (https://arxiv.org/abs/1505.01664).

   The effect of outliers like cosmic rays on the mean and standard
deviation can be removed through $\sigma$-clipping, see *note Sigma
clipping:: for a complete explanation.  Therefore, after asserting that
the mode and median are approximately equal in a tile (see *note
Tessellation::), the final Sky value and its standard deviation are
determined after $\sigma$-clipping with the ‘--sigmaclip’ option.

   In the end, some of the tiles will pass the mean and median quantile
difference test.  However, prior to interpolating over the failed tiles,
another point should be considered: large and extended galaxies, or
bright stars, have wings which sink into the noise very gradually.  In
some cases, the gradient over these wings can be on scales that is
larger than the tiles.  The mean-median distance test will pass on such
tiles and will cause a strong peak in the interpolated tile grid, see
*note Detecting large extended targets::.

   The tiles that exist over the wings of large galaxies or bright stars
are outliers in the distribution of tiles that passed the mean-median
quantile distance test.  Therefore, the final step of “quantifying
signal in a tile” is to look at this distribution and remove the
outliers.  $\sigma$-clipping is a good solution for removing a few
outliers, but the problem with outliers of this kind is that there may
be many such tiles (depending on the large/bright stars/galaxies in the
image).  Therefore a novel outlier rejection algorithm will be used.

   To identify the first outlier, we’ll use the distribution of
distances between sorted elements.  If there are $N$ successful tiles,
for every tile, the distance between the adjacent $N/2$ previous
elements is found, giving a distribution containing $N/2-1$ points.  The
$\sigma$-clipped median and standard deviation of this distribution is
then found ($\sigma$-clipping is configured with ‘--outliersclip’).
Finally, if the distance between the element and its previous element is
more than ‘--outliersigma’ multiples of the $\sigma$-clipped standard
deviation added with the $\sigma$-clipped median, that element is
considered an outlier and all tiles larger than that value are ignored.

   Formally, if we assume there are $N$ elements.  They are first
sorted.  Searching for the outlier starts on element $N/2$ (integer
division).  Let’s take $v_i$ to be the $i$-th element of the sorted
input (with no blank values) and $m$ and $\sigma$ as the
$\sigma$-clipped median and standard deviation from the distances of the
previous $N/2-1$ elements (not including $v_i$).  If the value given to
‘--outliersigma’ is displayed with $s$, the $i$-th element is considered
as an outlier when the condition below is true.

                  $${(v_i-v_{i-1})-m\over \sigma}>s$$

   Since $i$ begins from the median, the outlier has to be larger than
the median.  You can use the check images (for example ‘--checksky’ in
the Statistics program or ‘--checkqthresh’, ‘--checkdetsky’ and
‘--checksky’ options in NoiseChisel for any of its steps that uses this
outlier rejection) to inspect the steps and see which tiles have been
discarded as outliers prior to interpolation.

   ---------- Footnotes ----------

   (1) The options to customize the tessellation are discussed in *note
Processing options::.


File: gnuastro.info,  Node: Invoking aststatistics,  Prev: Sky value,  Up: Statistics

7.1.4 Invoking Statistics
-------------------------

Statistics will print statistical measures of an input dataset (table
column or image).  The executable name is ‘aststatistics’ with the
following general template

     $ aststatistics [OPTION ...] InputImage.fits

One line examples:

     ## Print some general statistics of input image:
     $ aststatistics image.fits

     ## Print some general statistics of column named MAG_F160W:
     $ aststatistics catalog.fits -h1 --column=MAG_F160W

     ## Make the histogram of the column named MAG_F160W:
     $ aststatistics table.fits -cMAG_F160W --histogram

     ## Find the Sky value on image with a given kernel:
     $ aststatistics image.fits --sky --kernel=kernel.fits

     ## Print Sigma-clipped results of records with a MAG_F160W
     ## column value between 26 and 27:
     $ aststatistics cat.fits -cMAG_F160W -g26 -l27 --sigmaclip=3,0.2

     ## Print the median value of all records in column MAG_F160W that
     ## have a value larger than 3 in column PHOTO_Z:
     $ aststatistics tab.txt -rPHOTO_Z -g3 -cMAG_F160W --median

     ## Calculate the median of the third column in the input table, but only
     ## for rows where the mean of the first and second columns is >5.
     $ awk '($1+$2)/2 > 5 {print $3}' table.txt | aststatistics --median

Statistics can take its input dataset either from a file (image or
table) or the Standard input (see *note Standard input::).  If any
output file is to be created, the value to the ‘--output’ option, is
used as the base name for the generated files.  Without ‘--output’, the
input name will be used to generate an output name, see *note Automatic
output::.  The options described below are particular to Statistics, but
for general operations, it shares a large collection of options with the
other Gnuastro programs, see *note Common options:: for the full list.
For more on reading from standard input, please see the description of
‘--stdintimeout’ option in *note Input output options::.  Options can
also be given in configuration files, for more, please see *note
Configuration files::.

   The input dataset may have blank values (see *note Blank pixels::),
in this case, all blank pixels are ignored during the calculation.
Initially, the full dataset will be read, but it is possible to select a
specific range of data elements to use in the analysis of each run.  You
can either directly specify a minimum and maximum value for the range of
data elements to use (with ‘--greaterequal’ or ‘--lessthan’), or specify
the range using quantiles (with ‘--qrange’).  If a range is specified,
all pixels outside of it are ignored before any processing.

   The following set of options are for specifying the input/outputs of
Statistics.  There are many other input/output options that are common
to all Gnuastro programs including Statistics, see *note Input output
options:: for those.

‘-c STR/INT’
‘--column=STR/INT’
     The column to use when the input file is a table with more than one
     column.  See *note Selecting table columns:: for a full description
     of how to use this option.  For more on how tables are read in
     Gnuastro, please see *note Tables::.

‘-r STR/INT’
‘--refcol=STR/INT’
     The reference column selector when the input file is a table.  When
     a reference column is given, the range options below will be
     applied to this column and only elements in the input column that
     have a reference value in the correct range will be used.  In
     practice this option allows you to select a subset of the input
     column based on values in another (the reference) column.  All the
     statistical calculations will be done on the selected input column,
     not the reference column.

‘-g FLT’
‘--greaterequal=FLT’
     Limit the range of inputs into those with values greater and equal
     to what is given to this option.  None of the values below this
     value will be used in any of the processing steps below.

‘-l FLT’
‘--lessthan=FLT’
     Limit the range of inputs into those with values less-than what is
     given to this option.  None of the values greater or equal to this
     value will be used in any of the processing steps below.

‘-Q FLT[,FLT]’
‘--qrange=FLT[,FLT]’
     Specify the range of usable inputs using the quantile.  This option
     can take one or two quantiles to specify the range.  When only one
     number is input (let’s call it $Q$), the range will be those values
     in the quantile range $Q$ to $1-Q$.  So when only one value is
     given, it must be less than 0.5.  When two values are given, the
     first is used as the lower quantile range and the second is used as
     the larger quantile range.

     The quantile of a given element in a dataset is defined by the
     fraction of its index to the total number of values in the sorted
     input array.  So the smallest and largest values in the dataset
     have a quantile of 0.0 and 1.0.  The quantile is a very useful
     non-parametric (making no assumptions about the input) relative
     measure to specify a range.  It can best be understood in terms of
     the cumulative frequency plot, see *note Histogram and Cumulative
     Frequency Plot::.  The quantile of each horizontal axis value in
     the cumulative frequency plot is the vertical axis value associate
     with it.

   When no operation is requested, Statistics will print some general
basic properties of the input dataset on the command-line like the
example below (ran on one of the output images of ‘make check’(1)).
This default behavior is designed to help give you a general feeling of
how the data are distributed and help in narrowing down your analysis.

     $ aststatistics convolve_spatial_scaled_noised.fits     \
                     --greaterequal=9500 --lessthan=11000
     Statistics (GNU Astronomy Utilities) X.X
     -------
     Input: convolve_spatial_scaled_noised.fits (hdu: 0)
     Range: from (inclusive) 9500, upto (exclusive) 11000.
     Unit: Brightness
     -------
       Number of elements:                      9074
       Minimum:                                 9622.35
       Maximum:                                 10999.7
       Mode:                                    10055.45996
       Mode quantile:                           0.4001983908
       Median:                                  10093.7
       Mean:                                    10143.98257
       Standard deviation:                      221.80834
     -------
     Histogram:
      |                   **
      |                 ******
      |                 *******
      |                *********
      |              *************
      |              **************
      |            ******************
      |            ********************
      |          *************************** *
      |        ***************************************** ***
      |*  **************************************************************
      |-----------------------------------------------------------------

   Gnuastro’s Statistics is a very general purpose program, so to be
able to easily understand this diversity in its operations (and how to
possibly run them together), we’ll divided the operations into two
types: those that don’t respect the position of the elements and those
that do (by tessellating the input on a tile grid, see *note
Tessellation::).  The former treat the whole dataset as one and can
re-arrange all the elements (for example sort them), but the former do
their processing on each tile independently.  First, we’ll review the
operations that work on the whole dataset.

   The group of options below can be used to get single value
measurement(s) of the whole dataset.  They will print only the requested
value as one field in a line/row, like the ‘--mean’, ‘--median’ options.
These options can be called any number of times and in any order.  The
outputs of all such options will be printed on one line following each
other (with a space character between them).  This feature makes these
options very useful in scripts, or to redirect into programs like GNU
AWK for higher-level processing.  These are some of the most basic
measures, Gnuastro is still under heavy development and this list will
grow.  If you want another statistical parameter, please contact us and
we will do out best to add it to this list, see *note Suggest new
feature::.

‘-n’
‘--number’
     Print the number of all used (non-blank and in range) elements.

‘--minimum’
     Print the minimum value of all used elements.

‘--maximum’
     Print the maximum value of all used elements.

‘--sum’
     Print the sum of all used elements.

‘-m’
‘--mean’
     Print the mean (average) of all used elements.

‘-t’
‘--std’
     Print the standard deviation of all used elements.

‘-E’
‘--median’
     Print the median of all used elements.

‘-u FLT[,FLT[,...]]’
‘--quantile=FLT[,FLT[,...]]’
     Print the values at the given quantiles of the input dataset.  Any
     number of quantiles may be given and one number will be printed for
     each.  Values can either be written as a single number or as
     fractions, but must be between zero and one (inclusive).  Hence, in
     effect ‘--quantile=0.25 --quantile=0.75’ is equivalent to
     ‘--quantile=0.25,3/4’, or ‘-u1/4,3/4’.

     The returned value is one of the elements from the dataset.  Taking
     $q$ to be your desired quantile, and $N$ to be the total number of
     used (non-blank and within the given range) elements, the returned
     value is at the following position in the sorted array:
     $round(q\times{}N$).

‘--quantfunc=FLT[,FLT[,...]]’
     Print the quantiles of the given values in the dataset.  This
     option is the inverse of the ‘--quantile’ and operates similarly
     except that the acceptable values are within the range of the
     dataset, not between 0 and 1.  Formally it is known as the
     “Quantile function”.

     Since the dataset is not continuous this function will find the
     nearest element of the dataset and use its position to estimate the
     quantile function.

‘-O’
‘--mode’
     Print the mode of all used elements.  The mode is found through the
     mirror distribution which is fully described in Appendix C of
     Akhlaghi and Ichikawa 2015 (https://arxiv.org/abs/1505.01664).  See
     that section for a full description.

     This mode calculation algorithm is non-parametric, so when the
     dataset is not large enough (larger than about 1000 elements
     usually), or doesn’t have a clear mode it can fail.  In such cases,
     this option will return a value of ‘nan’ (for the floating point
     NaN value).

     As described in that paper, the easiest way to assess the quality
     of this mode calculation method is to use it’s symmetricity (see
     ‘--modesym’ below).  A better way would be to use the ‘--mirror’
     option to generate the histogram and cumulative frequency tables
     for any given mirror value (the mode in this case) as a table.  If
     you generate plots like those shown in Figure 21 of that paper,
     then your mode is accurate.

‘--modequant’
     Print the quantile of the mode.  You can get the actual mode value
     from the ‘--mode’ described above.  In many cases, the absolute
     value of the mode is irrelevant, but its position within the
     distribution is important.  In such cases, this option will become
     handy.

‘--modesym’
     Print the symmetricity of the calculated mode.  See the description
     of ‘--mode’ for more.  This mode algorithm finds the mode based on
     how symmetric it is, so if the symmetricity returned by this option
     is too low, the mode is not too accurate.  See Appendix C of
     Akhlaghi and Ichikawa 2015 (https://arxiv.org/abs/1505.01664) for a
     full description.  In practice, symmetricity values larger than 0.2
     are mostly good.

‘--modesymvalue’
     Print the value in the distribution where the mirror and input
     distributions are no longer symmetric, see ‘--mode’ and Appendix C
     of Akhlaghi and Ichikawa 2015 (https://arxiv.org/abs/1505.01664)
     for more.

‘--sigclip-number’
     Number of elements after applying $\sigma$-clipping (see *note
     Sigma clipping::).  $\sigma$-clipping configuration is done with
     the ‘--sigclipparams’ option.

‘--sigclip-median’
     Median after applying $\sigma$-clipping (see *note Sigma
     clipping::).  $\sigma$-clipping configuration is done with the
     ‘--sigclipparams’ option.

     Here is one scenario where this can be useful: assume you have a
     table and you would like to remove the rows that are outliers (not
     within the $\sigma$-clipping range).  Let’s assume your table is
     called ‘table.fits’ and you only want to keep the rows that have a
     value in ‘COLUMN’ within the $\sigma$-clipped range (to $3\sigma$,
     with a tolerance of 0.1).  This command will return the
     $\sigma$-clipped median and standard deviation (used to define the
     range later).

          $ aststatistics table.fits -cCOLUMN --sclipparams=3,0.1 \
                          --sigclip-median --sigclip-std

     You can then use the ‘--range’ option of Table (see *note Table::)
     to select the proper rows.  But for that, you need the actual
     starting and ending values of the range ($m\pm s\sigma$; where $m$
     is the median and $s$ is the multiple of sigma to define an
     outlier).  Therefore, the raw outputs of Statistics in the command
     above aren’t enough.

     To get the starting and ending values of the non-outlier range (and
     put a ‘<,>’ between them, ready to be used in ‘--range’), pipe the
     result into AWK. But in AWK, we’ll also need the multiple of
     $\sigma$, so we’ll define it as a shell variable (‘s’) before
     calling Statistics (note how ‘$s’ is used two times now):

          $ s=3
          $ aststatistics table.fits -cCOLUMN --sclipparams=$s,0.1 \
                          --sigclip-median --sigclip-std           \
               | awk '{s='$s'; printf("%f,%f\n", $1-s*$2, $1+s*$2)}'

     To pass it onto Table, we’ll need to keep the printed output from
     the command above in another shell variable (‘r’), not print it.
     In Bash, can do this by putting the whole statement within a ‘$()’:

          $ s=3
          $ r=$(aststatistics table.fits -cCOLUMN --sclipparams=$s,0.1 \
                              --sigclip-median --sigclip-std           \
                  | awk '{s='$s'; printf("%f,%f\n", $1-s*$2, $1+s*$2)}')
          $ echo $r      # Just to confirm.

     Now you can use Table with the ‘--range’ option to only print the
     rows that have a value in ‘COLUMN’ within the desired range:

          $ asttable table.fits --range=COLUMN,$r

     To save the resulting table (that is clean of outliers) in another
     file (for example named ‘cleaned.fits’, it can also have a ‘.txt’
     suffix), just add ‘--output=cleaned.fits’ to the command above.

‘--sigclip-mean’
     Mean after applying $\sigma$-clipping (see *note Sigma clipping::).
     $\sigma$-clipping configuration is done with the ‘--sigclipparams’
     option.

‘--sigclip-std’
     Standard deviation after applying $\sigma$-clipping (see *note
     Sigma clipping::).  $\sigma$-clipping configuration is done with
     the ‘--sigclipparams’ option.

   The list of options below are for those statistical operations that
output more than one value.  So while they can be called together in one
run, their outputs will be distinct (each one’s output will usually be
printed in more than one line).

‘-A’
‘--asciihist’
     Print an ASCII histogram of the usable values within the input
     dataset along with some basic information like the example below
     (from the UVUDF catalog(2)).  The width and height of the histogram
     (in units of character widths and heights on your command-line
     terminal) can be set with the ‘--numasciibins’ (for the width) and
     ‘--asciiheight’ options.

     For a full description of the histogram, please see *note Histogram
     and Cumulative Frequency Plot::.  An ASCII plot is certainly very
     crude and cannot be used in any publication, but it is very useful
     for getting a general feeling of the input dataset very fast and
     easily on the command-line without having to take your hands off
     the keyboard (which is a major distraction!).  If you want to try
     it out, you can write it all in one line and ignore the <\> and
     extra spaces.

          $ aststatistics uvudf_rafelski_2015.fits.gz --hdu=1         \
                          --column=MAG_F160W --lessthan=40            \
                          --asciihist --numasciibins=55
          ASCII Histogram:
          Number: 8593
          Y: (linear: 0 to 660)
          X: (linear: 17.7735 -- 31.4679, in 55 bins)
           |                                         ****
           |                                        *****
           |                                       ******
           |                                      ********
           |                                      *********
           |                                    ***********
           |                                  **************
           |                                *****************
           |                           ***********************
           |                    ********************************
           |*** ***************************************************
           |-------------------------------------------------------

‘--asciicfp’
     Print the cumulative frequency plot of the usable elements in the
     input dataset.  Please see descriptions under ‘--asciihist’ for
     more, the example below is from the same input table as that
     example.  To better understand the cumulative frequency plot,
     please see *note Histogram and Cumulative Frequency Plot::.

          $ aststatistics uvudf_rafelski_2015.fits.gz --hdu=1         \
                          --column=MAG_F160W --lessthan=40            \
                          --asciicfp --numasciibins=55
          ASCII Cumulative frequency plot:
          Y: (linear: 0 to 8593)
          X: (linear: 17.7735 -- 31.4679, in 55 bins)
           |                                                *******
           |                                             **********
           |                                            ***********
           |                                          *************
           |                                         **************
           |                                        ***************
           |                                      *****************
           |                                    *******************
           |                                ***********************
           |                         ******************************
           |*******************************************************
           |-------------------------------------------------------

‘-H’
‘--histogram’
     Save the histogram of the usable values in the input dataset into a
     table.  The first column is the value at the center of the bin and
     the second is the number of points in that bin.  If the
     ‘--cumulative’ option is also called with this option in a run,
     then the table will have three columns (the third is the cumulative
     frequency plot).  Through the ‘--numbins’, ‘--onebinstart’, or
     ‘--manualbinrange’, you can modify the first column values and with
     ‘--normalize’ and ‘--maxbinone’ you can modify the second columns.
     See below for the description of each.

     By default (when no ‘--output’ is specified) a plain text table
     will be created, see *note Gnuastro text table format::.  If a FITS
     name is specified, you can use the common option ‘--tableformat’ to
     have it as a FITS ASCII or FITS binary format, see *note Common
     options::.  This table can then be fed into your favorite plotting
     tool and get a much more clean and nice histogram than what the raw
     command-line can offer you (with the ‘--asciihist’ option).

‘-C’
‘--cumulative’
     Save the cumulative frequency plot of the usable values in the
     input dataset into a table, similar to ‘--histogram’.

‘-s’
‘--sigmaclip’
     Do $\sigma$-clipping on the usable pixels of the input dataset.
     See *note Sigma clipping:: for a full description on
     $\sigma$-clipping and also to better understand this option.  The
     $\sigma$-clipping parameters can be set through the ‘--sclipparams’
     option (see below).

‘--mirror=FLT’
     Make a histogram and cumulative frequency plot of the mirror
     distribution for the given dataset when the mirror is located at
     the value to this option.  The mirror distribution is fully
     described in Appendix C of Akhlaghi and Ichikawa 2015
     (https://arxiv.org/abs/1505.01664) and currently it is only used to
     calculate the mode (see ‘--mode’).

     Just note that the mirror distribution is a discrete distribution
     like the input, so while you may give any number as the value to
     this option, the actual mirror value is the closest number in the
     input dataset to this value.  If the two numbers are different,
     Statistics will warn you of the actual mirror value used.

     This option will make a table as output.  Depending on your
     selected name for the output, it will be either a FITS table or a
     plain text table (which is the default).  It contains three
     columns: the first is the center of the bins, the second is the
     histogram (with the largest value set to 1) and the third is the
     normalized cumulative frequency plot of the mirror distribution.
     The bins will be positioned such that the mode is on the starting
     interval of one of the bins to make it symmetric around the mirror.
     With this output file and the input histogram (that you can
     generate in another run of Statistics, using the ‘--onebinvalue’),
     it is possible to make plots like Figure 21 of Akhlaghi and
     Ichikawa 2015 (https://arxiv.org/abs/1505.01664).

   The list of options below allow customization of the histogram and
cumulative frequency plots (for the ‘--histogram’, ‘--cumulative’,
‘--asciihist’, and ‘--asciicfp’ options).

‘--numbins’
     The number of bins (rows) to use in the histogram and the
     cumulative frequency plot tables (outputs of ‘--histogram’ and
     ‘--cumulative’).

‘--numasciibins’
     The number of bins (characters) to use in the ASCII plots when
     printing the histogram and the cumulative frequency plot (outputs
     of ‘--asciihist’ and ‘--asciicfp’).

‘--asciiheight’
     The number of lines to use when printing the ASCII histogram and
     cumulative frequency plot on the command-line (outputs of
     ‘--asciihist’ and ‘--asciicfp’).

‘-n’
‘--normalize’
     Normalize the histogram or cumulative frequency plot tables
     (outputs of ‘--histogram’ and ‘--cumulative’).  For a histogram,
     the sum of all bins will become one and for a cumulative frequency
     plot the last bin value will be one.

‘--maxbinone’
     Divide all the histogram values by the maximum bin value so it
     becomes one and the rest are similarly scaled.  In some situations
     (for example if you want to plot the histogram and cumulative
     frequency plot in one plot) this can be very useful.

‘--onebinstart=FLT’
     Make sure that one bin starts with the value to this option.  In
     practice, this will shift the bins used to find the histogram and
     cumulative frequency plot such that one bin’s lower interval
     becomes this value.

     For example when a histogram range includes negative and positive
     values and zero has a special significance in your analysis, then
     zero might fall somewhere in one bin.  As a result that bin will
     have counts of positive and negative.  By setting
     ‘--onebinstart=0’, you can make sure that one bin will only count
     negative values in the vicinity of zero and the next bin will only
     count positive ones in that vicinity.

     Note that by default, the first row of the histogram and cumulative
     frequency plot show the central values of each bin.  So in the
     example above you will not see the 0.000 in the first column, you
     will see two symmetric values.

     If the value is not within the usable input range, this option will
     be ignored.  When it is, this option is the last operation before
     the bins are finalized, therefore it has a higher priority than
     options like ‘--manualbinrange’.

‘--manualbinrange’
     Use the values given to the ‘--greaterequal’ and ‘--lessthan’ to
     define the range of all bin-based calculations like the histogram.
     This option itself doesn’t take any value, but just tells the
     program to use the values of those two options instead of the
     minimum and maximum values of a plot.  If any of the two options
     are not given, then the minimum or maximum will be used
     respectively.  Therefore, if none of them are called calling this
     option is redundant.

     The ‘--onebinstart’ option has a higher priority than this option.
     In other words, ‘--onebinstart’ takes effect after the range has
     been finalized and the initial bins have been defined, therefore it
     has the power to (possibly) shift the bins.  If you want to
     manually set the range of the bins _and_ have one bin on a special
     value, it is thus better to avoid ‘--onebinstart’.

   All the options described until now were from the first class of
operations discussed above: those that treat the whole dataset as one.
However, it often happens that the relative position of the dataset
elements over the dataset is significant.  For example you don’t want
one median value for the whole input image, you want to know how the
median changes over the image.  For such operations, the input has to be
tessellated (see *note Tessellation::).  Thus this class of options
can’t currently be called along with the options above in one run of
Statistics.

‘-t’
‘--ontile’
     Do the respective single-valued calculation over one tile of the
     input dataset, not the whole dataset.  This option must be called
     with at least one of the single valued options discussed above (for
     example ‘--mean’ or ‘--quantile’).  The output will be a file in
     the same format as the input.  If the ‘--oneelempertile’ option is
     called, then one element/pixel will be used for each tile (see
     *note Processing options::).  Otherwise, the output will have the
     same size as the input, but each element will have the value
     corresponding to that tile’s value.  If multiple single valued
     operations are called, then for each operation there will be one
     extension in the output FITS file.

‘-R FLT[,FLT[,FLT...]]’
‘--contour=FLT[,FLT[,FLT...]]’
     Write the contours for the requested levels in a file ending with
     ‘_contour.txt’.  It will have three columns: the first two are the
     coordinates of each point and the third is the level it belongs to
     (one of the input values).  Each disconnected contour region will
     be separated by a blank line.  This is the requested format for
     adding contours with PGFPlots in LaTeX.  If any other format can be
     useful for your work please let us know so we can add it.  If the
     image has World Coordinate System information, the written
     coordinates will be in RA and Dec, otherwise, they will be in pixel
     coordinates.

     Note that currently, this is a very crude/simple implementation,
     please let us know if you find problematic situations so we can fix
     it.

‘-y’
‘--sky’
     Estimate the Sky value on each tile as fully described in *note
     Quantifying signal in a tile::.  As described in that section,
     several options are necessary to configure the Sky estimation which
     are listed below.  The output file will have two extensions: the
     first is the Sky value and the second is the Sky standard deviation
     on each tile.  Similar to ‘--ontile’, if the ‘--oneelempertile’
     option is called, then one element/pixel will be used for each tile
     (see *note Processing options::).

   The parameters for estimating the sky value can be set with the
following options, except for the ‘--sclipparams’ option (which is also
used by the ‘--sigmaclip’), the rest are only used for the Sky value
estimation.

‘-k=STR’
‘--kernel=STR’
     File name of kernel to help in estimating the significance of
     signal in a tile, see *note Quantifying signal in a tile::.

‘--khdu=STR’
     Kernel HDU to help in estimating the significance of signal in a
     tile, see *note Quantifying signal in a tile::.

‘--meanmedqdiff=FLT’
     The maximum acceptable distance between the quantiles of the mean
     and median, see *note Quantifying signal in a tile::.  The initial
     Sky and its standard deviation estimates are measured on tiles
     where the quantiles of their mean and median are less distant than
     the value given to this option.  For example ‘--meanmedqdiff=0.01’
     means that only tiles where the mean’s quantile is between 0.49 and
     0.51 (recall that the median’s quantile is 0.5) will be used.

‘--sclipparams=FLT,FLT’
     The $\sigma$-clipping parameters, see *note Sigma clipping::.  This
     option takes two values which are separated by a comma (<,>).  Each
     value can either be written as a single number or as a fraction of
     two numbers (for example ‘3,1/10’).  The first value to this option
     is the multiple of $\sigma$ that will be clipped ($\alpha$ in that
     section).  The second value is the exit criteria.  If it is less
     than 1, then it is interpreted as tolerance and if it is larger
     than one it is a specific number.  Hence, in the latter case the
     value must be an integer.

‘--outliersclip=FLT,FLT’
     $\sigma$-clipping parameters for the outlier rejection of the Sky
     value (similar to ‘--sclipparams’).

     Outlier rejection is useful when the dataset contains a large and
     diffuse (almost flat within each tile) signal.  The flatness of the
     profile will cause it to successfully pass the mean-median quantile
     difference test, so we’ll need to use the distribution of
     successful tiles for removing these false positive.  For more, see
     the latter half of *note Quantifying signal in a tile::.

‘--outliersigma=FLT’
     Multiple of sigma to define an outlier in the Sky value estimation.
     If this option is given a value of zero, no outlier rejection will
     take place.  For more see ‘--outliersclip’ and the latter half of
     *note Quantifying signal in a tile::.

‘--smoothwidth=INT’
     Width of a flat kernel to convolve the interpolated tile values.
     Tile interpolation is done using the median of the ‘--interpnumngb’
     neighbors of each tile (see *note Processing options::).  If this
     option is given a value of zero or one, no smoothing will be done.
     Without smoothing, strong boundaries will probably be created
     between the values estimated for each tile.  It is thus good to
     smooth the interpolated image so strong discontinuities do not show
     up in the final Sky values.  The smoothing is done through
     convolution (see *note Convolution process::) with a flat kernel,
     so the value to this option must be an odd number.

‘--ignoreblankintiles’
     Don’t set the input’s blank pixels to blank in the tiled outputs
     (for example Sky and Sky standard deviation extensions of the
     output).  This is only applicable when the tiled output has the
     same size as the input, in other words, when ‘--oneelempertile’
     isn’t called.

     By default, blank values in the input (commonly on the edges which
     are outside the survey/field area) will be set to blank in the
     tiled outputs also.  But in other scenarios this default behavior
     is not desired: for example if you have masked something in the
     input, but want the tiled output under that also.

‘--checksky’
     Create a multi-extension FITS file showing the steps that were used
     to estimate the Sky value over the input, see *note Quantifying
     signal in a tile::.  The file will have two extensions for each
     step (one for the Sky and one for the Sky standard deviation).

   ---------- Footnotes ----------

   (1) You can try it by running the command in the ‘tests’ directory,
open the image with a FITS viewer and have a look at it to get a sense
of how these statistics relate to the input image/dataset.

   (2) <https://asd.gsfc.nasa.gov/UVUDF/uvudf_rafelski_2015.fits.gz>


File: gnuastro.info,  Node: NoiseChisel,  Next: Segment,  Prev: Statistics,  Up: Data analysis

7.2 NoiseChisel
===============

Once instrumental signatures are removed from the raw data (image) in
the initial reduction process (see *note Data manipulation::).  You are
naturally eager to start answering the scientific questions that
motivated the data collection in the first place.  However, the raw
dataset/image is just an array of values/pixels, that is all!  These raw
values cannot directly be used to answer your scientific questions: for
example “how many galaxies are there in the image?”.

   The first high-level step in the analysis of your dataset will thus
be to classify, or label, the dataset elements (pixels) into two
classes: 1) Noise, where random effects are the major contributor to the
value, and 2) Signal, where non-random factors (for example light from a
distant galaxy) are present.  This classification of the elements in a
dataset is formally known as _detection_.

   In an observational/experimental dataset, signal is always buried in
noise: only mock/simulated datasets are free of noise.  Therefore
detection, or the process of separating signal from noise, determines
the number of objects you study and the accuracy of any higher-level
measurement you do on them.  Detection is thus the most important step
of any analysis and is not trivial.  In particular, the most
scientifically interesting astronomical targets are faint, can have a
large variety of morphologies, along with a large distribution in
brightness and size.  Therefore when noise is significant, proper
detection of your targets is a uniquely decisive step in your final
scientific analysis/result.

   NoiseChisel is Gnuastro’s program for detection of targets that don’t
have a sharp border (almost all astronomical objects).  When the targets
have sharp edges/borders (for example cells in biological imaging), a
simple threshold is enough to separate them from noise and each other
(if they are not touching).  To detect such sharp-edged targets, you can
use Gnuastro’s Arithmetic program in a command like below (assuming the
threshold is ‘100’, see *note Arithmetic::):

     $ astarithmetic in.fits 100 gt 2 connected-components

   Since almost no astronomical target has such sharp edges, we need a
more advanced detection methodology.  NoiseChisel uses a new noise-based
paradigm for detection of very extended and diffuse targets that are
drowned deeply in the ocean of noise.  It was initially introduced in
Akhlaghi and Ichikawa [2015] (https://arxiv.org/abs/1505.01664) and
improvements after the first four were published in Akhlaghi [2019]
(https://arxiv.org/abs/1909.11230).  Please take the time to go through
these papers to most effectively use NoiseChisel.

   The name of NoiseChisel is derived from the first thing it does after
thresholding the dataset: to erode it.  In mathematical morphology,
erosion on pixels can be pictured as carving-off boundary pixels.
Hence, what NoiseChisel does is similar to what a wood chisel or stone
chisel do.  It is just not a hardware, but a software.  In fact, looking
at it as a chisel and your dataset as a solid cube of rock will greatly
help in effectively understanding and optimally using it: with
NoiseChisel you literally carve your targets out of the noise.  Try
running it with the ‘--checkdetection’ option, and open the temporary
output as a multi-extension cube, to see each step of the carving
process on your input dataset (see *note Viewing multiextension FITS
images::).

   NoiseChisel’s primary output is a binary detection map with the same
size as the input but its pixels only have two values: 0 (background)
and 1 (foreground).  Pixels that don’t harbor any detected signal
(noise) are given a label (or value) of zero and those with a value of 1
have been identified as hosting signal.

   Segmentation is the process of classifying the signal into
higher-level constructs.  For example if you have two separate galaxies
in one image, NoiseChisel will give a value of 1 to the pixels of both
(each forming an “island” of touching foreground pixels).  After
segmentation, the connected foreground pixels will get separate labels,
enabling you to study them individually.  NoiseChisel is only focused on
detection (separating signal from noise), to _segment_ the signal (into
separate galaxies for example), Gnuastro has a separate specialized
program *note Segment::.  NoiseChisel’s output can be directly/readily
fed into Segment.

   For more on NoiseChisel’s output format and its benefits (especially
in conjunction with *note Segment:: and later *note MakeCatalog::),
please see Akhlaghi [2016] (https://arxiv.org/abs/1611.06387).  Just
note that when that paper was published, Segment was not yet spun-off
into a separate program, and NoiseChisel done both detection and
segmentation.

   NoiseChisel’s output is designed to be generic enough to be easily
used in any higher-level analysis.  If your targets are not touching
after running NoiseChisel and you aren’t interested in their
sub-structure, you don’t need the Segment program at all.  You can ask
NoiseChisel to find the connected pixels in the output with the
‘--label’ option.  In this case, the output won’t be a binary image any
more, the signal will have counters/labels starting from 1 for each
connected group of pixels.  You can then directly feed NoiseChisel’s
output into MakeCatalog for measurements over the detections and the
production of a catalog (see *note MakeCatalog::).

   Thanks to the published papers mentioned above, there is no need to
provide a more complete introduction to NoiseChisel in this book.  In
*note Invoking astnoisechisel::, the details of running NoiseChisel and
its options are discussed.

   As discussed above, detection is one of the most important steps for
your scientific result.  It is therefore very important to obtain a good
understanding of NoiseChisel (and afterwards *note Segment:: and *note
MakeCatalog::).  We strongly recommend reviewing two tutorials of *note
General program usage tutorial:: and *note Detecting large extended
targets::.  They are designed to show how to most effectively use
NoiseChisel for the detection of small faint objects and large extended
objects.  In the meantime, they also show the modular principle behind
Gnuastro’s programs and how they are built to complement, and build
upon, each other.

   *note General program usage tutorial:: culminates in using
NoiseChisel to detect galaxies and use its outputs to find the galaxy
colors.  Defining colors is a very common process in most science-cases.
Therefore it is also recommended to (patiently) complete that tutorial
for optimal usage of NoiseChisel in conjunction with all the other
Gnuastro programs.  *note Detecting large extended targets:: shows you
can optimize NoiseChisel’s settings for very extended objects to
successfully carve out to signal-to-noise ratio levels of below 1/10.
After going through those tutorials, play a little with the settings (in
the order presented in the paper and *note Invoking astnoisechisel::) on
a dataset you are familiar with and inspect all the check images
(options starting with ‘--check’) to see the effect of each parameter.

   Below, in *note Invoking astnoisechisel::, we will review
NoiseChisel’s input, detection, and output options in *note NoiseChisel
input::, *note Detection options::, and *note NoiseChisel output::.  If
you have used NoiseChisel within your research, please run it with
‘--cite’ to list the papers you should cite and how to acknowledge its
funding sources.

* Menu:

* Invoking astnoisechisel::     Options and arguments for NoiseChisel.


File: gnuastro.info,  Node: Invoking astnoisechisel,  Prev: NoiseChisel,  Up: NoiseChisel

7.2.1 Invoking NoiseChisel
--------------------------

NoiseChisel will detect signal in noise producing a multi-extension
dataset containing a binary detection map which is the same size as the
input.  Its output can be readily used for input into *note Segment::,
for higher-level segmentation, or *note MakeCatalog:: to do measurements
and generate a catalog.  The executable name is ‘astnoisechisel’ with
the following general template

     $ astnoisechisel [OPTION ...] InputImage.fits

One line examples:

     ## Detect signal in input.fits.
     $ astnoisechisel input.fits

     ## Inspect all the detection steps after changing a parameter.
     $ astnoisechisel input.fits --qthresh=0.4 --checkdetection

     ## Detect signal assuming input has 4 amplifier channels along first
     ## dimension and 1 along the second. Also set the regular tile size
     ## to 100 along both dimensions:
     $ astnoisechisel --numchannels=4,1 --tilesize=100,100 input.fits

If NoiseChisel is to do processing (for example you don’t want to get
help, or see the values to each input parameter), an input image should
be provided with the recognized extensions (see *note Arguments::).
NoiseChisel shares a large set of common operations with other Gnuastro
programs, mainly regarding input/output, general processing steps, and
general operating modes.  To help in a unified experience between all of
Gnuastro’s programs, these operations have the same command-line
options, see *note Common options:: for a full list/description (they
are not repeated here).

   As in all Gnuastro programs, options can also be given to NoiseChisel
in configuration files.  For a thorough description on Gnuastro’s
configuration file parsing, please see *note Configuration files::.  All
of NoiseChisel’s options with a short description are also always
available on the command-line with the ‘--help’ option, see *note
Getting help::.  To inspect the option values without actually running
NoiseChisel, append your command with ‘--printparams’ (or ‘-P’).

   NoiseChisel’s input image may contain blank elements (see *note Blank
pixels::).  Blank elements will be ignored in all steps of NoiseChisel.
Hence if your dataset has bad pixels which should be masked with a mask
image, please use Gnuastro’s *note Arithmetic:: program (in particular
its ‘where’ operator) to convert those pixels to blank pixels before
running NoiseChisel.  Gnuastro’s Arithmetic program has bitwise
operators helping you select specific kinds of bad-pixels when
necessary.

   A convolution kernel can also be optionally given.  If a value (file
name) is given to ‘--kernel’ on the command-line or in a configuration
file (see *note Configuration files::), then that file will be used to
convolve the image prior to thresholding.  Otherwise a default kernel
will be used.  The default kernel is a 2D Gaussian with a FWHM of 2
pixels truncated at 5 times the FWHM. This choice of the default kernel
is discussed in Section 3.1.1 of Akhlaghi and Ichikawa [2015]
(https://arxiv.org/abs/1505.01664).  See *note Convolution kernel:: for
kernel related options.  Passing ‘none’ to ‘--kernel’ will disable
convolution.  On the other hand, through the ‘--convolved’ option, you
may provide an already convolved image, see descriptions below for more.

   NoiseChisel defines two tessellations over the input (see *note
Tessellation::).  This enables it to deal with possible gradients in the
input dataset and also significantly improve speed by processing each
tile on different threads simultaneously.  Tessellation related options
are discussed in *note Processing options::.  In particular, NoiseChisel
uses two tessellations (with everything between them identical except
the tile sizes): a fine-grained one with smaller tiles (used in
thresholding and Sky value estimations) and another with larger tiles
which is used for pseudo-detections over non-detected regions of the
image.  The common Tessellation options described in *note Processing
options:: define all parameters of both tessellations.  The large tile
size for the latter tessellation is set through the ‘--largetilesize’
option.  To inspect the tessellations on your input dataset, run
NoiseChisel with ‘--checktiles’.

*Usage TIP:* Frequently use the options starting with ‘--check’.  Since
the noise properties differ between different datasets, you can often
play with the parameters/options for a better result than the default
parameters.  You can start with ‘--checkdetection’ for the main steps.
For the full list of NoiseChisel’s checking options please run:
     $ astnoisechisel --help | grep check

   Below, we’ll discuss NoiseChisel’s options, classified into two
general classes, to help in easy navigation.  *note NoiseChisel input::
mainly discusses the basic options relating to inputs and prior to the
detection process detection.  Afterwards, *note Detection options::
fully describes every configuration parameter (option) related to
detection and how they affect the final result.  The order of options in
this section follow the logical order within NoiseChisel.  On first
reading (while you are still new to NoiseChisel), it is therefore
strongly recommended to read the options in the given order below.  The
output of ‘--printparams’ (or ‘-P’) also has this order.  However, the
output of ‘--help’ is sorted alphabetically.  Finally, in *note
NoiseChisel output:: the format of NoiseChisel’s output is discussed.

* Menu:

* NoiseChisel input::           NoiseChisel’s input options.
* Detection options::           Configure detection in NoiseChisel.
* NoiseChisel output::          NoiseChisel’s output options and format.


File: gnuastro.info,  Node: NoiseChisel input,  Next: Detection options,  Prev: Invoking astnoisechisel,  Up: Invoking astnoisechisel

7.2.1.1 NoiseChisel input
.........................

The options here can be used to configure the inputs and output of
NoiseChisel, along with some general processing options.  Recall that
you can always see the full list of Gnuastro’s options with the ‘--help’
(see *note Getting help::), or ‘--printparams’ (or ‘-P’) to see their
values (see *note Operating mode options::).

‘-k STR’
‘--kernel=STR’
     File name of kernel to smooth the image before applying the
     threshold, see *note Convolution kernel::.  If no convolution is
     needed, give this option a value of ‘none’.

     The first step of NoiseChisel is to convolve/smooth the image and
     use the convolved image in multiple steps including the finding and
     applying of the quantile threshold (see ‘--qthresh’).

     The ‘--kernel’ option is not mandatory.  If not called, a 2D
     Gaussian profile with a FWHM of 2 pixels truncated at 5 times the
     FWHM is used.  This choice of the default kernel is discussed in
     Section 3.1.1 of Akhlaghi and Ichikawa [2015].  You can use
     MakeProfiles to build a kernel with any of its recognized profile
     types and parameters.  For more details, please see *note
     MakeProfiles output dataset::.  For example, the command below will
     make a Moffat kernel (with $\beta=2.8$) with FWHM of 2 pixels
     truncated at 10 times the FWHM.

          $ astmkprof --oversample=1 --kernel=moffat,2,2.8,10

     Since convolution can be the slowest step of NoiseChisel, for large
     datasets, you can convolve the image once with Gnuastro’s Convolve
     (see *note Convolve::), and use the ‘--convolved’ option to feed it
     directly to NoiseChisel.  This can help getting faster results when
     you are playing/testing the higher-level options.

‘--khdu=STR’
     HDU containing the kernel in the file given to the ‘--kernel’
     option.

‘--convolved=STR’
     Use this file as the convolved image and don’t do convolution
     (ignore ‘--kernel’).  NoiseChisel will just check the size of the
     given dataset is the same as the input’s size.  If a wrong image
     (with the same size) is given to this option, the results (errors,
     bugs, etc) are unpredictable.  So please use this option with care
     and in a highly controlled environment, for example in the scenario
     discussed below.

     In almost all situations, as the input gets larger, the single most
     CPU (and time) consuming step in NoiseChisel (and other programs
     that need a convolved image) is convolution.  Therefore minimizing
     the number of convolutions can save a significant amount of time in
     some scenarios.  One such scenario is when you want to segment
     NoiseChisel’s detections using the same kernel (with *note
     Segment::, which also supports this ‘--convolved’ option).  This
     scenario would require two convolutions of the same dataset: once
     by NoiseChisel and once by Segment.  Using this option in both
     programs, only one convolution (prior to running NoiseChisel) is
     enough.

     Another common scenario where this option can be convenient is when
     you are testing NoiseChisel (or Segment) for the best parameters.
     You have to run NoiseChisel multiple times and see the effect of
     each change.  However, once you are happy with the kernel,
     re-convolving the input on every change of higher-level parameters
     will greatly hinder, or discourage, further testing.  With this
     option, you can convolve the input image with your chosen kernel
     once before running NoiseChisel, then feed it to NoiseChisel on
     each test run and thus save valuable time for better/more tests.

     To build your desired convolution kernel, you can use *note
     MakeProfiles::.  To convolve the image with a given kernel you can
     use *note Convolve::.  Spatial domain convolution is mandatory: in
     the frequency domain, blank pixels (if present) will cover the
     whole image and gradients will appear on the edges, see *note
     Spatial vs. Frequency domain::.

     Below you can see an example of the second scenario: you want to
     see how variation of the growth level (through the ‘--detgrowquant’
     option) will affect the final result.  Recall that you can ignore
     all the extra spaces, new lines, and backslash’s (‘‘\’’) if you are
     typing in the terminal.  In a shell script, remove the ‘$’ signs at
     the start of the lines.

          ## Make the kernel to convolve with.
          $ astmkprof --oversample=1 --kernel=gaussian,2,5

          ## Convolve the input with the given kernel.
          $ astconvolve input.fits --kernel=kernel.fits                \
                        --domain=spatial --output=convolved.fits

          ## Run NoiseChisel with seven growth quantile values.
          $ for g in 60 65 70 75 80 85 90; do                          \
              astnoisechisel input.fits --convolved=convolved.fits     \
                             --detgrowquant=0.$g --output=$g.fits;     \
            done

‘--chdu=STR’
     The HDU/extension containing the convolved image in the file given
     to ‘--convolved’.

‘-w STR’
‘--widekernel=STR’
     File name of a wider kernel to use in estimating the difference of
     the mode and median in a tile (this difference is used to identify
     the significance of signal in that tile, see *note Quantifying
     signal in a tile::).  As displayed in Figure 4 of Akhlaghi and
     Ichikawa [2015] (https://arxiv.org/abs/1505.01664), a wider kernel
     will help in identifying the skewness caused by data in noise.  The
     image that is convolved with this kernel is _only_ used for this
     purpose.  Once the mode is found to be sufficiently close to the
     median, the quantile threshold is found on the image convolved with
     the sharper kernel (‘--kernel’), see ‘--qthresh’).

     Since convolution will significantly slow down the processing, this
     feature is optional.  When it isn’t given, the image that is
     convolved with ‘--kernel’ will be used to identify good tiles _and_
     apply the quantile threshold.  This option is mainly useful in
     conditions were you have a very large, extended, diffuse signal
     that is still present in the usable tiles when using ‘--kernel’.
     See *note Detecting large extended targets:: for a practical
     demonstration on how to inspect the tiles used in identifying the
     quantile threshold.

‘--whdu=STR’
     HDU containing the kernel file given to the ‘--widekernel’ option.

‘-L INT[,INT]’
‘--largetilesize=INT[,INT]’
     The size of each tile for the tessellation with the larger tile
     sizes.  Except for the tile size, all the other parameters for this
     tessellation are taken from the common options described in *note
     Processing options::.  The format is identical to that of the
     ‘--tilesize’ option that is discussed in that section.


File: gnuastro.info,  Node: Detection options,  Next: NoiseChisel output,  Prev: NoiseChisel input,  Up: Invoking astnoisechisel

7.2.1.2 Detection options
.........................

Detection is the process of separating the pixels in the image into two
groups: 1) Signal, and 2) Noise.  Through the parameters below, you can
customize the detection process in NoiseChisel.  Recall that you can
always see the full list of NoiseChisel’s options with the ‘--help’ (see
*note Getting help::), or ‘--printparams’ (or ‘-P’) to see their values
(see *note Operating mode options::).

‘-Q FLT’
‘--meanmedqdiff=FLT’
     The maximum acceptable distance between the quantiles of the mean
     and median in each tile, see *note Quantifying signal in a tile::.
     The quantile threshold estimates are measured on tiles where the
     quantiles of their mean and median are less distant than the value
     given to this option.  For example ‘--meanmedqdiff=0.01’ means that
     only tiles where the mean’s quantile is between 0.49 and 0.51
     (recall that the median’s quantile is 0.5) will be used.

‘--outliersclip=FLT,FLT’
     $\sigma$-clipping parameters for the outlier rejection of the
     quantile threshold.  The format of the given values is similar to
     ‘--sigmaclip’ below.  In NoiseChisel, outlier rejection on tiles is
     used when identifying the quantile thresholds (‘--qthresh’,
     ‘--noerodequant’, and ‘detgrowquant’).

     Outlier rejection is useful when the dataset contains a large and
     diffuse (almost flat within each tile) signal.  The flatness of the
     profile will cause it to successfully pass the mean-median quantile
     difference test, so we’ll need to use the distribution of
     successful tiles for removing these false positives.  For more, see
     the latter half of *note Quantifying signal in a tile::.

‘--outliersigma=FLT’
     Multiple of sigma to define an outlier.  If this option is given a
     value of zero, no outlier rejection will take place.  For more see
     ‘--outliersclip’ and the latter half of *note Quantifying signal in
     a tile::.

‘-t FLT’
‘--qthresh=FLT’
     The quantile threshold to apply to the convolved image.  The
     detection process begins with applying a quantile threshold to each
     of the tiles in the small tessellation.  The quantile is only
     calculated for tiles that don’t have any significant signal within
     them, see *note Quantifying signal in a tile::.  Interpolation is
     then used to give a value to the unsuccessful tiles and it is
     finally smoothed.

     The quantile value is a floating point value between 0 and 1.
     Assume that we have sorted the $N$ data elements of a distribution
     (the pixels in each mesh on the convolved image).  The quantile
     ($q$) of this distribution is the value of the element with an
     index of (the nearest integer to) $q\times{N}$ in the sorted data
     set.  After thresholding is complete, we will have a binary (two
     valued) image.  The pixels above the threshold are known as
     foreground pixels (have a value of 1) while those which lie below
     the threshold are known as background (have a value of 0).

‘--smoothwidth=INT’
     Width of flat kernel used to smooth the interpolated quantile
     thresholds, see ‘--qthresh’ for more.

‘--checkqthresh’
     Check the quantile threshold values on the mesh grid.  A file
     suffixed with ‘_qthresh.fits’ will be created showing each step.
     With this option, NoiseChisel will abort as soon as quantile
     estimation has been completed, allowing you to inspect the steps
     leading to the final quantile threshold, this can be disabled with
     ‘--continueaftercheck’.  By default the output will have the same
     pixel size as the input, but with the ‘--oneelempertile’ option,
     only one pixel will be used for each tile (see *note Processing
     options::).

‘--blankasforeground’
     In the erosion and opening steps below, treat blank elements as
     foreground (regions above the threshold).  By default, blank
     elements in the dataset are considered to be background, so if a
     foreground pixel is touching it, it will be eroded.  This option is
     irrelevant if the datasets contains no blank elements.

     When there are many blank elements in the dataset, treating them as
     foreground will systematically erode their regions less, therefore
     systematically creating more false positives.  So use this option
     (when blank values are present) with care.

‘-e INT’
‘--erode=INT’
     The number of erosions to apply to the binary thresholded image.
     Erosion is simply the process of flipping (from 1 to 0) any of the
     foreground pixels that neighbor a background pixel.  In a 2D image,
     there are two kinds of neighbors, 4-connected and 8-connected
     neighbors.  You can specify which type of neighbors should be used
     for erosion with the ‘--erodengb’ option, see below.

     Erosion has the effect of shrinking the foreground pixels.  To put
     it another way, it expands the holes.  This is a founding principle
     in NoiseChisel: it exploits the fact that with very low thresholds,
     the holes in the very low surface brightness regions of an image
     will be smaller than regions that have no signal.  Therefore by
     expanding those holes, we are able to separate the regions
     harboring signal.

‘--erodengb=INT’
     The type of neighborhood (structuring element) used in erosion, see
     ‘--erode’ for an explanation on erosion.  Only two integer values
     are acceptable: 4 or 8.  In 4-connectivity, the neighbors of a
     pixel are defined as the four pixels on the top, bottom, right and
     left of a pixel that share an edge with it.  The 8-connected
     neighbors on the other hand include the 4-connected neighbors along
     with the other 4 pixels that share a corner with this pixel.  See
     Figure 6 (a) and (b) in Akhlaghi and Ichikawa (2015) for a
     demonstration.

‘--noerodequant’
     Pure erosion is going to carve off sharp and small objects
     completely out of the detected regions.  This option can be used to
     avoid missing such sharp and small objects (which have significant
     pixels, but not over a large area).  All pixels with a value larger
     than the significance level specified by this option will not be
     eroded during the erosion step above.  However, they will undergo
     the erosion and dilation of the opening step below.

     Like the ‘--qthresh’ option, the significance level is determined
     using the quantile (a value between 0 and 1).  Just as a reminder,
     in the normal distribution, $1\sigma$, $1.5\sigma$, and $2\sigma$
     are approximately on the 0.84, 0.93, and 0.98 quantiles.

‘-p INT’
‘--opening=INT’
     Depth of opening to be applied to the eroded binary image.  Opening
     is a composite operation.  When opening a binary image with a depth
     of $n$, $n$ erosions (explained in ‘--erode’) are followed by $n$
     dilations.  Simply put, dilation is the inverse of erosion.  When
     dilating an image any background pixel is flipped (from 0 to 1) to
     become a foreground pixel.  Dilation has the effect of fattening
     the foreground.  Note that in NoiseChisel, the erosion which is
     part of opening is independent of the initial erosion that is done
     on the thresholded image (explained in ‘--erode’).  The structuring
     element for the opening can be specified with the ‘--openingngb’
     option.  Opening has the effect of removing the thin foreground
     connections (mostly noise) between separate foreground ‘islands’
     (detections) thereby completely isolating them.  Once opening is
     complete, we have _initial_ detections.

‘--openingngb=INT’
     The structuring element used for opening, see ‘--erodengb’ for more
     information about a structuring element.

‘--skyfracnoblank’
     Ignore blank pixels when estimating the fraction of undetected
     pixels for Sky estimation.  NoiseChisel only measures the Sky over
     the tiles that have a sufficiently large fraction of undetected
     pixels (value given to ‘--minskyfrac’).  By default this fraction
     is found by dividing number of undetected pixels in a tile by the
     tile’s area.  But this default behavior ignores the possibility of
     blank pixels.  In situations that blank/masked pixels are scattered
     across the image and if they are large enough, all the tiles can
     fail the ‘--minskyfrac’ test, thus not allowing NoiseChisel to
     proceed.  With this option, such scenarios can be fixed: the
     denominator of the fraction will be the number of non-blank
     elements in the tile, not the total tile area.

‘-B FLT’
‘--minskyfrac=FLT’
     Minimum fraction (value between 0 and 1) of Sky (undetected) areas
     in a tile.  Only tiles with a fraction of undetected pixels (Sky)
     larger than this value will be used to estimate the Sky value.
     NoiseChisel uses this option value twice to estimate the Sky value:
     after initial detections and in the end when false detections have
     been removed.

     Because of the PSF and their intrinsic amorphous properties,
     astronomical objects (except cosmic rays) never have a clear cutoff
     and commonly sink into the noise very slowly.  Even below the very
     low thresholds used by NoiseChisel.  So when a large fraction of
     the area of one mesh is covered by detections, it is very plausible
     that their faint wings are present in the undetected regions (hence
     causing a bias in any measurement).  To get an accurate measurement
     of the above parameters over the tessellation, tiles that harbor
     too many detected regions should be excluded.  The used tiles are
     visible in the respective ‘--check’ option of the given step.

‘--checkdetsky’
     Check the initial approximation of the sky value and its standard
     deviation in a FITS file ending with ‘_detsky.fits’.  With this
     option, NoiseChisel will abort as soon as the sky value used for
     defining pseudo-detections is complete.  This allows you to inspect
     the steps leading to the final quantile threshold, this behavior
     can be disabled with ‘--continueaftercheck’.  By default the output
     will have the same pixel size as the input, but with the
     ‘--oneelempertile’ option, only one pixel will be used for each
     tile (see *note Processing options::).

‘-s FLT,FLT’
‘--sigmaclip=FLT,FLT’
     The $\sigma$-clipping parameters for measuring the initial and
     final Sky values from the undetected pixels, see *note Sigma
     clipping::.

     This option takes two values which are separated by a comma (<,>).
     Each value can either be written as a single number or as a
     fraction of two numbers (for example ‘3,1/10’).  The first value to
     this option is the multiple of $\sigma$ that will be clipped
     ($\alpha$ in that section).  The second value is the exit criteria.
     If it is less than 1, then it is interpreted as tolerance and if it
     is larger than one it is assumed to be the fixed number of
     iterations.  Hence, in the latter case the value must be an
     integer.

‘-R FLT’
‘--dthresh=FLT’
     The detection threshold: a multiple of the initial Sky standard
     deviation added with the initial Sky approximation (which you can
     inspect with ‘--checkdetsky’).  This flux threshold is applied to
     the initially undetected regions on the unconvolved image.  The
     background pixels that are completely engulfed in a 4-connected
     foreground region are converted to background (holes are filled)
     and one opening (depth of 1) is applied over both the initially
     detected and undetected regions.  The Signal to noise ratio of the
     resulting ‘pseudo-detections’ are used to identify true vs.  false
     detections.  See Section 3.1.5 and Figure 7 in Akhlaghi and
     Ichikawa (2015) for a very complete explanation.

‘--dopening=INT’
     The number of openings to do after applying ‘--dthresh’.

‘--dopeningngb=INT’
     The connectivity used in the opening of ‘--dopening’.  In a 2D
     image this must be either 4 or 8.  The stronger the connectivity,
     the more smaller regions will be discarded.

‘--holengb=INT’
     The connectivity (defined by the number of neighbors) to fill holes
     after applying ‘--dthresh’ (above) to find pseudo-detections.  For
     example in a 2D image it must be 4 (the neighbors that are most
     strongly connected) or 8 (all neighbors).  The stronger the
     connectivity, the stronger the hole will be enclosed.  So setting a
     value of 8 in a 2D image means that the walls of the hole are
     4-connected.  If standard (near Sky level) values are given to
     ‘--dthresh’, setting ‘--holengb=4’, might fill the complete dataset
     and thus not create enough pseudo-detections.

‘--pseudoconcomp=INT’
     The connectivity (defined by the number of neighbors) to find
     individual pseudo-detections.  If it is a weaker connectivity (4 in
     a 2D image), then pseudo-detections that are connected on the
     corners will be treated as separate.

‘-m INT’
‘--snminarea=INT’
     The minimum area to calculate the Signal to noise ratio on the
     pseudo-detections of both the initially detected and undetected
     regions.  When the area in a pseudo-detection is too small, the
     Signal to noise ratio measurements will not be accurate and their
     distribution will be heavily skewed to the positive.  So it is best
     to ignore any pseudo-detection that is smaller than this area.  Use
     ‘--detsnhistnbins’ to check if this value is reasonable or not.

‘--checksn’
     Save the S/N values of the pseudo-detections (and possibly grown
     detections if ‘--cleangrowndet’ is called) into separate tables.
     If ‘--tableformat’ is a FITS table, each table will be written into
     a separate extension of one file suffixed with ‘_detsn.fits’.  If
     it is plain text, a separate file will be made for each table
     (ending in ‘_detsn_sky.txt’, ‘_detsn_det.txt’ and
     ‘_detsn_grown.txt’).  For more on ‘--tableformat’ see *note Input
     output options::.

     You can use these to inspect the S/N values and their distribution
     (in combination with the ‘--checkdetection’ option to see where the
     pseudo-detections are).  You can use Gnuastro’s *note Statistics::
     to make a histogram of the distribution or any other analysis you
     would like for better understanding of the distribution (for
     example through a histogram).

‘--minnumfalse=INT’
     The minimum number of ‘pseudo-detections’ over the undetected
     regions to identify a Signal-to-Noise ratio threshold.  The Signal
     to noise ratio (S/N) of false pseudo-detections in each tile is
     found using the quantile of the S/N distribution of the
     pseudo-detections over the undetected pixels in each mesh.  If the
     number of S/N measurements is not large enough, the quantile will
     not be accurate (can have large scatter).  For example if you set
     ‘--snquant=0.99’ (or the top 1 percent), then it is best to have at
     least 100 S/N measurements.

‘-c FLT’
‘--snquant=FLT’
     The quantile of the Signal to noise ratio distribution of the
     pseudo-detections in each mesh to use for filling the large mesh
     grid.  Note that this is only calculated for the large mesh grids
     that satisfy the minimum fraction of undetected pixels (value of
     ‘--minbfrac’) and minimum number of pseudo-detections (value of
     ‘--minnumfalse’).

‘--snthresh=FLT’
     Manually set the signal-to-noise ratio of true pseudo-detections.
     With this option, NoiseChisel will not attempt to find
     pseudo-detections over the noisy regions of the dataset, but will
     directly go onto applying the manually input value.

     This option is useful in crowded images where there is no blank sky
     to find the sky pseudo-detections.  You can get this value on a
     similarly reduced dataset (from another region of the Sky with more
     undetected regions spaces).

‘-d FLT’
‘--detgrowquant=FLT’
     Quantile limit to “grow” the final detections.  As discussed in the
     previous options, after applying the initial quantile threshold,
     layers of pixels are carved off the objects to identify true
     signal.  With this step you can return those low surface brightness
     layers that were carved off back to the detections.  To disable
     growth, set the value of this option to ‘1’.

     The process is as follows: after the true detections are found, all
     the non-detected pixels above this quantile will be put in a list
     and used to “grow” the true detections (seeds of the growth).  Like
     all quantile thresholds, this threshold is defined and applied to
     the convolved dataset.  Afterwards, the dataset is dilated once
     (with minimum connectivity) to connect very thin regions on the
     boundary: imagine building a dam at the point rivers spill into an
     open sea/ocean.  Finally, all holes are filled.  In the geography
     metaphor, holes can be seen as the closed (by the dams) rivers and
     lakes, so this process is like turning the water in all such rivers
     and lakes into soil.  See ‘--detgrowmaxholesize’ for configuring
     the hole filling.

‘--detgrowmaxholesize=INT’
     The maximum hole size to fill during the final expansion of the
     true detections as described in ‘--detgrowquant’.  This is
     necessary when the input contains many smaller objects and can be
     used to avoid marking blank sky regions as detections.

     For example multiple galaxies can be positioned such that they
     surround an empty region of sky.  If all the holes are filled, the
     Sky region in between them will be taken as a detection which is
     not desired.  To avoid such cases, the integer given to this option
     must be smaller than the hole between such objects.  However, we
     should caution that unless the “hole” is very large, the combined
     faint wings of the galaxies might actually be present in between
     them, so be very careful in not filling such holes.

     On the other hand, if you have a very large (and extended) galaxy,
     the diffuse wings of the galaxy may create very large holes over
     the detections.  In such cases, a large enough value to this option
     will cause all such holes to be detected as part of the large
     galaxy and thus help in detecting it to extremely low surface
     brightness limits.  Therefore, especially when large and extended
     objects are present in the image, it is recommended to give this
     option (very) large values.  For one real-world example, see *note
     Detecting large extended targets::.

‘--cleangrowndet’
     After dilation, if the signal-to-noise ratio of a detection is less
     than the derived pseudo-detection S/N limit, that detection will be
     discarded.  In an ideal/clean noise, a true detection’s S/N should
     be larger than its constituent pseudo-detections because its area
     is larger and it also covers more signal.  However, on a false
     detections (especially at lower ‘--snquant’ values), the increase
     in size can cause a decrease in S/N below that threshold.

     This will improve purity and not change completeness (a true
     detection will not be discarded).  Because a true detection has
     flux in its vicinity and dilation will catch more of that flux and
     increase the S/N. So on a true detection, the final S/N cannot be
     less than pseudo-detections.

     However, in many real images bad processing creates artifacts that
     cannot be accurately removed by the Sky subtraction.  In such
     cases, this option will decrease the completeness (will
     artificially discard true detections).  So this feature is not
     default and should to be explicitly called when you know the noise
     is clean.

‘--checkdetection’
     Every step of the detection process will be added as an extension
     to a file with the suffix ‘_det.fits’.  Going through each would
     just be a repeat of the explanations above and also of those in
     Akhlaghi and Ichikawa (2015).  The extension label should be
     sufficient to recognize which step you are observing.  Viewing all
     the steps can be the best guide in choosing the best set of
     parameters.  With this option, NoiseChisel will abort as soon as a
     snapshot of all the detection process is saved.  This behavior can
     be disabled with ‘--continueaftercheck’.

‘--checksky’
     Check the derivation of the final sky and its standard deviation
     values on the mesh grid.  With this option, NoiseChisel will abort
     as soon as the sky value is estimated over the image (on each
     tile).  This behavior can be disabled with ‘--continueaftercheck’.
     By default the output will have the same pixel size as the input,
     but with the ‘--oneelempertile’ option, only one pixel will be used
     for each tile (see *note Processing options::).


File: gnuastro.info,  Node: NoiseChisel output,  Prev: Detection options,  Up: Invoking astnoisechisel

7.2.1.3 NoiseChisel output
..........................

NoiseChisel’s output is a multi-extension FITS file.  The main
extension/dataset is a (binary) detection map.  It has the same size as
the input but with only two possible values for all pixels: 0 (for
pixels identified as noise) and 1 (for those identified as
signal/detections).  The detection map is followed by a Sky and Sky
standard deviation dataset (which are calculated from the binary image).
By default (when ‘--rawoutput’ isn’t called), NoiseChisel will also
subtract the Sky value from the input and save the sky-subtracted input
as the first extension in the output with data.  The zero-th extension
(that contains no data), contains NoiseChisel’s configuration as FITS
keywords, see *note Output FITS files::.

   The name of the output file can be set by giving a value to
‘--output’ (this is a common option between all programs and is
therefore discussed in *note Input output options::).  If ‘--output’
isn’t used, the input name will be suffixed with ‘_detected.fits’ and
used as output, see *note Automatic output::.  If any of the options
starting with ‘--check*’ are given, NoiseChisel won’t complete and will
abort as soon as the respective check images are created.  For more
information on the different check images, see the description for the
‘--check*’ options in *note Detection options:: (this can be disabled
with ‘--continueaftercheck’).

   The last two extensions of the output are the Sky and its Standard
deviation, see *note Sky value:: for a complete explanation.  They are
calculated on the tile grid that you defined for NoiseChisel.  By
default these datasets will have the same size as the input, but with
all the pixels in one tile given one value.  To be more space-efficient
(keep only one pixel per tile), you can use the ‘--oneelempertile’
option, see *note Tessellation::.

   To inspect any of NoiseChisel’s output files, assuming you use SAO
DS9, you can configure your Graphic User Interface (GUI) to open
NoiseChisel’s output as a multi-extension data cube.  This will allow
you to flip through the different extensions and visually inspect the
results.  This process has been described for the GNOME GUI (most common
GUI in GNU/Linux operating systems) in *note Viewing multiextension FITS
images::.

   NoiseChisel’s output configuration options are described in detail
below.

‘--continueaftercheck’
     Continue NoiseChisel after any of the options starting with
     ‘--check’ (see *note Detection options::.  NoiseChisel involves
     many steps and as a result, there are many checks, allowing you to
     inspect the status of the processing.  The results of each step
     affect the next steps of processing.  Therefore, when you want to
     check the status of the processing at one step, the time spent to
     complete NoiseChisel is just wasted/distracting time.

     To encourage easier experimentation with the option values, when
     you use any of the NoiseChisel options that start with ‘--check’,
     NoiseChisel will abort once its desired extensions have been
     written.  With ‘--continueaftercheck’ option, you can disable this
     behavior and ask NoiseChisel to continue with the rest of the
     processing, even after the requested check files are complete.

‘--ignoreblankintiles’
     Don’t set the input’s blank pixels to blank in the tiled outputs
     (for example Sky and Sky standard deviation extensions of the
     output).  This is only applicable when the tiled output has the
     same size as the input, in other words, when ‘--oneelempertile’
     isn’t called.

     By default, blank values in the input (commonly on the edges which
     are outside the survey/field area) will be set to blank in the
     tiled outputs also.  But in other scenarios this default behavior
     is not desired: for example if you have masked something in the
     input, but want the tiled output under that also.

‘-l’
‘--label’
     Run a connected-components algorithm on the finally detected pixels
     to identify which pixels are connected to which.  By default the
     main output is a binary dataset with only two values: 0 (for noise)
     and 1 (for signal/detections).  See *note NoiseChisel output:: for
     more.

     The purpose of NoiseChisel is to detect targets that are extended
     and diffuse, with outer parts that sink into the noise very
     gradually (galaxies and stars for example).  Since NoiseChisel digs
     down to extremely low surface brightness values, many such targets
     will commonly be detected together as a single large body of
     connected pixels.

     To properly separate connected objects, sophisticated segmentation
     methods are commonly necessary on NoiseChisel’s output.  Gnuastro
     has the dedicated *note Segment:: program for this job.  Since
     input images are commonly large and can take a significant volume,
     the extra volume necessary to store the labels of the connected
     components in the detection map (which will be created with this
     ‘--label’ option, in 32-bit signed integer type) can thus be a
     major waste of space.  Since the default output is just a binary
     dataset, an 8-bit unsigned dataset is enough.

     The binary output will also encourage users to segment the result
     separately prior to doing higher-level analysis.  As an alternative
     to ‘--label’, if you have the binary detection image, you can use
     the ‘connected-components’ operator in Gnuastro’s Arithmetic
     program to identify regions that are connected with each other.
     For example with this command (assuming NoiseChisel’s output is
     called ‘nc.fits’):

          $ astarithmetic nc.fits 2 connected-components -hDETECTIONS

‘--rawoutput’
     Don’t include the Sky-subtracted input image as the first extension
     of the output.  By default, the Sky-subtracted input is put in the
     first extension of the output.  The next extensions are
     NoiseChisel’s main outputs described above.

     The extra Sky-subtracted input can be convenient in checking
     NoiseChisel’s output and comparing the detection map with the
     input: visually see if everything you expected is detected
     (reasonable completeness) and that you don’t have too many false
     detections (reasonable purity).  This visual inspection is
     simplified if you use SAO DS9 to view NoiseChisel’s output as a
     multi-extension data-cube, see *note Viewing multiextension FITS
     images::.

     When you are satisfied with your NoiseChisel configuration
     (therefore you don’t need to check on every run), or you want to
     archive/transfer the outputs, or the datasets become large, or you
     are running NoiseChisel as part of a pipeline, this Sky-subtracted
     input image can be a significant burden (take up a large volume).
     The fact that the input is also noisy, makes it hard to compress it
     efficiently.

     In such cases, this ‘--rawoutput’ can be used to avoid the extra
     sky-subtracted input in the output.  It is always possible to
     easily produce the Sky-subtracted dataset from the input (assuming
     it is in extension ‘1’ of ‘in.fits’) and the ‘SKY’ extension of
     NoiseChisel’s output (let’s call it ‘nc.fits’) with a command like
     below (assuming NoiseChisel wasn’t run with ‘--oneelempertile’, see
     *note Tessellation::):

          $ astarithmetic in.fits nc.fits - -h1 -hSKY

*Save space:* with the ‘--rawoutput’ and ‘--oneelempertile’,
NoiseChisel’s output will only be one binary detection map and two much
smaller arrays with one value per tile.  Since none of these have noise
they can be compressed very effectively (without any loss of data) with
exceptionally high compression ratios.  This makes it easy to archive,
or transfer, NoiseChisel’s output even on huge datasets.  To compress it
with the most efficient method (take up less volume), run the following
command:

     $ gzip --best noisechisel_output.fits

The resulting ‘.fits.gz’ file can then be fed into any of Gnuastro’s
programs directly, or viewed in viewers like SAO DS9, without having to
decompress it separately (they will just take a little longer, because
they have to internally decompress it before starting).  See *note
NoiseChisel optimization for storage:: for an example on a real dataset.


File: gnuastro.info,  Node: Segment,  Next: MakeCatalog,  Prev: NoiseChisel,  Up: Data analysis

7.3 Segment
===========

Once signal is separated from noise (for example with *note
NoiseChisel::), you have a binary dataset: each pixel is either signal
(1) or noise (0).  Signal (for example every galaxy in your image) has
been “detected”, but all detections have a label of 1.  Therefore while
we know which pixels contain signal, we still can’t find out how many
galaxies they contain or which detected pixels correspond to which
galaxy.  At the lowest (most generic) level, detection is a kind of
segmentation (segmenting the whole dataset into signal and noise, see
*note NoiseChisel::).  Here, we’ll define segmentation only on signal:
to separate sub-structure within the detections.

   If the targets are clearly separated, or their detected regions
aren’t touching, a simple connected components(1) algorithm (very basic
segmentation) is enough to separate the regions that are
touching/connected.  This is such a basic and simple form of
segmentation that Gnuastro’s Arithmetic program has an operator for it:
see ‘connected-components’ in *note Arithmetic operators::.  Assuming
the binary dataset is called ‘binary.fits’, you can use it with a
command like this:

     $ astarithmetic binary.fits 2 connected-components

You can even do a very basic detection (a threshold, say at value ‘100’)
_and_ segmentation in Arithmetic with a single command like below:

     $ astarithmetic in.fits 100 gt 2 connected-components

   However, in most astronomical situations our targets are not nicely
separated or have a sharp boundary/edge (for a threshold to suffice):
they touch (for example merging galaxies), or are simply in the same
line-of-sight (which is much more common).  This causes their images to
overlap.

   In particular, when you do your detection with NoiseChisel, you will
detect signal to very low surface brightness limits: deep into the faint
wings of galaxies or bright stars (which can extend very far and
irregularly from their center).  Therefore, it often happens that
several galaxies are detected as one large detection.  Since they are
touching, a simple connected components algorithm will not suffice.  It
is therefore necessary to do a more sophisticated segmentation and break
up the detected pixels (even those that are touching) into multiple
target objects as accurately as possible.

   Segment will use a detection map and its corresponding dataset to
find sub-structure over the detected areas and use them for its
segmentation.  Until Gnuastro version 0.6 (released in 2018), Segment
was part of *note NoiseChisel::.  Therefore, similar to NoiseChisel, the
best place to start reading about Segment and understanding what it does
(with many illustrative figures) is Section 3.2 of Akhlaghi and Ichikawa
[2015] (https://arxiv.org/abs/1505.01664), and continue with Akhlaghi
[2019] (https://arxiv.org/abs/1909.11230).

   As a summary, Segment first finds true _clump_s over the detections.
Clumps are associated with local maxima/minima(2) and extend over the
neighboring pixels until they reach a local minimum/maximum
(_river_/_watershed_).  By default, Segment will use the distribution of
clump signal-to-noise ratios over the undetected regions as reference to
find “true” clumps over the detections.  Using the undetected regions
can be disabled by directly giving a signal-to-noise ratio to
‘--clumpsnthresh’.

   The true clumps are then grown to a certain threshold over the
detections.  Based on the strength of the connections
(rivers/watersheds) between the grown clumps, they are considered parts
of one _object_ or as separate _object_s.  See Section 3.2 of Akhlaghi
and Ichikawa [2015] (https://arxiv.org/abs/1505.01664) for more.
Segment’s main output are thus two labeled datasets: 1) clumps, and 2)
objects.  See *note Segment output:: for more.

   To start learning about Segment, especially in relation to detection
(*note NoiseChisel::) and measurement (*note MakeCatalog::), the
recommended references are Akhlaghi and Ichikawa [2015]
(https://arxiv.org/abs/1505.01664), Akhlaghi [2016]
(https://arxiv.org/abs/1611.06387) and Akhlaghi [2019]
(https://arxiv.org/abs/1909.11230).  If you have used Segment within
your research, please run it with ‘--cite’ to list the papers you should
cite and how to acknowledge its funding sources.

   Those papers cannot be updated any more but the software will evolve.
For example Segment became a separate program (from NoiseChisel) in 2018
(after those papers were published).  Therefore this book is the
definitive reference.  Finally, in *note Invoking astsegment::, we’ll
discuss Segment’s inputs, outputs and configuration options.

* Menu:

* Invoking astsegment::         Inputs, outputs and options to Segment

   ---------- Footnotes ----------

   (1) <https://en.wikipedia.org/wiki/Connected-component_labeling>

   (2) By default the maximum is used as the first clump pixel, to
define clumps based on local minima, use the ‘--minima’ option.


File: gnuastro.info,  Node: Invoking astsegment,  Prev: Segment,  Up: Segment

7.3.1 Invoking Segment
----------------------

Segment will identify substructure within the detected regions of an
input image.  Segment’s output labels can be directly used for
measurements (for example with *note MakeCatalog::).  The executable
name is ‘astsegment’ with the following general template

     $ astsegment [OPTION ...] InputImage.fits

One line examples:

     ## Segment NoiseChisel's detected regions.
     $ astsegment default-noisechisel-output.fits

     ## Use a hand-input S/N value for keeping true clumps
     ## (avoid finding the S/N using the undetected regions).
     $ astsegment nc-out.fits --clumpsnthresh=10

     ## Inspect all the segmentation steps after changing a parameter.
     $ astsegment input.fits --snquant=0.9 --checksegmentaion

     ## Use the fixed value of 0.01 for the input's Sky standard deviation
     ## (in the units of the input), and assume all the pixels are a
     ## detection (for example a large structure extending over the whole
     ## image), and only keep clumps with S/N>10 as true clumps.
     $ astsegment in.fits --std=0.01 --detection=all --clumpsnthresh=10

If Segment is to do processing (for example you don’t want to get help,
or see the values of each option), at least one input dataset is
necessary along with detection and error information, either as separate
datasets (per-pixel) or fixed values, see *note Segment input::.
Segment shares a large set of common operations with other Gnuastro
programs, mainly regarding input/output, general processing steps, and
general operating modes.  To help in a unified experience between all of
Gnuastro’s programs, these common operations have the same names and
defined in *note Common options::.

   As in all Gnuastro programs, options can also be given to Segment in
configuration files.  For a thorough description of Gnuastro’s
configuration file parsing, please see *note Configuration files::.  All
of Segment’s options with a short description are also always available
on the command-line with the ‘--help’ option, see *note Getting help::.
To inspect the option values without actually running Segment, append
your command with ‘--printparams’ (or ‘-P’).

   To help in easy navigation between Segment’s options, they are
separately discussed in the three sub-sections below: *note Segment
input:: discusses how you can customize the inputs to Segment.  *note
Segmentation options:: is devoted to options specific to the high-level
segmentation process.  Finally, in *note Segment output::, we’ll discuss
options that affect Segment’s output.

* Menu:

* Segment input::               Input files and options.
* Segmentation options::        Parameters of the segmentation process.
* Segment output::              Outputs of Segment


File: gnuastro.info,  Node: Segment input,  Next: Segmentation options,  Prev: Invoking astsegment,  Up: Invoking astsegment

7.3.1.1 Segment input
.....................

Besides the input dataset (for example astronomical image), Segment also
needs to know the Sky standard deviation and the regions of the dataset
that it should segment.  The values dataset is assumed to be Sky
subtracted by default.  If it isn’t, you can ask Segment to subtract the
Sky internally by calling ‘--sky’.  For the rest of this discussion,
we’ll assume it is already sky subtracted.

   The Sky and its standard deviation can be a single value (to be used
for the whole dataset) or a separate dataset (for a separate value per
pixel).  If a dataset is used for the Sky and its standard deviation,
they must either be the size of the input image, or have a single value
per tile (generated with ‘--oneelempertile’, see *note Processing
options:: and *note Tessellation::).

   The detected regions/pixels can be specified as a detection map (for
example see *note NoiseChisel output::).  If ‘--detection=all’, Segment
won’t read any detection map and assume the whole input is a single
detection.  For example when the dataset is fully covered by a large
nearby galaxy/globular cluster.

   When dataset are to be used for any of the inputs, Segment will
assume they are multiple extensions of a single file by default (when
‘--std’ or ‘--detection’ aren’t called).  For example NoiseChisel’s
default output *note NoiseChisel output::.  When the Sky-subtracted
values are in one file, and the detection and Sky standard deviation are
in another, you just need to use ‘--detection’: in the absence of
‘--std’, Segment will look for both the detection labels and Sky
standard deviation in the file given to ‘--detection’.  Ultimately, if
all three are in separate files, you need to call both ‘--detection’ and
‘--std’.

   The extensions of the three mandatory inputs can be specified with
‘--hdu’, ‘--dhdu’, and ‘--stdhdu’.  For a full discussion on what to
give to these options, see the description of ‘--hdu’ in *note Input
output options::.  To see their default values (along with all the other
options), run Segment with the ‘--printparams’ (or ‘-P’) option.  Just
recall that in the absence of ‘--detection’ and ‘--std’, all three are
assumed to be in the same file.  If you only want to see Segment’s
default values for HDUs on your system, run this command:

     $ astsegment -P | grep hdu

   By default Segment will convolve the input with a kernel to improve
the signal-to-noise ratio of true peaks.  If you already have the
convolved input dataset, you can pass it directly to Segment for faster
processing (using the ‘--convolved’ and ‘--chdu’ options).  Just don’t
forget that the convolved image must also be Sky-subtracted before
calling Segment.  If a value/file is given to ‘--sky’, the convolved
values will also be Sky subtracted internally.  Alternatively, if you
prefer to give a kernel (with ‘--kernel’ and ‘--khdu’), Segment can do
the convolution internally.  To disable convolution, use
‘--kernel=none’.

‘--sky=STR/FLT’
     The Sky value(s) to subtract from the input.  This option can
     either be given a constant number or a file name containing a
     dataset (multiple values, per pixel or per tile).  By default,
     Segment will assume the input dataset is Sky subtracted, so this
     option is not mandatory.

     If the value can’t be read as a number, it is assumed to be a file
     name.  When the value is a file, the extension can be specified
     with ‘--skyhdu’.  When its not a single number, the given dataset
     must either have the same size as the output or the same size as
     the tessellation (so there is one pixel per tile, see *note
     Tessellation::).

     When this option is given, its value(s) will be subtracted from the
     input and the (optional) convolved dataset (given to ‘--convolved’)
     prior to starting the segmentation process.

‘--skyhdu=STR/INT’
     The HDU/extension containing the Sky values.  This is mandatory
     when the value given to ‘--sky’ is not a number.  Please see the
     description of ‘--hdu’ in *note Input output options:: for the
     different ways you can identify a special extension.

‘--std=STR/FLT’
     The Sky standard deviation value(s) corresponding to the input.
     The value can either be a constant number or a file name containing
     a dataset (multiple values, per pixel or per tile).  The Sky
     standard deviation is mandatory for Segment to operate.

     If the value can’t be read as a number, it is assumed to be a file
     name.  When the value is a file, the extension can be specified
     with ‘--skyhdu’.  When its not a single number, the given dataset
     must either have the same size as the output or the same size as
     the tessellation (so there is one pixel per tile, see *note
     Tessellation::).

     When this option is not called, Segment will assume the standard
     deviation is a dataset and in a HDU/extension (‘--stdhdu’) of
     another one of the input file(s).  If a file is given to
     ‘--detection’, it will assume that file contains the standard
     deviation dataset, otherwise, it will look into input filename (the
     main argument, without any option).

‘--stdhdu=INT/STR’
     The HDU/extension containing the Sky standard deviation values,
     when the value given to ‘--std’ is a file name.  Please see the
     description of ‘--hdu’ in *note Input output options:: for the
     different ways you can identify a special extension.

‘--variance’
     The input Sky standard deviation value/dataset is actually
     variance.  When this option is called, the square root of input Sky
     standard deviation (see ‘--std’) is used internally, not its raw
     value(s).

‘-d STR’
‘--detection=STR’
     Detection map to use for segmentation.  If given a value of ‘all’,
     Segment will assume the whole dataset must be segmented, see below.
     If a detection map is given, the extension can be specified with
     ‘--dhdu’.  If not given, Segment will assume the desired
     HDU/extension is in the main input argument (input file specified
     with no option).

     The final segmentation (clumps or objects) will only be over the
     non-zero pixels of this detection map.  The dataset must have the
     same size as the input image.  Only datasets with an integer type
     are acceptable for the labeled image, see *note Numeric data
     types::.  If your detection map only has integer values, but it is
     stored in a floating point container, you can use Gnuastro’s
     Arithmetic program (see *note Arithmetic::) to convert it to an
     integer container, like the example below:

          $ astarithmetic float.fits int32 --output=int.fits

     It may happen that the whole input dataset is covered by signal,
     for example when working on parts of the Andromeda galaxy, or
     nearby globular clusters (that cover the whole field of view).  In
     such cases, segmentation is necessary over the complete dataset,
     not just specific regions (detections).  By default Segment will
     first use the undetected regions as a reference to find the proper
     signal-to-noise ratio of “true” clumps (give a purity level
     specified with ‘--snquant’).  Therefore, in such scenarios you also
     need to manually give a “true” clump signal-to-noise ratio with the
     ‘--clumpsnthresh’ option to disable looking into the undetected
     regions, see *note Segmentation options::.  In such cases, is
     possible to make a detection map that only has the value ‘1’ for
     all pixels (for example using *note Arithmetic::), but for
     convenience, you can also use ‘--detection=all’.

‘--dhdu’
     The HDU/extension containing the detection map given to
     ‘--detection’.  Please see the description of ‘--hdu’ in *note
     Input output options:: for the different ways you can identify a
     special extension.

‘-k STR’
‘--kernel=STR’
     The kernel used to convolve the input image.  The usage of this
     option is identical to NoiseChisel’s ‘--kernel’ option (*note
     NoiseChisel input::).  Please see the descriptions there for more.
     To disable convolution, you can give it a value of ‘none’.

‘--khdu’
     The HDU/extension containing the kernel used for convolution.  For
     acceptable values, please see the description of ‘--hdu’ in *note
     Input output options::.

‘--convolved’
     The convolved image to avoid internal convolution by Segment.  The
     usage of this option is identical to NoiseChisel’s ‘--convolved’
     option.  Please see *note NoiseChisel input:: for a thorough
     discussion of the usefulness and best practices of using this
     option.

     If you want to use the same convolution kernel for detection (with
     *note NoiseChisel::) and segmentation, with this option, you can
     use the same convolved image (that is also available in
     NoiseChisel) and avoid two convolutions.  However, just be careful
     to use the input to NoiseChisel as the input to Segment also, then
     use the ‘--sky’ and ‘--std’ to specify the Sky and its standard
     deviation (from NoiseChisel’s output).  Recall that when
     NoiseChisel is not called with ‘--rawoutput’, the first extension
     of NoiseChisel’s output is the _Sky-subtracted_ input (see *note
     NoiseChisel output::).  So if you use the same convolved image that
     you fed to NoiseChisel, but use NoiseChisel’s output with Segment’s
     ‘--convolved’, then the convolved image won’t be Sky subtracted.

‘--chdu’
     The HDU/extension containing the convolved image (given to
     ‘--convolved’).  For acceptable values, please see the description
     of ‘--hdu’ in *note Input output options::.

‘-L INT[,INT]’
‘--largetilesize=INT[,INT]’
     The size of the large tiles to use for identifying the clump S/N
     threshold over the undetected regions.  The usage of this option is
     identical to NoiseChisel’s ‘--largetilesize’ option (*note
     NoiseChisel input::).  Please see the descriptions there for more.

     The undetected regions can be a significant fraction of the dataset
     and finding clumps requires sorting of the desired regions, which
     can be slow.  To speed up the processing, Segment finds clumps in
     the undetected regions over separate large tiles.  This allows it
     to have to sort a much smaller set of pixels and also to treat them
     independently and in parallel.  Both these issues greatly speed it
     up.  Just be sure to not decrease the large tile sizes too much
     (less than 100 pixels in each dimension).  It is important for them
     to be much larger than the clumps.

