This is gnuastro.info, produced by makeinfo version 6.7 from
gnuastro.texi.

This book documents version 0.12 of the GNU Astronomy Utilities
(Gnuastro).  Gnuastro provides various programs and libraries for
astronomical data manipulation and analysis.

   Copyright © 2015-2020, Free Software Foundation, Inc.

     Permission is granted to copy, distribute and/or modify this
     document under the terms of the GNU Free Documentation License,
     Version 1.3 or any later version published by the Free Software
     Foundation; with no Invariant Sections, no Front-Cover Texts, and
     no Back-Cover Texts.  A copy of the license is included in the
     section entitled “GNU Free Documentation License”.
INFO-DIR-SECTION Astronomy
START-INFO-DIR-ENTRY
* Gnuastro: (gnuastro).       GNU Astronomy Utilities.
* libgnuastro: (gnuastro)Gnuastro library. Full Gnuastro library doc.

* help-gnuastro: (gnuastro)help-gnuastro mailing list. Getting help.

* bug-gnuastro: (gnuastro)Report a bug. How to report bugs

* Arithmetic: (gnuastro)Arithmetic. Arithmetic operations on pixels.
* astarithmetic: (gnuastro)Invoking astarithmetic. Options to Arithmetic.

* BuildProgram: (gnuastro)BuildProgram. Compile and run programs using Gnuastro’s library.
* astbuildprog: (gnuastro)Invoking astbuildprog. Options to BuildProgram.

* ConvertType: (gnuastro)ConvertType. Convert different file types.
* astconvertt: (gnuastro)Invoking astconvertt. Options to ConvertType.

* Convolve: (gnuastro)Convolve. Convolve an input file with kernel.
* astconvolve: (gnuastro)Invoking astconvolve. Options to Convolve.

* CosmicCalculator: (gnuastro)CosmicCalculator. For cosmological params.
* astcosmiccal: (gnuastro)Invoking astcosmiccal. Options to CosmicCalculator.

* Crop: (gnuastro)Crop. Crop region(s) from image(s).
* astcrop: (gnuastro)Invoking astcrop. Options to Crop.

* Fits: (gnuastro)Fits. View and manipulate FITS extensions and keywords.
* astfits: (gnuastro)Invoking astfits. Options to Fits.

* MakeCatalog: (gnuastro)MakeCatalog. Make a catalog from labeled image.
* astmkcatalog: (gnuastro)Invoking astmkcatalog. Options to MakeCatalog.

* MakeNoise: (gnuastro)MakeNoise. Make (add) noise to an image.
* astmknoise: (gnuastro)Invoking astmknoise. Options to MakeNoise.

* MakeProfiles: (gnuastro)MakeProfiles. Make mock profiles.
* astmkprof: (gnuastro)Invoking astmkprof. Options to MakeProfiles.

* Match: (gnuastro)Match. Match two separate catalogs.
* astmatch: (gnuastro)Invoking astmatch. Options to Match.

* NoiseChisel: (gnuastro)NoiseChisel. Detect signal in noise.
* astnoisechisel: (gnuastro)Invoking astnoisechisel. Options to NoiseChisel.

* Segment: (gnuastro)Segment. Segment detections based on signal structure.
* astsegment: (gnuastro)Invoking astsegment. Options to Segment.

* Statistics: (gnuastro)Statistics. Get image Statistics.
* aststatistics: (gnuastro)Invoking aststatistics. Options to Statistics.

* Table: (gnuastro)Table. Read and write FITS binary or ASCII tables.
* asttable: (gnuastro)Invoking asttable. Options to Table.

* Warp: (gnuastro)Warp. Warp a dataset to a new grid.
* astwarp: (gnuastro)Invoking astwarp. Options to Warp.

* astscript-sort-by-night: (gnuastro)Invoking astscript-sort-by-night. Options to this script

END-INFO-DIR-ENTRY


File: gnuastro.info,  Node: Segmentation options,  Next: Segment output,  Prev: Segment input,  Up: Invoking astsegment

7.3.1.2 Segmentation options
............................

The options below can be used to configure every step of the
segmentation process in the Segment program.  For a more complete
explanation (with figures to demonstrate each step), please see Section
3.2 of Akhlaghi and Ichikawa [2015] (https://arxiv.org/abs/1505.01664),
and also *note Segment::.  By default, Segment will follow the procedure
described in the paper to find the S/N threshold based on the noise
properties.  This can be disabled by directly giving a trustable
signal-to-noise ratio to the ‘--clumpsnthresh’ option.

   Recall that you can always see the full list of Gnuastro’s options
with the ‘--help’ (see *note Getting help::), or ‘--printparams’ (or
‘-P’) to see their values (see *note Operating mode options::).

‘-B FLT’
‘--minskyfrac=FLT’
     Minimum fraction (value between 0 and 1) of Sky (undetected) areas
     in a large tile.  Only (large) tiles with a fraction of undetected
     pixels (Sky) greater than this value will be used for finding
     clumps.  The clumps found in the undetected areas will be used to
     estimate a S/N threshold for true clumps.  Therefore this is an
     important option (to decrease) in crowded fields.  Operationally,
     this is almost identical to NoiseChisel’s ‘--minskyfrac’ option
     (*note Detection options::).  Please see the descriptions there for
     more.

‘--minima’
     Build the clumps based on the local minima, not maxima.  By
     default, clumps are built starting from local maxima (see Figure 8
     of Akhlaghi and Ichikawa [2015]
     (https://arxiv.org/abs/1505.01664)).  Therefore, this option can be
     useful when you are searching for true local minima (for example
     absorption features).

‘-m INT’
‘--snminarea=INT’
     The minimum area which a clump in the undetected regions should
     have in order to be considered in the clump Signal to noise ratio
     measurement.  If this size is set to a small value, the Signal to
     noise ratio of false clumps will not be accurately found.  It is
     recommended that this value be larger than the value to
     NoiseChisel’s ‘--snminarea’.  Because the clumps are found on the
     convolved (smoothed) image while the pseudo-detections are found on
     the input image.  You can use ‘--checksn’ and ‘--checksegmentation’
     to see if your chosen value is reasonable or not.

‘--checksn’
     Save the S/N values of the clumps over the sky and detected regions
     into separate tables.  If ‘--tableformat’ is a FITS format, each
     table will be written into a separate extension of one file
     suffixed with ‘_clumpsn.fits’.  If it is plain text, a separate
     file will be made for each table (ending in ‘_clumpsn_sky.txt’ and
     ‘_clumpsn_det.txt’).  For more on ‘--tableformat’ see *note Input
     output options::.

     You can use these tables to inspect the S/N values and their
     distribution (in combination with the ‘--checksegmentation’ option
     to see where the clumps are).  You can use Gnuastro’s *note
     Statistics:: to make a histogram of the distribution (ready for
     plotting in a text file, or a crude ASCII-art demonstration on the
     command-line).

     With this option, Segment will abort as soon as the two tables are
     created.  This allows you to inspect the steps leading to the final
     S/N quantile threshold, this behavior can be disabled with
     ‘--continueaftercheck’.

‘--minnumfalse=INT’
     The minimum number of clumps over undetected (Sky) regions to
     identify the requested Signal-to-Noise ratio threshold.
     Operationally, this is almost identical to NoiseChisel’s
     ‘--minnumfalse’ option (*note Detection options::).  Please see the
     descriptions there for more.

‘-c FLT’
‘--snquant=FLT’
     The quantile of the signal-to-noise ratio distribution of clumps in
     undetected regions, used to define true clumps.  After identifying
     all the usable clumps in the undetected regions of the dataset, the
     given quantile of their signal-to-noise ratios is used to define
     the signal-to-noise ratio of a “true” clump.  Effectively, this can
     be seen as an inverse p-value measure.  See Figure 9 and Section
     3.2.1 of Akhlaghi and Ichikawa [2015]
     (https://arxiv.org/abs/1505.01664) for a complete explanation.  The
     full distribution of clump signal-to-noise ratios over the
     undetected areas can be saved into a table with ‘--checksn’ option
     and visually inspected with ‘--checksegmentation’.

‘-v’
‘--keepmaxnearriver’
     Keep a clump whose maximum (minimum if ‘--minima’ is called) flux
     is 8-connected to a river pixel.  By default such clumps over
     detections are considered to be noise and are removed irrespective
     of their brightness (see *note Flux Brightness and magnitude::).
     Over large profiles, that sink into the noise very slowly, noise
     can cause part of the profile (which was flat without noise) to
     become a very large and with a very high Signal to noise ratio.  In
     such cases, the pixel with the maximum flux in the clump will be
     immediately touching a river pixel.

‘-s FLT’
‘--clumpsnthresh=FLT’
     The signal-to-noise threshold for true clumps.  If this option is
     given, then the segmentation options above will be ignored and the
     given value will be directly used to identify true clumps over the
     detections.  This can be useful if you have a large dataset with
     similar noise properties.  You can find a robust signal-to-noise
     ratio based on a (sufficiently large) smaller portion of the
     dataset.  Afterwards, with this option, you can speed up the
     processing on the whole dataset.  Other scenarios where this option
     may be useful is when, the image might not contain enough/any Sky
     regions.

‘-G FLT’
‘--gthresh=FLT’
     Threshold (multiple of the sky standard deviation added with the
     sky) to stop growing true clumps.  Once true clumps are found, they
     are set as the basis to segment the detected region.  They are
     grown until the threshold specified by this option.

‘-y INT’
‘--minriverlength=INT’
     The minimum length of a river between two grown clumps for it to be
     considered in signal-to-noise ratio estimations.  Similar to
     ‘--snminarea’, if the length of the river is too short, the
     signal-to-noise ratio can be noisy and unreliable.  Any existing
     rivers shorter than this length will be considered as non-existent,
     independent of their Signal to noise ratio.  The clumps are grown
     on the input image, therefore this value can be smaller than the
     value given to ‘--snminarea’.  Recall that the clumps were defined
     on the convolved image so ‘--snminarea’ should be larger.

‘-O FLT’
‘--objbordersn=FLT’
     The maximum Signal to noise ratio of the rivers between two grown
     clumps in order to consider them as separate ‘objects’.  If the
     Signal to noise ratio of the river between two grown clumps is
     larger than this value, they are defined to be part of one
     ‘object’.  Note that the physical reality of these ‘objects’ can
     never be established with one image, or even multiple images from
     one broad-band filter.  Any method we devise to define ‘object’s
     over a detected region is ultimately subjective.

     Two very distant galaxies or satellites in one halo might lie in
     the same line of sight and be detected as clumps on one detection.
     On the other hand, the connection (through a spiral arm or tidal
     tail for example) between two parts of one galaxy might have such a
     low surface brightness that they are broken up into multiple
     detections or objects.  In fact if you have noticed, exactly for
     this purpose, this is the only Signal to noise ratio that the user
     gives into NoiseChisel.  The ‘true’ detections and clumps can be
     objectively identified from the noise characteristics of the image,
     so you don’t have to give any hand input Signal to noise ratio.

‘--checksegmentation’
     A file with the suffix ‘_seg.fits’ will be created.  This file
     keeps all the relevant steps in finding true clumps and segmenting
     the detections into multiple objects in various extensions.  Having
     read the paper or the steps above.  Examining this file can be an
     excellent guide in choosing the best set of parameters.  Note that
     calling this function will significantly slow NoiseChisel.  In
     verbose mode (without the ‘--quiet’ option, see *note Operating
     mode options::) the important steps (along with their extension
     names) will also be reported.

     With this option, NoiseChisel will abort as soon as the two tables
     are created.  This behavior can be disabled with
     ‘--continueaftercheck’.


File: gnuastro.info,  Node: Segment output,  Prev: Segmentation options,  Up: Invoking astsegment

7.3.1.3 Segment output
......................

The main output of Segment are two label datasets (with integer types,
separating the dataset’s elements into different classes).  They have
HDU/extension names of ‘CLUMPS’ and ‘OBJECTS’.

   Similar to all Gnuastro’s FITS outputs, the zero-th extension/HDU of
the main output file only contains header keywords and image or table.
It contains the Segment input files and parameters (option names and
values) as FITS keywords.  Note that if an option name is longer than 8
characters, the keyword name is the second word.  The first word is
‘HIERARCH’.  Also note that according to the FITS standard, the keyword
names must be in capital letters, therefore, if you want to use Grep to
inspect these keywords, use the ‘-i’ option, like the example below.

     $ astfits image_segmented.fits -h0 | grep -i snquant

   By default, besides the ‘CLUMPS’ and ‘OBJECTS’ extensions, Segment’s
output will also contain the (technically redundant) input dataset and
the sky standard deviation dataset (if it wasn’t a constant number).
This can help in visually inspecting the result when viewing the images
as a “Multi-extension data cube” in SAO DS9 for example (see *note
Viewing multiextension FITS images::).  You can simply flip through the
extensions and see the same region of the image and its corresponding
clumps/object labels.  It also makes it easy to feed the output (as one
file) into MakeCatalog when you intend to make a catalog afterwards (see
*note MakeCatalog::.  To remove these redundant extensions from the
output (for example when designing a pipeline), you can use
‘--rawoutput’.

   The ‘OBJECTS’ and ‘CLUMPS’ extensions can be used as input into *note
MakeCatalog:: to generate a catalog for higher-level analysis.  If you
want to treat each clump separately, you can give a very large value (or
even a NaN, which will always fail) to the ‘--gthresh’ option (for
example ‘--gthresh=1e10’ or ‘--gthresh=nan’), see *note Segmentation
options::.

   For a complete definition of clumps and objects, please see Section
3.2 of Akhlaghi and Ichikawa [2015] (https://arxiv.org/abs/1505.01664)
and *note Segmentation options::.  The clumps are “true” local maxima
(minima if ‘--minima’ is called) and their surrounding pixels until a
local minimum/maximum (caused by noise fluctuations, or another “true”
clump).  Therefore it may happen that some of the input detections
aren’t covered by clumps at all (very diffuse objects without any strong
peak), while some objects may contain many clumps.  Even in those that
have clumps, there will be regions that are too diffuse.  The diffuse
regions (within the input detected regions) are given a negative label
(-1) to help you separate them from the undetected regions (with a value
of zero).

   Each clump is labeled with respect to its host object.  Therefore, if
an object has three clumps for example, the clumps within it have labels
1, 2 and 3.  As a result, if an initial detected region has multiple
objects, each with a single clump, all the clumps will have a label of
1.  The total number of clumps in the dataset is stored in the ‘NCLUMPS’
keyword of the ‘CLUMPS’ extension and printed in the verbose output of
Segment (when ‘--quiet’ is not called).

   The ‘OBJECTS’ extension of the output will give a positive
counter/label to every detected pixel in the input.  As described in
Akhlaghi and Ichikawa [2015], the true clumps are grown until a certain
threshold.  If the grown clumps touch other clumps and the connection is
strong enough, they are considered part of the same _object_.  Once
objects (grown clumps) are identified, they are grown to cover the whole
detected area.

   The options to configure the output of Segment are listed below:

‘--continueaftercheck’
     Don’t abort Segment after producing the check image(s).  The usage
     of this option is identical to NoiseChisel’s ‘--continueaftercheck’
     option (*note NoiseChisel input::).  Please see the descriptions
     there for more.

‘--onlyclumps’
     Abort Segment after finding true clumps and don’t continue with
     finding options.  Therefore, no ‘OBJECTS’ extension will be present
     in the output.  Each true clump in ‘CLUMPS’ will get a unique
     label, but diffuse regions will still have a negative value.

     To make a catalog of the clumps, the input detection map (where all
     the labels are one) can be fed into *note MakeCatalog:: along with
     the input detection map to Segment (that only had a value of ‘1’
     for all detected pixels) with ‘--clumpscat’.  In this way,
     MakeCatalog will assume all the clumps belong to a single “object”.

‘--grownclumps’
     In the output ‘CLUMPS’ extension, store the grown clumps.  If a
     detected region contains no clumps or only one clump, then it will
     be fully given a label of ‘1’ (no negative valued pixels).

‘--rawoutput’
     Only write the ‘CLUMPS’ and ‘OBJECTS’ datasets in the output file.
     Without this option (by default), the first and last extensions of
     the output will the Sky-subtracted input dataset and the Sky
     standard deviation dataset (if it wasn’t a number).  When the
     datasets are small, these redundant extensions can make it
     convenient to inspect the results visually or feed the output to
     *note MakeCatalog:: for measurements.  Ultimately both the input
     and Sky standard deviation datasets are redundant (you had them
     before running Segment).  When the inputs are large/numerous, these
     extra dataset can be a burden.

*Save space:* with the ‘--rawoutput’, Segment’s output will only be two
labeled datasets (only containing integers).  Since they have no noise,
such datasets can be compressed very effectively (without any loss of
data) with exceptionally high compression ratios.  You can use the
following command to compress it with the best ratio:

     $ gzip --best segment_output.fits

The resulting ‘.fits.gz’ file can then be fed into any of Gnuastro’s
programs directly, without having to decompress it separately (it will
just take them a little longer, because they have to decompress it
internally before use).


File: gnuastro.info,  Node: MakeCatalog,  Next: Match,  Prev: Segment,  Up: Data analysis

7.4 MakeCatalog
===============

At the lowest level, a dataset (for example an image) is just a
collection of values, placed after each other in any number of
dimensions (for example an image is a 2D dataset).  Each data-element
(pixel) just has two properties: its position (relative to the rest) and
its value.  In higher-level analysis, an entire dataset (an image for
example) is rarely treated as a singular entity(1).  You usually want to
know/measure the properties of the (separate) scientifically interesting
targets that are embedded in it.  For example the magnitudes, positions
and elliptical properties of the galaxies that are in the image.

   MakeCatalog is Gnuastro’s program for localized measurements over a
dataset.  In other words, MakeCatalog is Gnuastro’s program to convert
low-level datasets (like images), to high level catalogs.  The role of
MakeCatalog in a scientific analysis and the benefits of its model
(where detection/segmentation is separated from measurement) is
discussed in Akhlaghi [2016] (https://arxiv.org/abs/1611.06387v1)(2) and
summarized in *note Detection and catalog production::.  We strongly
recommend reading this short paper for a better understanding of this
methodology.  Understanding the effective usage of MakeCatalog, will
thus also help effective use of other (lower-level) Gnuastro’s programs
like *note NoiseChisel:: or *note Segment::.

   It is important to define your regions of interest for measurements
_before_ running MakeCatalog.  MakeCatalog is specialized in doing
measurements accurately and efficiently.  Therefore MakeCatalog will not
do detection, segmentation, or defining apertures on requested positions
in your dataset.  Following Gnuastro’s modularity principle, there are
separate and highly specialized and customizable programs in Gnuastro
for these other jobs as shown below (for a usage example in a real-world
analysis, see *note General program usage tutorial:: and *note Detecting
large extended targets::).

   • *note Arithmetic::: Detection with a simple threshold.

   • *note NoiseChisel::: Advanced detection.

   • *note Segment::: Segmentation (substructure over detections).

   • *note MakeProfiles::: Aperture creation for known positions.

   These programs will/can return labeled dataset(s) to be fed into
MakeCatalog.  A labeled dataset for measurement has the same
size/dimensions as the input, but with integer valued pixels that have
the label/counter for each sub-set of pixels that must be measured
together.  For example all the pixels covering one galaxy in an image,
get the same label.

   The requested measurements are then done on similarly labeled pixels.
The final result is a catalog where each row corresponds to the
measurements on pixels with a specific label.  For example the flux
weighted average position of all the pixels with a label of 42 will be
written into the 42nd row of the output catalog/table’s central position
column(3).  Similarly, the sum of all these pixels will be the 42nd row
in the brightness column, etc.  Pixels with labels equal to, or smaller
than, zero will be ignored by MakeCatalog.  In other words, the number
of rows in MakeCatalog’s output is already known before running it (the
maximum value of the labeled dataset).

   Before getting into the details of running MakeCatalog (in *note
Invoking astmkcatalog::, we’ll start with a discussion on the basics of
its approach to separating detection from measurements in *note
Detection and catalog production::.  A very important factor in any
measurement is understanding its validity range, or limits.  Therefore
in *note Quantifying measurement limits::, we’ll discuss how to estimate
the reliability of the detection and basic measurements.  This section
will continue with a derivation of elliptical parameters from the
labeled datasets in *note Measuring elliptical parameters::.  For those
who feel MakeCatalog’s existing measurements/columns aren’t enough and
would like to add further measurements, in *note Adding new columns to
MakeCatalog::, a checklist of steps is provided for readily adding your
own new measurements/columns.

* Menu:

* Detection and catalog production::  Discussing why/how to treat these separately
* Quantifying measurement limits::  For comparing different catalogs.
* Measuring elliptical parameters::  Estimating elliptical parameters.
* Adding new columns to MakeCatalog::  How to add new columns.
* Invoking astmkcatalog::       Options and arguments to MakeCatalog.

   ---------- Footnotes ----------

   (1) You can derive the over-all properties of a complete dataset (1D
table column, 2D image, or 3D data-cube) treated as a single entity with
Gnuastro’s Statistics program (see *note Statistics::).

   (2) A published paper cannot undergo any more change, so this manual
is the definitive guide.

   (3) See *note Measuring elliptical parameters:: for a discussion on
this and the derivation of positional parameters, which includes the
center.


File: gnuastro.info,  Node: Detection and catalog production,  Next: Quantifying measurement limits,  Prev: MakeCatalog,  Up: MakeCatalog

7.4.1 Detection and catalog production
--------------------------------------

Most existing common tools in low-level astronomical data-analysis (for
example SExtractor(1)) merge the two processes of detection and
measurement (catalog production) in one program.  However, in light of
Gnuastro’s modularized approach (modeled on the Unix system) detection
is separated from measurements and catalog production.  This modularity
is therefore new to many experienced astronomers and deserves a short
review here.  Further discussion on the benefits of this methodology can
be seen in Akhlaghi [2016] (https://arxiv.org/abs/1611.06387v1).

   As discussed in the introduction of *note MakeCatalog::, detection
(identifying which pixels to do measurements on) can be done with
different programs.  Their outputs (a labeled dataset) can be directly
fed into MakeCatalog to do the measurements and write the result as a
catalog/table.  Beyond that, Gnuastro’s modular approach has many
benefits that will become clear as you get more experienced in
astronomical data analysis and want to be more creative in using your
valuable data for the exciting scientific project you are working on.
In short the reasons for this modularity can be classified as below:

   • Simplicity/robustness of independent, modular tools: making a
     catalog is a logically separate process from labeling (detection,
     segmentation, or aperture production).  A user might want to do
     certain operations on the labeled regions before creating a catalog
     for them.  Another user might want the properties of the same
     pixels/objects in another image (another filter for example) to
     measure the colors or SED fittings.

     Here is an example of doing both: suppose you have images in
     various broad band filters at various resolutions and orientations.
     The image of one color will thus not lie exactly on another or even
     be in the same scale.  However, it is imperative that the same
     pixels be used in measuring the colors of galaxies.

     To solve the problem, NoiseChisel can be run on the reference image
     to generate the labeled detection image.  Afterwards, the labeled
     image can be warped into the grid of the other color (using *note
     Warp::).  MakeCatalog will then generate the same catalog for both
     colors (with the different labeled images).  It is currently
     customary to warp the images to the same pixel grid, however,
     modification of the scientific dataset is very harmful for the data
     and creates correlated noise.  It is much more accurate to do the
     transformations on the labeled image.

   • Complexity of a monolith: Adding in a catalog functionality to the
     detector program will add several more steps (and many more
     options) to its processing that can equally well be done outside of
     it.  This makes following what the program does harder for the
     users and developers, it can also potentially add many bugs.

     As an example, if the parameter you want to measure over one
     profile is not provided by the developers of MakeCatalog.  You can
     simply open this tiny little program and add your desired
     calculation easily.  This process is discussed in *note Adding new
     columns to MakeCatalog::.  However, if making a catalog was part of
     NoiseChisel for example, adding a new column/measurement would
     require a lot of energy to understand all the steps and internal
     structures of that huge program.  It might even be so intertwined
     with its processing, that adding new columns might cause
     problems/bugs in its primary job (detection).

   ---------- Footnotes ----------

   (1) <https://www.astromatic.net/software/sextractor>


File: gnuastro.info,  Node: Quantifying measurement limits,  Next: Measuring elliptical parameters,  Prev: Detection and catalog production,  Up: MakeCatalog

7.4.2 Quantifying measurement limits
------------------------------------

No measurement on a real dataset can be perfect: you can only reach a
certain level/limit of accuracy.  Therefore, a meaningful (scientific)
analysis requires an understanding of these limits for the dataset and
your analysis tools: different datasets have different noise properties
and different detection methods (one method/algorithm/software that is
run with a different set of parameters is considered as a different
detection method) will have different abilities to detect or measure
certain kinds of signal (astronomical objects) and their properties in
the dataset.  Hence, quantifying the detection and measurement
limitations with a particular dataset and analysis tool is the most
crucial/critical aspect of any high-level analysis.

   Here, we’ll review some of the most general limits that are important
in any astronomical data analysis and how MakeCatalog makes it easy to
find them.  Depending on the higher-level analysis, there are more tests
that must be done, but these are relatively low-level and usually
necessary in most cases.  In astronomy, it is common to use the
magnitude (a unit-less scale) and physical units, see *note Flux
Brightness and magnitude::.  Therefore the measurements discussed here
are commonly used in units of magnitudes.

Surface brightness limit (of whole dataset)
     As we make more observations on one region of the sky, and add the
     observations into one dataset, the signal and noise both increase.
     However, the signal increase much faster than the noise: assuming
     you add $N$ datasets with equal exposure times, the signal will
     increases as a multiple of $N$, while noise increases as
     $\sqrt{N}$.  Thus this increases the signal-to-noise ratio.
     Qualitatively, fainter (per pixel) parts of the objects/signal in
     the image will become more visible/detectable.  The noise-level is
     known as the dataset’s surface brightness limit.

     You can think of the noise as muddy water that is completely
     covering a flat ground(1).  The signal (or astronomical objects in
     this analogy) will be summits/hills that start from the flat sky
     level (under the muddy water) and can sometimes reach outside of
     the muddy water.  Let’s assume that in your first observation the
     muddy water has just been stirred and you can’t see anything
     through it.  As you wait and make more observations/exposures, the
     mud settles down and the _depth_ of the transparent water
     increases, making the summits visible.  As the depth of clear water
     increases, the parts of the hills with lower heights (parts with
     lower surface brightness) can be seen more clearly.  In this
     analogy, height (from the ground) is _surface brightness_(2) and
     the height of the muddy water is your surface brightness limit.

     The outputs of NoiseChisel include the Sky standard deviation
     ($\sigma$) on every group of pixels (a mesh) that were calculated
     from the undetected pixels in each tile, see *note Tessellation::
     and *note NoiseChisel output::.  Let’s take $\sigma_m$ as the
     median $\sigma$ over the successful meshes in the image (prior to
     interpolation or smoothing).

     On different instruments, pixels have different physical sizes (for
     example in micro-meters, or spatial angle over the sky).
     Nevertheless, a pixel is our unit of data collection.  In other
     words, while quantifying the noise, the physical or projected size
     of the pixels is irrelevant.  We thus define the Surface brightness
     limit or _depth_, in units of magnitude/pixel, of a data-set, with
     zeropoint magnitude $z$, with the $n$th multiple of $\sigma_m$ as
     (see *note Flux Brightness and magnitude::):

         $$SB_{\rm Pixel}=-2.5\times\log_{10}{(n\sigma_m)}+z$$

     As an example, the XDF survey covers part of the sky that the
     Hubble space telescope has observed the most (for 85 orbits) and is
     consequently very small ($\sim4$ arcmin$^2$).  On the other hand,
     the CANDELS survey, is one of the widest multi-color surveys
     covering several fields (about 720 arcmin$^2$) but its deepest
     fields have only 9 orbits observation.  The depth of the XDF and
     CANDELS-deep surveys in the near infrared WFC3/F160W filter are
     respectively 34.40 and 32.45 magnitudes/pixel.  In a single orbit
     image, this same field has a depth of 31.32.  Recall that a larger
     magnitude corresponds to less brightness.

     The low-level magnitude/pixel measurement above is only useful when
     all the datasets you want to use belong to one instrument
     (telescope and camera).  However, you will often find yourself
     using datasets from various instruments with different pixel scales
     (projected pixel sizes).  If we know the pixel scale, we can obtain
     a more easily comparable surface brightness limit in units of:
     magnitude/arcsec$^2$.  Let’s assume that the dataset has a
     zeropoint value of $z$, and every pixel is $p$ arcsec$^2$ (so $A/p$
     is the number of pixels that cover an area of $A$ arcsec$^2$).  If
     the surface brightness is desired at the $n$th multiple of
     $\sigma_m$, the following equation (in units of magnitudes per $A$
     arcsec$^2$) can be used:

$$SB_{\rm Projected}=-2.5\times\log_{10}{\left(n\sigma_m\sqrt{A\over p}\right)+z}$$

     The $\sqrt{A/p}$ term comes from the fact that noise is added in
     RMS: if you add three datasets with noise $\sigma_1$, $\sigma_2$
     and $\sigma_3$, the resulting noise level is
     $\sigma_t=\sqrt{\sigma_1^2+\sigma_2^2+\sigma_3^2}$, so when
     $\sigma_1=\sigma_2=\sigma_3=\sigma$, then
     $\sigma_t=\sqrt{3}\sigma$.  As mentioned above, there are $A/p$
     pixels in the area $A$.  Therefore, as $A/p$ increases, the surface
     brightness limiting magnitude will become brighter.

     It is just important to understand that the surface brightness
     limit is the raw noise level, _not_ the signal-to-noise.  To get a
     feeling for it you can try these commands on any FITS image (let’s
     assume its called ‘image.fits’), the output of the first command
     (‘zero.fits’) will be the same size as the input, but all pixels
     will have a value of zero.  We then add an ideal noise to this
     image and warp it to a new pixel size (such that the area of the
     new pixels is ‘area_per_pixel’ times the input’s), then we print
     the standard deviation of the raw noise and warped noise.  Please
     open the output images an compare them (their sizes, or their pixel
     values) to get a good feeling of what is going on.  Just note that
     this demo only works when ‘area_per_pixel’ is larger than one.

          area_per_pixel=25
          scale=$(echo $area_per_pixel | awk '{print sqrt($1)}')
          astarithmetic image.fits -h0 nan + isblank not -ozero.fits
          astmknoise zero.fits -onoise.fits
          astwarp --scale=1/$scale,1/$scale noise.fits -onoise-w.fits
          std_raw=$(aststatistics noise.fits --std)
          std_warped=$(aststatistics noise-w.fits --std)
          echo;
          echo "(warped pixel area) = $area_per_pixel x (pixel area)"
          echo "Raw STD:    $std_raw"
          echo "Warped STD: $std_warped"

     As you see in this example, this is thus just an extrapolation of
     the per-pixel measurement $\sigma_m$.  So it should be used with
     extreme care: for example the dataset must have an approximately
     flat depth or noise properties overall.  A more accurate measure
     for each detection is known as the _upper-limit magnitude_ which
     actually uses random positioning of each detection’s
     area/footprint, see the respective item below.  The upper-limit
     magnitude doesn’t extrapolate and even accounts for correlated
     noise patterns in relation to that detection.  Therefore, the
     upper-limit magnitude is a much better measure of your dataset’s
     surface brightness limit for each particular object.

     MakeCatalog will calculate the input dataset’s $SB_{\rm Pixel}$ and
     $SB_{\rm Projected}$ and write them as comments/meta-data in the
     output catalog(s).  Just note that $SB_{\rm Projected}$ is only
     calculated if the input has World Coordinate System (WCS).

Completeness limit (of each detection)
     As the surface brightness of the objects decreases, the ability to
     detect them will also decrease.  An important statistic is thus the
     fraction of objects of similar morphology and brightness that will
     be identified with our detection algorithm/parameters in the given
     image.  This fraction is known as completeness.  For brighter
     objects, completeness is 1: all bright objects that might exist
     over the image will be detected.  However, as we go to objects of
     lower overall surface brightness, we will fail to detect some, and
     gradually we are not able to detect anything any more.  For a given
     profile, the magnitude where the completeness drops below a certain
     level (usually above $90\%$) is known as the completeness limit.

     Another important parameter in measuring completeness is purity:
     the fraction of true detections to all true detections.  In effect
     purity is the measure of contamination by false detections: the
     higher the purity, the lower the contamination.  Completeness and
     purity are anti-correlated: if we can allow a large number of false
     detections (that we might be able to remove by other means), we can
     significantly increase the completeness limit.

     One traditional way to measure the completeness and purity of a
     given sample is by embedding mock profiles in regions of the image
     with no detection.  However in such a study we must be really
     careful to choose model profiles as similar to the target of
     interest as possible.

Magnitude measurement error (of each detection)
     Any measurement has an error and this includes the derived
     magnitude for an object.  Note that this value is only meaningful
     when the object’s magnitude is brighter than the upper-limit
     magnitude (see the next items in this list).  As discussed in *note
     Flux Brightness and magnitude::, the magnitude ($M$) of an object
     with brightness $B$ and Zeropoint magnitude $z$ can be written as:

                       $$M=-2.5\log_{10}(B)+z$$

     Calculating the derivative with respect to $B$, we get:

            $${dM\over dB} = {-2.5\over {B\times ln(10)}}$$

     From the Tailor series ($\Delta{M}=dM/dB\times\Delta{B}$), we can
     write:

$$\Delta{M} = \left|{-2.5\over ln(10)}\right|\times{\Delta{B}\over{B}}$$

     But, $\Delta{B}/B$ is just the inverse of the Signal-to-noise ratio
     ($S/N$), so we can write the error in magnitude in terms of the
     signal-to-noise ratio:

             $$\Delta{M} = {2.5\over{S/N\times ln(10)}} $$

     MakeCatalog uses this relation to estimate the magnitude errors.
     The signal-to-noise ratio is calculated in different ways for
     clumps and objects (see Akhlaghi and Ichikawa [2015]
     (https://arxiv.org/abs/1505.01664)), but this single equation can
     be used to estimate the measured magnitude error afterwards for any
     type of target.

Upper limit magnitude (of each detection)
     Due to the noisy nature of data, it is possible to get arbitrarily
     low values for a faint object’s brightness (or arbitrarily high
     _magnitudes_).  Given the scatter caused by the dataset’s noise,
     values fainter than a certain level are meaningless: another
     similar depth observation will give a radically different value.

     For example, while the depth of the image is 32 magnitudes/pixel, a
     measurement that gives a magnitude of 36 for a $\sim100$ pixel
     object is clearly unreliable.  In another similar depth image, we
     might measure a magnitude of 30 for it, and yet another might give
     33.  Furthermore, due to the noise scatter so close to the depth of
     the data-set, the total brightness might actually get measured as a
     negative value, so no magnitude can be defined (recall that a
     magnitude is a base-10 logarithm).  This problem usually becomes
     relevant when the detection labels were not derived from the values
     being measured (for example when you are estimating colors, see
     *note MakeCatalog::).

     Using such unreliable measurements will directly affect our
     analysis, so we must not use the raw measurements.  But how can we
     know how reliable a measurement on a given dataset is?

     When we confront such unreasonably faint magnitudes, there is one
     thing we can deduce: that if something actually exists here
     (possibly buried deep under the noise), it’s inherent magnitude is
     fainter than an _upper limit magnitude_.  To find this upper limit
     magnitude, we place the object’s footprint (segmentation map) over
     random parts of the image where there are no detections, so we only
     have pure (possibly correlated) noise, along with undetected
     objects.  Doing this a large number of times will give us a
     distribution of brightness values.  The standard deviation
     ($\sigma$) of that distribution can be used to quantify the upper
     limit magnitude.

     Traditionally, faint/small object photometry was done using fixed
     circular apertures (for example with a diameter of $N$
     arc-seconds).  Hence, the upper limit was like the depth discussed
     above: one value for the whole image.  The problem with this
     simplified approach is that the number of pixels in the aperture
     directly affects the final distribution and thus magnitude.  Also
     the image correlated noise might actually create certain patters,
     so the shape of the object can also affect the final result result.
     Fortunately, with the much more advanced hardware and software of
     today, we can make customized segmentation maps for each object.

     When requested, MakeCatalog will randomly place each target’s
     footprint over the dataset as described above and estimate the
     resulting distribution’s properties (like the upper limit
     magnitude).  The procedure is fully configurable with the options
     in *note Upper-limit settings::.  If one value for the whole image
     is required, you can either use the surface brightness limit above
     or make a circular aperture and feed it into MakeCatalog to request
     an upper-limit magnitude for it(3).

   ---------- Footnotes ----------

   (1) The ground is the sky value in this analogy, see *note Sky
value::.  Note that this analogy only holds for a flat sky value across
the surface of the image or ground.

   (2) Note that this muddy water analogy is not perfect, because while
the water-level remains the same all over a peak, in data analysis, the
Poisson noise increases with the level of data.

   (3) If you intend to make apertures manually and not use a detection
map (for example from *note Segment::), don’t forget to use the
‘--upmaskfile’ to give NoiseChisel’s output (or any a binary map,
marking detected pixels, see *note NoiseChisel output::) as a mask.
Otherwise, the footprints may randomly fall over detections, giving
highly skewed distributions, with wrong upper-limit distributions.  See
The description of ‘--upmaskfile’ in *note Upper-limit settings:: for
more.


File: gnuastro.info,  Node: Measuring elliptical parameters,  Next: Adding new columns to MakeCatalog,  Prev: Quantifying measurement limits,  Up: MakeCatalog

7.4.3 Measuring elliptical parameters
-------------------------------------

The shape or morphology of a target is one of the most commonly desired
parameters of a target.  Here, we will review the derivation of the most
basic/simple morphological parameters: the elliptical parameters for a
set of labeled pixels.  The elliptical parameters are: the (semi-)major
axis, the (semi-)minor axis and the position angle along with the
central position of the profile.  The derivations below follow the
SExtractor manual derivations with some added explanations for easier
reading.

   Let’s begin with one dimension for simplicity: Assume we have a set
of $N$ values $B_i$ (for example showing the spatial distribution of a
target’s brightness), each at position $x_i$.  The simplest parameter we
can define is the geometric center of the object ($x_g$) (ignoring the
brightness values): $x_g=(\sum_ix_i)/N$.  _Moments_ are defined to
incorporate both the value (brightness) and position of the data.  The
first moment can be written as:

            $$\overline{x}={\sum_iB_ix_i \over \sum_iB_i}$$

This is essentially the weighted (by $B_i$) mean position.  The
geometric center ($x_g$, defined above) is a special case of this with
all $B_i=1$.  The second moment is essentially the variance of the
distribution:

      $$\overline{x^2}\equiv{\sum_iB_i(x_i-\overline{x})^2 \over
            \sum_iB_i} = {\sum_iB_ix_i^2 \over \sum_iB_i} -
      2\overline{x}{\sum_iB_ix_i\over\sum_iB_i} + \overline{x}^2
         ={\sum_iB_ix_i^2 \over \sum_iB_i} - \overline{x}^2$$

The last step was done from the definition of $\overline{x}$.  Hence,
the square root of $\overline{x^2}$ is the spatial standard deviation
(along the one-dimension) of this particular brightness distribution
($B_i$).  Crudely (or qualitatively), you can think of its square root
as the distance (from $\overline{x}$) which contains a specific amount
of the flux (depending on the $B_i$ distribution).  Similar to the first
moment, the geometric second moment can be found by setting all $B_i=1$.
So while the first moment quantified the position of the brightness
distribution, the second moment quantifies how that brightness is
dispersed about the first moment.  In other words, it quantifies how
“sharp” the object’s image is.

   Before continuing to two dimensions and the derivation of the
elliptical parameters, let’s pause for an important implementation
technicality.  You can ignore this paragraph and the next two if you
don’t want to implement these concepts.  The basic definition (first
definition of $\overline{x^2}$ above) can be used without any major
problem.  However, using this fraction requires two runs over the data:
one run to find $\overline{x}$ and another run to find $\overline{x^2}$
from $\overline{x}$, this can be slow.  The advantage of the last
fraction above, is that we can estimate both the first and second
moments in one run (since the $-\overline{x}^2$ term can easily be added
later).

   The logarithmic nature of floating point number digitization creates
a complication however: suppose the object is located between pixels
10000 and 10020.  Hence the target’s pixels are only distributed over 20
pixels (with a standard deviation $<20$), while the mean has a value of
$\sim10000$.  The $\sum_iB_i^2x_i^2$ will go to very very large values
while the individual pixel differences will be orders of magnitude
smaller.  This will lower the accuracy of our calculation due to the
limited accuracy of floating point operations.  The variance only
depends on the distance of each point from the mean, so we can shift all
position by a constant/arbitrary $K$ which is much closer to the mean:
$\overline{x-K}=\overline{x}-K$.  Hence we can calculate the second
order moment using:

        $$\overline{x^2}={\sum_iB_i(x_i-K)^2 \over \sum_iB_i} -
                         (\overline{x}-K)^2 $$

The closer $K$ is to $\overline{x}$, the better (the sums of squares
will involve smaller numbers), as long as $K$ is within the object
limits (in the example above: $10000\leq{K}\leq10020$), the floating
point error induced in our calculation will be negligible.  For the most
simplest implementation, MakeCatalog takes $K$ to be the smallest
position of the object in each dimension.  Since $K$ is arbitrary and an
implementation/technical detail, we will ignore it for the remainder of
this discussion.

   In two dimensions, the mean and variances can be written as:

             $$\overline{x}={\sum_iB_ix_i\over B_i}, \quad
           \overline{x^2}={\sum_iB_ix_i^2 \over \sum_iB_i} -
                           \overline{x}^2$$
             $$\overline{y}={\sum_iB_iy_i\over B_i}, \quad
           \overline{y^2}={\sum_iB_iy_i^2 \over \sum_iB_i} -
                           \overline{y}^2$$
            $$\quad\quad\quad\quad\quad\quad\quad\quad\quad
           \overline{xy}={\sum_iB_ix_iy_i \over \sum_iB_i} -
                   \overline{x}\times\overline{y}$$

   If an elliptical profile’s major axis exactly lies along the $x$
axis, then $\overline{x^2}$ will be directly proportional with the
profile’s major axis, $\overline{y^2}$ with its minor axis and
$\overline{xy}=0$.  However, in reality we are not that lucky and
(assuming galaxies can be parameterized as an ellipse) the major axis of
galaxies can be in any direction on the image (in fact this is one of
the core principles behind weak-lensing by shear estimation).  So the
purpose of the remainder of this section is to define a strategy to
measure the position angle and axis ratio of some randomly positioned
ellipses in an image, using the raw second moments that we have
calculated above in our image coordinates.

   Let’s assume we have rotated the galaxy by $\theta$, the new second
order moments are:

        $$\overline{x_\theta^2} = \overline{x^2}\cos^2\theta +
                     \overline{y^2}\sin^2\theta -
                 2\overline{xy}\cos\theta\sin\theta $$
        $$\overline{y_\theta^2} = \overline{x^2}\sin^2\theta +
                     \overline{y^2}\cos^2\theta +
                 2\overline{xy}\cos\theta\sin\theta$$
     $$\overline{xy_\theta} = \overline{x^2}\cos\theta\sin\theta -
                 \overline{y^2}\cos\theta\sin\theta +
              \overline{xy}(\cos^2\theta-\sin^2\theta)$$

The best $\theta$ ($\theta_0$, where major axis lies along the
$x_\theta$ axis) can be found by:

$$\left.{\partial \overline{x_\theta^2} \over \partial \theta}\right|_{\theta_0}=0$$
   Taking the derivative, we get:
     $$2\cos\theta_0\sin\theta_0(\overline{y^2}-\overline{x^2}) +
        2(\cos^2\theta_0-\sin^2\theta_0)\overline{xy}=0$$ When
   $\overline{x^2}\neq\overline{y^2}$, we can write:
                           $$\tan2\theta_0 =
        2{\overline{xy} \over \overline{x^2}-\overline{y^2}}.$$

MakeCatalog uses the standard C math library’s ‘atan2’ function to
estimate $\theta_0$, which we define as the position angle of the
ellipse.  To recall, this is the angle of the major axis of the ellipse
with the $x$ axis.  By definition, when the elliptical profile is
rotated by $\theta_0$, then $\overline{xy_{\theta_0}}=0$,
$\overline{x_{\theta_0}^2}$ will be the extent of the maximum variance
and $\overline{y_{\theta_0}^2}$ the extent of the minimum variance
(which are perpendicular for an ellipse).  Replacing $\theta_0$ in the
equations above for $\overline{x_\theta}$ and $\overline{y_\theta}$, we
can get the semi-major ($A$) and semi-minor ($B$) lengths:

        $$A^2\equiv\overline{x_{\theta_0}^2}= {\overline{x^2} +
\overline{y^2} \over 2} + \sqrt{\left({\overline{x^2}-\overline{y^2} \over 2}\right)^2 + \overline{xy}^2}$$

        $$B^2\equiv\overline{y_{\theta_0}^2}= {\overline{x^2} +
\overline{y^2} \over 2} - \sqrt{\left({\overline{x^2}-\overline{y^2} \over 2}\right)^2 + \overline{xy}^2}$$

   As a summary, it is important to remember that the units of $A$ and
$B$ are in pixels (the standard deviation of a positional distribution)
and that they represent the spatial light distribution of the object in
both image dimensions (rotated by $\theta_0$).  When the object cannot
be represented as an ellipse, this interpretation breaks down:
$\overline{xy_{\theta_0}}\neq0$ and $\overline{y_{\theta_0}^2}$ will not
be the direction of minimum variance.


File: gnuastro.info,  Node: Adding new columns to MakeCatalog,  Next: Invoking astmkcatalog,  Prev: Measuring elliptical parameters,  Up: MakeCatalog

7.4.4 Adding new columns to MakeCatalog
---------------------------------------

MakeCatalog is designed to allow easy addition of different measurements
over a labeled image (see Akhlaghi [2016]
(https://arxiv.org/abs/1611.06387v1)).  A check-list style description
of necessary steps to do that is described in this section.  The common
development characteristics of MakeCatalog and other Gnuastro programs
is explained in *note Developing::.  We strongly encourage you to have a
look at that chapter to greatly simplify your navigation in the code.
After adding and testing your column, you are most welcome (and
encouraged) to share it with us so we can add to the next release of
Gnuastro for everyone else to also benefit from your efforts.

   MakeCatalog will first pass over each label’s pixels two times and do
necessary raw/internal calculations.  Once the passes are done, it will
use the raw information for filling the final catalog’s columns.  In the
first pass it will gather mainly object information and in the second
run, it will mainly focus on the clumps, or any other measurement that
needs an output from the first pass.  These two passes are designed to
be raw summations: no extra processing.  This will allow parallel
processing and simplicity/clarity.  So if your new calculation, needs
new raw information from the pixels, then you will need to also modify
the respective ‘mkcatalog_first_pass’ and ‘mkcatalog_second_pass’
functions (both in ‘bin/mkcatalog/mkcatalog.c’) and define new raw table
columns in ‘main.h’ (hopefully the comments in the code are clear
enough).

   In all these different places, the final columns are sorted in the
same order (same order as *note Invoking astmkcatalog::).  This allows a
particular column/option to be easily found in all steps.  Therefore in
adding your new option, be sure to keep it in the same relative place in
the list in all the separate places (it doesn’t necessarily have to be
in the end), and near conceptually similar options.

‘main.h’
     The ‘objectcols’ and ‘clumpcols’ enumerated variables (‘enum’)
     define the raw/internal calculation columns.  If your new column
     requires new raw calculations, add a row to the respective list.
     If your calculation requires any other settings parameters, you
     should add a variable to the ‘mkcatalogparams’ structure.

‘ui.c’
     If the new column needs raw calculations (an entry was added in
     ‘objectcols’ and ‘clumpcols’), specify which inputs it needs in
     ‘ui_necessary_inputs’, similar to the other options.  Afterwards,
     if your column includes any particular settings (you needed to add
     a variable to the ‘mkcatalogparams’ structure in ‘main.h’), you
     should do the sanity checks and preparations for it here.

‘ui.h’
     The ‘option_keys_enum’ associates a unique value for each option to
     MakeCatalog.  The options that have a short option version, the
     single character short comment is used for the value.  Those that
     don’t have a short option version, get a large integer
     automatically.  You should add a variable here to identify your
     desired column.

‘args.h’
     This file specifies all the parameters for the GNU C library, Argp
     structure that is in charge of reading the user’s options.  To
     define your new column, just copy an existing set of parameters and
     change the first, second and 5th values (the only ones that differ
     between all the columns), you should use the macro you defined in
     ‘ui.h’ here.

‘columns.c’
     This file contains the main definition and high-level calculation
     of your new column through the ‘columns_define_alloc’ and
     ‘columns_fill’ functions.  In the first, you specify the basic
     information about the column: its name, units, comments, type (see
     *note Numeric data types::) and how it should be printed if the
     output is a text file.  You should also specify the raw/internal
     columns that are necessary for this column here as the many
     existing examples show.  Through the types for objects and rows,
     you can specify if this column is only for clumps, objects or both.

     The second main function (‘columns_fill’) writes the final value
     into the appropriate column for each object and clump.  As you can
     see in the many existing examples, you can define your processing
     on the raw/internal calculations here and save them in the output.

‘mkcatalog.c’
     As described before, this file contains the two main MakeCatalog
     work-horses: ‘mkcatalog_first_pass’ and ‘mkcatalog_second_pass’,
     their names are descriptive enough and their internals are also
     clear and heavily commented.

‘doc/gnuastro.texi’
     Update this manual and add a description for the new column.


File: gnuastro.info,  Node: Invoking astmkcatalog,  Prev: Adding new columns to MakeCatalog,  Up: MakeCatalog

7.4.5 Invoking MakeCatalog
--------------------------

MakeCatalog will do measurements and produce a catalog from a labeled
dataset and optional values dataset(s).  The executable name is
‘astmkcatalog’ with the following general template

     $ astmkcatalog [OPTION ...] InputImage.fits

One line examples:

     ## Create catalog with RA, Dec, Magnitude and Magnitude error,
     ## from Segment's output:
     $ astmkcatalog --ra --dec --magnitude --magnitudeerr seg-out.fits

     ## Same catalog as above (using short options):
     $ asmkcatalog -rdmG seg-out.fits

     ## Write the catalog to a text table:
     $ astmkcatalog -mpQ seg-out.fits --output=cat.txt

     ## Output columns specified in `columns.conf':
     $ astmkcatalog --config=columns.conf seg-out.fits

     ## Use object and clump labels from a K-band image, but pixel values
     ## from an i-band image.
     $ astmkcatalog K_segmented.fits --hdu=DETECTIONS --clumpscat     \
                    --clumpsfile=K_segmented.fits --clumpshdu=CLUMPS  \
                    --valuesfile=i_band.fits

If MakeCatalog is to do processing (not printing help or option values),
an input labeled image should be provided.  The options described in
this section are those that are particular to MakeProfiles.  For
operations that MakeProfiles shares with other programs (mainly
involving input/output or general processing steps), see *note Common
options::.  Also see *note Common program behavior:: for some general
characteristics of all Gnuastro programs including MakeCatalog.

   The various measurements/columns of MakeCatalog are requested as
options, either on the command-line or in configuration files, see *note
Configuration files::.  The full list of available columns is available
in *note MakeCatalog measurements::.  Depending on the requested
columns, MakeCatalog needs more than one input dataset, for more
details, please see *note MakeCatalog inputs and basic settings::.  The
upper-limit measurements in particular need several configuration
options which are thoroughly discussed in *note Upper-limit settings::.
Finally, in *note MakeCatalog output:: the output file(s) created by
MakeCatalog are discussed.

* Menu:

* MakeCatalog inputs and basic settings::  Input files and basic settings.
* Upper-limit settings::        Settings for upper-limit measurements.
* MakeCatalog measurements::    Available measurements in MakeCatalog.
* MakeCatalog output::          File names of MakeCatalog’s output table.


File: gnuastro.info,  Node: MakeCatalog inputs and basic settings,  Next: Upper-limit settings,  Prev: Invoking astmkcatalog,  Up: Invoking astmkcatalog

7.4.5.1 MakeCatalog inputs and basic settings
.............................................

MakeCatalog works by using a localized/labeled dataset (see *note
MakeCatalog::).  This dataset maps/labels pixels to a specific target
(row number in the final catalog) and is thus the only necessary input
dataset to produce a minimal catalog in any situation.  Because it only
has labels/counters, it must have an integer type (see *note Numeric
data types::), see below if your labels are in a floating point
container.  When the requested measurements only need this dataset (for
example ‘--geox’, ‘--geoy’, or ‘--geoarea’), MakeCatalog won’t read any
more datasets.

   Low-level measurements that only use the labeled image are rarely
sufficient for any high-level science case.  Therefore necessary input
datasets depend on the requested columns in each run.  For example,
let’s assume you want the brightness/magnitude and signal-to-noise ratio
of your labeled regions.  For these columns, you will also need to
provide an extra dataset containing values for every pixel of the
labeled input (to measure brightness) and another for the Sky standard
deviation (to measure error).  All such auxiliary input files have to
have the same size (number of pixels in each dimension) as the input
labeled image.  Their numeric data type is irrelevant (they will be
converted to 32-bit floating point internally).  For the full list of
available measurements, see *note MakeCatalog measurements::.

   The “values” dataset is used for measurements like
brightness/magnitude, or flux-weighted positions.  If it is a real
image, by default it is assumed to be already Sky-subtracted prior to
running MakeCatalog.  If it isn’t, you use the ‘--subtractsky’ option
to, so MakeCatalog reads and subtracts the Sky dataset before any
processing.  To obtain the Sky value, you can use the ‘--sky’ option of
*note Statistics::, but the best recommended method is *note
NoiseChisel::, see *note Sky value::.

   MakeCatalog can also do measurements on sub-structures of detections.
In other words, it can produce two catalogs.  Following the nomenclature
of Segment (see *note Segment::), the main labeled input dataset is
known as “object” labels and the (optional) sub-structure input dataset
is known as “clumps”.  If MakeCatalog is run with the ‘--clumpscat’
option, it will also need a labeled image containing clumps, similar to
what Segment produces (see *note Segment output::).  Since clumps are
defined within detected regions (they exist over signal, not noise),
MakeCatalog uses their boundaries to subtract the level of signal under
them.

   There are separate options to explicitly request a file name and
HDU/extension for each of the required input datasets as fully described
below (with the ‘--*file’ format).  When each dataset is in a separate
file, these options are necessary.  However, one great advantage of the
FITS file format (that is heavily used in astronomy) is that it allows
the storage of multiple datasets in one file.  So in most situations
(for example if you are using the outputs of *note NoiseChisel:: or
*note Segment::), all the necessary input datasets can be in one file.

   When none of the ‘--*file’ options are given, MakeCatalog will assume
the necessary input datasets are in the file given as its argument
(without any option).  When the Sky or Sky standard deviation datasets
are necessary and the only ‘--*file’ option called is ‘--valuesfile’,
MakeCatalog will search for these datasets (with the default/given HDUs)
in the file given to ‘--valuesfile’ (before looking into the main
argument file).

   When the clumps image (necessary with the ‘--clumpscat’ option) is
used, MakeCatalog looks into the (possibly existing) ‘NUMLABS’ keyword
for the total number of clumps in the image (irrespective of how many
objects there are).  If its not present, it will count them and possibly
re-label the clumps so the clump labels always start with 1 and finish
with the total number of clumps in each object.  The re-labeled clumps
image will be stored with the ‘-clumps-relab.fits’ suffix.  This can
slightly slow-down the run.

   Note that ‘NUMLABS’ is automatically written by Segment in its
outputs, so if you are feeding Segment’s clump labels, you can benefit
from the improved speed.  Otherwise, if you are creating the clumps
label dataset manually, it may be good to include the ‘NUMLABS’ keyword
in its header and also be sure that there is no gap in the clump labels.
For example if an object has three clumps, they are labeled as 1, 2, 3.
If they are labeled as 1, 3, 4, or any other combination of three
positive integers that aren’t an increment of the previous, you might
get unknown behavior.

   It may happen that your labeled objects image was created with a
program that only outputs floating point files.  However, you know it
only has integer valued pixels that are stored in a floating point
container.  In such cases, you can use Gnuastro’s Arithmetic program
(see *note Arithmetic::) to change the numerical data type of the image
(‘float.fits’) to an integer type image (‘int.fits’) with a command like
below:

     $ astarithmetic float.fits int32 --output=int.fits

   To summarize: if the input file to MakeCatalog is the default/full
output of Segment (see *note Segment output::) you don’t have to worry
about any of the ‘--*file’ options below.  You can just give Segment’s
output file to MakeCatalog as described in *note Invoking
astmkcatalog::.  To feed NoiseChisel’s output into MakeCatalog, just
change the labeled dataset’s header (with ‘--hdu=DETECTIONS’).  The full
list of input dataset options and general setting options are described
below.

‘-l STR’
‘--clumpsfile=STR’
     The file containing the labeled clumps dataset when ‘--clumpscat’
     is called (see *note MakeCatalog output::).  When ‘--clumpscat’ is
     called, but this option isn’t, MakeCatalog will look into the main
     input file (given as an argument) for the required extension/HDU
     (value to ‘--clumpshdu’).

‘--clumpshdu=STR’
     The HDU/extension of the clump labels dataset.  Only pixels with
     values above zero will be considered.  The clump labels dataset has
     to be an integer data type (see *note Numeric data types::) and
     only pixels with a value larger than zero will be used.  See *note
     Segment output:: for a description of the expected format.

‘-v STR’
‘--valuesfile=STR’
     The file name of the (sky-subtracted) values dataset.  When any of
     the columns need values to associate with the input labels (for
     example to measure the brightness/magnitude of a galaxy),
     MakeCatalog will look into a “values” for the respective pixel
     values.  In most common processing, this is the actual astronomical
     image that the labels were defined, or detected, over.  The
     HDU/extension of this dataset in the given file can be specified
     with ‘--valueshdu’.  If this option is not called, MakeCatalog will
     look for the given extension in the main input file.

‘--valueshdu=STR/INT’
     The name or number (counting from zero) of the extension containing
     the “values” dataset, see the descriptions above and those in
     ‘--valuesfile’ for more.

‘-s STR/FLT’
‘--insky=STR/FLT’
     Sky value as a single number, or the file name containing a dataset
     (different values per pixel or tile).  The Sky dataset is only
     necessary when ‘--subtractsky’ is called or when a column directly
     related to the Sky value is requested (currently ‘--sky’).  This
     dataset may be a tessellation, with one element per tile (see
     ‘--oneelempertile’ of NoiseChisel’s *note Processing options::).

     When the Sky dataset is necessary but this option is not called,
     MakeCatalog will assume it is an HDU/extension (specified by
     ‘--skyhdu’) in one of the already given files.  First it will look
     for it in the ‘--valuesfile’ (if it is given) and then the main
     input file (given as an argument).

     By default the values dataset is assumed to be already Sky
     subtracted, so this dataset is not necessary for many of the
     columns.

‘--skyhdu=STR’
     HDU/extension of the Sky dataset, see ‘--skyfile’.

‘--subtractsky’
     Subtract the sky value or dataset from the values file prior to any
     processing.

‘-t STR/FLT’
‘--instd=STR/FLT’
     Sky standard deviation value as a single number, or the file name
     containing a dataset (different values per pixel or tile).  With
     the ‘--variance’ option you can tell MakeCatalog to interpret this
     value/dataset as a variance image, not standard deviation.

     *Important note:* This must only be the SKY standard deviation or
     variance (not including the signal’s contribution to the error).
     In other words, the final standard deviation of a pixel depends on
     how much signal there is in it.  MakeCatalog will find the amount
     of signal within each pixel (while subtracting the Sky, if
     ‘--subtractsky’ is called) and account for the extra error due to
     it’s value (signal).  Therefore if the input standard deviation (or
     variance) image also contains the contribution of signal to the
     error, then the final error measurements will be over-estimated.

‘--stdhdu=STR’
     The HDU of the Sky value standard deviation image.

‘--variance’
     The dataset given to ‘--stdfile’ (and ‘--stdhdu’ has the Sky
     variance of every pixel, not the Sky standard deviation.

‘--forcereadstd’
     Read the input STD image even if it is not required by any of the
     requested columns.  This is because some of the output catalog’s
     metadata may need it, for example to calculate the dataset’s
     surface brightness limit (see *note Quantifying measurement
     limits::, configured with ‘--sfmagarea’ and ‘--sfmagnsigma’ in
     *note MakeCatalog output::).

‘-z FLT’
‘--zeropoint=FLT’
     The zero point magnitude for the input image, see *note Flux
     Brightness and magnitude::.

‘--sigmaclip FLT,FLT’
     The sigma-clipping parameters when any of the sigma-clipping
     related columns are requested (for example ‘--sigclip-median’ or
     ‘--sigclip-number’).

     This option takes two values: the first is the multiple of
     $\sigma$, and the second is the termination criteria.  If the
     latter is larger than 1, it is read as an integer number and will
     be the number of times to clip.  If it is smaller than 1, it is
     interpreted as the tolerance level to stop clipping.  See *note
     Sigma clipping:: for a complete explanation.


File: gnuastro.info,  Node: Upper-limit settings,  Next: MakeCatalog measurements,  Prev: MakeCatalog inputs and basic settings,  Up: Invoking astmkcatalog

7.4.5.2 Upper-limit settings
............................

The upper-limit magnitude was discussed in *note Quantifying measurement
limits::.  Unlike other measured values/columns in MakeCatalog, the
upper limit magnitude needs several extra parameters which are discussed
here.  All the options specific to the upper-limit measurements start
with ‘up’ for “upper-limit”.  The only exception is ‘--envseed’ that is
also present in other programs and is general for any job requiring
random number generation in Gnuastro (see *note Generating random
numbers::).

   One very important consideration in Gnuastro is reproducibility.
Therefore, the values to all of these parameters along with others (like
the random number generator type and seed) are also reported in the
comments of the final catalog when the upper limit magnitude column is
desired.  The random seed that is used to define the random positions
for each object or clump is unique and set based on the (optionally)
given seed, the total number of objects and clumps and also the labels
of the clumps and objects.  So with identical inputs, an identical
upper-limit magnitude will be found.  However, even if the seed is
identical, when the ordering of the object/clump labels differs between
different runs, the result of upper-limit measurements will not be
identical.

   MakeCatalog will randomly place the object/clump footprint over the
dataset.  When the randomly placed footprint doesn’t fall on any object
or masked region (see ‘--upmaskfile’) it will be used in the final
distribution.  Otherwise that particular random position will be ignored
and another random position will be generated.  Finally, when the
distribution has the desired number of successfully measured random
samples (‘--upnum’) the distribution’s properties will be measured and
placed in the catalog.

   When the profile is very large or the image is significantly covered
by detections, it might not be possible to find the desired number of
samplings in a reasonable time.  MakeProfiles will continue searching
until it is unable to find a successful position (since the last
successful measurement(1)), for a large multiple of ‘--upnum’
(currently(2) this is 10).  If ‘--upnum’ successful samples cannot be
found until this limit is reached, MakeCatalog will set the upper-limit
magnitude for that object to NaN (blank).

   MakeCatalog will also print a warning if the range of positions
available for the labeled region is smaller than double the size of the
region.  In such cases, the limited range of random positions can
artificially decrease the standard deviation of the final distribution.
If your dataset can allow it (it is large enough), it is recommended to
use a larger range if you see such warnings.

‘--upmaskfile=STR’
     File name of mask image to use for upper-limit calculation.  In
     some cases (especially when doing matched photometry), the object
     labels specified in the main input and mask image might not be
     adequate.  In other words they do not necessarily have to cover
     _all_ detected objects: the user might have selected only a few of
     the objects in their labeled image.  This option can be used to
     ignore regions in the image in these situations when estimating the
     upper-limit magnitude.  All the non-zero pixels of the image
     specified by this option (in the ‘--upmaskhdu’ extension) will be
     ignored in the upper-limit magnitude measurements.

     For example, when you are using labels from another image, you can
     give NoiseChisel’s objects image output for this image as the value
     to this option.  In this way, you can be sure that regions with
     data do not harm your distribution.  See *note Quantifying
     measurement limits:: for more on the upper limit magnitude.

‘--upmaskhdu=STR’
     The extension in the file specified by ‘--upmask’.

‘--upnum=INT’
     The number of random samples to take for all the objects.  A larger
     value to this option will give a more accurate result
     (asymptotically), but it will also slow down the process.  When a
     randomly positioned sample overlaps with a detected/masked pixel it
     is not counted and another random position is found until the
     object completely lies over an undetected region.  So you can be
     sure that for each object, this many samples over undetected
     objects are made.  See the upper limit magnitude discussion in
     *note Quantifying measurement limits:: for more.

‘--uprange=INT,INT’
     The range/width of the region (in pixels) to do random sampling
     along each dimension of the input image around each object’s
     position.  This is not a mandatory option and if not given (or
     given a value of zero in a dimension), the full possible range of
     the dataset along that dimension will be used.  This is useful when
     the noise properties of the dataset vary gradually.  In such cases,
     using the full range of the input dataset is going to bias the
     result.  However, note that decreasing the range of available
     positions too much will also artificially decrease the standard
     deviation of the final distribution (and thus bias the upper-limit
     measurement).

‘--envseed’
     Read the random number generator type and seed value from the
     environment (see *note Generating random numbers::).  Random
     numbers are used in calculating the random positions of different
     samples of each object.

‘--upsigmaclip=FLT,FLT’
     The raw distribution of random values will not be used to find the
     upper-limit magnitude, it will first be $\sigma$-clipped (see *note
     Sigma clipping::) to avoid outliers in the distribution (mainly the
     faint undetected wings of bright/large objects in the image).  This
     option takes two values: the first is the multiple of $\sigma$, and
     the second is the termination criteria.  If the latter is larger
     than 1, it is read as an integer number and will be the number of
     times to clip.  If it is smaller than 1, it is interpreted as the
     tolerance level to stop clipping.  See *note Sigma clipping:: for a
     complete explanation.

‘--upnsigma=FLT’
     The multiple of the final ($\sigma$-clipped) standard deviation (or
     $\sigma$) used to measure the upper-limit brightness or magnitude.

‘--checkuplim=INT[,INT]’
     Print a table of positions and measured values for all the full
     random distribution used for one particular object or clump.  If
     only one integer is given to this option, it is interpreted to be
     an object’s label.  If two values are given, the first is the
     object label and the second is the ID of requested clump within it.

     The output is a table with three columns (its type is determined
     with the ‘--tableformat’ option, see *note Input output options::).
     The first two columns are the position of the first pixel in each
     random sampling of this particular object/clump.  The the third
     column is the measured flux over that region.  If the region
     overlapped with a detection or masked pixel, then its measured
     value will be a NaN (not-a-number).  The total number of rows is
     thus unknown, but you can be sure that the number of rows with
     non-NaN measurements is the number given to the ‘--upnum’ option.

   ---------- Footnotes ----------

   (1) The counting of failed positions restarts on every successful
measurement.

   (2) In Gnuastro’s source, this constant number is defined as the
‘MKCATALOG_UPPERLIMIT_MAXFAILS_MULTIP’ macro in ‘bin/mkcatalog/main.h’,
see *note Downloading the source::.


File: gnuastro.info,  Node: MakeCatalog measurements,  Next: MakeCatalog output,  Prev: Upper-limit settings,  Up: Invoking astmkcatalog

7.4.5.3 MakeCatalog measurements
................................

The final group of options particular to MakeCatalog are those that
specify which measurements/columns should be written into the final
output table.  The current measurements in MakeCatalog are those which
only produce one final value for each label (for example its total
brightness: a single number).  All the different label’s measurements
can be written as one column in a final table/catalog that contains
other columns for other similar single-number measurements.

   In this case, all the different label’s measurements can be written
as one column in a final table/catalog that contains other columns for
other similar single-number measurements.  The majority of this section
is devoted to MakeCatalog’s single-valued measurements.  However,
MakeCatalog can also do measurements that produce more than one value
for each label.  Currently the only such measurement is generation of
spectra from 3D cubes with the ‘--spectrum’ option and it is discussed
in the end of this section.

   Command-line options are used to identify which measurements you want
in the final catalog(s) and in what order.  If any of the options below
is called on the command line or in any of the configuration files, it
will be included as a column in the output catalog.  The order of the
columns is in the same order as the options were seen by MakeCatalog
(see *note Configuration file precedence::).  Some of the columns apply
to both “objects” and “clumps” and some are particular to only one of
them (for the definition of “objects” and “clumps”, see *note
Segment::).  Columns/options that are unique to one catalog (only
objects, or only clumps), are explicitly marked with [Objects] or
[Clumps] to specify the catalog they will be placed in.

‘--i’
‘--ids’
     This is a unique option which can add multiple columns to the final
     catalog(s).  Calling this option will put the object IDs
     (‘--objid’) in the objects catalog and host-object-ID
     (‘--hostobjid’) and ID-in-host-object (‘--idinhostobj’) into the
     clumps catalog.  Hence if only object catalogs are required, it has
     the same effect as ‘--objid’.

‘--objid’
     [Objects] ID of this object.

‘-j’
‘--hostobjid’
     [Clumps] The ID of the object which hosts this clump.

‘--idinhostobj’
     [Clumps] The ID of this clump in its host object.

‘-x’
‘--x’
     The flux weighted center of all objects and clumps along the first
     FITS axis (horizontal when viewed in SAO ds9), see $\overline{x}$
     in *note Measuring elliptical parameters::.  The weight has to have
     a positive value (pixel value larger than the Sky value) to be
     meaningful!  Specially when doing matched photometry, this might
     not happen: no pixel value might be above the Sky value.  For such
     detections, the geometric center will be reported in this column
     (see ‘--geox’).  You can use ‘--weightarea’ to see which was used.

‘-y’
‘--y’
     The flux weighted center of all objects and clumps along the second
     FITS axis (vertical when viewed in SAO ds9).  See ‘--x’.

‘-z’
‘--z’
     The flux weighted center of all objects and clumps along the third
     FITS axis.  See ‘--x’.

‘--geox’
     The geometric center of all objects and clumps along the first FITS
     axis axis.  The geometric center is the average pixel positions
     irrespective of their pixel values.

‘--geoy’
     The geometric center of all objects and clumps along the second
     FITS axis axis, see ‘--geox’.

‘--geoz’
     The geometric center of all objects and clumps along the third FITS
     axis axis, see ‘--geox’.

‘--minx’
     The minimum position of all objects and clumps along the first FITS
     axis.

‘--maxx’
     The maximum position of all objects and clumps along the first FITS
     axis.

‘--miny’
     The minimum position of all objects and clumps along the second
     FITS axis.

‘--maxy’
     The maximum position of all objects and clumps along the second
     FITS axis.

‘--minz’
     The minimum position of all objects and clumps along the third FITS
     axis.

‘--maxz’
     The maximum position of all objects and clumps along the third FITS
     axis.

‘--clumpsx’
     [Objects] The flux weighted center of all the clumps in this object
     along the first FITS axis.  See ‘--x’.

‘--clumpsy’
     [Objects] The flux weighted center of all the clumps in this object
     along the second FITS axis.  See ‘--x’.

‘--clumpsz’
     [Objects] The flux weighted center of all the clumps in this object
     along the third FITS axis.  See ‘--x’.

‘--clumpsgeox’
     [Objects] The geometric center of all the clumps in this object
     along the first FITS axis.  See ‘--geox’.

‘--clumpsgeoy’
     [Objects] The geometric center of all the clumps in this object
     along the second FITS axis.  See ‘--geox’.

‘--clumpsgeoz’
     [Objects] The geometric center of all the clumps in this object
     along the third FITS axis.  See ‘--geoz’.

‘-r’
‘--ra’
     Flux weighted right ascension of all objects or clumps, see ‘--x’.
     This is just an alias for one of the lower-level ‘--w1’ or ‘--w2’
     options.  Using the FITS WCS keywords (‘CTYPE’), MakeCatalog will
     determine which axis corresponds to the right ascension.  If no
     ‘CTYPE’ keywords start with ‘RA’, an error will be printed when
     requesting this column and MakeCatalog will abort.

‘-d’
‘--dec’
     Flux weighted declination of all objects or clumps, see ‘--x’.
     This is just an alias for one of the lower-level ‘--w1’ or ‘--w2’
     options.  Using the FITS WCS keywords (‘CTYPE’), MakeCatalog will
     determine which axis corresponds to the declination.  If no ‘CTYPE’
     keywords start with ‘DEC’, an error will be printed when requesting
     this column and MakeCatalog will abort.

‘--w1’
     Flux weighted first WCS axis of all objects or clumps, see ‘--x’.
     The first WCS axis is commonly used as right ascension in images.

‘--w2’
     Flux weighted second WCS axis of all objects or clumps, see ‘--x’.
     The second WCS axis is commonly used as declination in images.

‘--w3’
     Flux weighted third WCS axis of all objects or clumps, see ‘--x’.
     The third WCS axis is commonly used as wavelength in integral field
     unit data cubes.

‘--geow1’
     Geometric center in first WCS axis of all objects or clumps, see
     ‘--geox’.  The first WCS axis is commonly used as right ascension
     in images.

‘--geow2’
     Geometric center in second WCS axis of all objects or clumps, see
     ‘--geox’.  The second WCS axis is commonly used as declination in
     images.

‘--geow3’
     Geometric center in third WCS axis of all objects or clumps, see
     ‘--geox’.  The third WCS axis is commonly used as wavelength in
     integral field unit data cubes.

‘--clumpsw1’
     [Objects] Flux weighted center in first WCS axis of all clumps in
     this object, see ‘--x’.  The first WCS axis is commonly used as
     right ascension in images.

‘--clumpsw2’
     [Objects] Flux weighted declination of all clumps in this object,
     see ‘--x’.  The second WCS axis is commonly used as declination in
     images.

‘--clumpsw3’
     [Objects] Flux weighted center in third WCS axis of all clumps in
     this object, see ‘--x’.  The third WCS axis is commonly used as
     wavelength in integral field unit data cubes.

‘--clumpsgeow1’
     [Objects] Geometric center right ascension of all clumps in this
     object, see ‘--geox’.  The first WCS axis is commonly used as right
     ascension in images.

‘--clumpsgeow2’
     [Objects] Geometric center declination of all clumps in this
     object, see ‘--geox’.  The second WCS axis is commonly used as
     declination in images.

‘--clumpsgeow3’
     [Objects] Geometric center in third WCS axis of all clumps in this
     object, see ‘--geox’.  The third WCS axis is commonly used as
     wavelength in integral field unit data cubes.

‘-b’
‘--brightness’
     The brightness (sum of all pixel values), see *note Flux Brightness
     and magnitude::.  For clumps, the ambient brightness (flux of river
     pixels around the clump multiplied by the area of the clump) is
     removed, see ‘--riverave’.  So the sum of all the clumps brightness
     in the clump catalog will be smaller than the total clump
     brightness in the ‘--clumpbrightness’ column of the objects
     catalog.

     If no usable pixels are present over the clump or object (for
     example they are all blank), the returned value will be NaN (note
     that zero is meaningful).

‘--brightnesserr’
     The ($1\sigma$) error in measuring the brightness of objects or
     clumps.

‘--clumpbrightness’
     [Objects] The total brightness of the clumps within an object.
     This is simply the sum of the pixels associated with clumps in the
     object.  If no usable pixels are present over the clump or object
     (for example they are all blank), the stored value will be NaN
     (note that zero is meaningful).

‘--brightnessnoriver’
     [Clumps] The Sky (not river) subtracted clump brightness.  By
     definition, for the clumps, the average brightness of the rivers
     surrounding it are subtracted from it for a first order accounting
     for contamination by neighbors.  In cases where you will be
     calculating the flux brightness difference later (one example
     below) the contamination will be (mostly) removed at that stage,
     which is why this column was added.

     If no usable pixels are present over the clump or object (for
     example they are all blank), the stored value will be NaN (note
     that zero is meaningful).

‘--mean’
     The mean sky subtracted value of pixels within the object or clump.
     For clumps, the average river flux is subtracted from the sky
     subtracted mean.

‘--median’
     The median sky subtracted value of pixels within the object or
     clump.  For clumps, the average river flux is subtracted from the
     sky subtracted median.

‘--sigclip-number’
     The number of elements/pixels in the dataset after sigma-clipping
     the object or clump.  The sigma-clipping parameters can be set with
     the ‘--sigmaclip’ option described in *note MakeCatalog inputs and
     basic settings::.  For more on Sigma-clipping, see *note Sigma
     clipping::.

‘--sigclip-median’
     The sigma-clipped median value of the object of clump’s pixel
     distribution.  For more on sigma-clipping and how to define it, see
     ‘--sigclip-number’.

‘--sigclip-mean’
     The sigma-clipped mean value of the object of clump’s pixel
     distribution.  For more on sigma-clipping and how to define it, see
     ‘--sigclip-number’.

‘--sigclip-std’
     The sigma-clipped standard deviation of the object of clump’s pixel
     distribution.  For more on sigma-clipping and how to define it, see
     ‘--sigclip-number’.

‘-m’
‘--magnitude’
     The magnitude of clumps or objects, see ‘--brightness’.

‘-e’
‘--magnitudeerr’
     The magnitude error of clumps or objects.  The magnitude error is
     calculated from the signal-to-noise ratio (see ‘--sn’ and *note
     Quantifying measurement limits::).  Note that until now this error
     assumes uncorrelated pixel values and also does not include the
     error in estimating the aperture (or error in generating the
     labeled image).

     For now these factors have to be found by other means.  Task 14124
     (https://savannah.gnu.org/task/index.php?14124) has been defined
     for work on adding these sources of error too.

‘--clumpsmagnitude’
     [Objects] The magnitude of all clumps in this object, see
     ‘--clumpbrightness’.

‘--upperlimit’
     The upper limit value (in units of the input image) for this object
     or clump.  See *note Quantifying measurement limits:: and *note
     Upper-limit settings:: for a complete explanation.  This is very
     important for the fainter and smaller objects in the image where
     the measured magnitudes are not reliable.

‘--upperlimitmag’
     The upper limit magnitude for this object or clump.  See *note
     Quantifying measurement limits:: and *note Upper-limit settings::
     for a complete explanation.  This is very important for the fainter
     and smaller objects in the image where the measured magnitudes are
     not reliable.

‘--upperlimitonesigma’
     The $1\sigma$ upper limit value (in units of the input image) for
     this object or clump.  See *note Quantifying measurement limits::
     and *note Upper-limit settings:: for a complete explanation.  When
     ‘--upnsigma=1’, this column’s values will be the same as
     ‘--upperlimit’.

‘--upperlimitsigma’
     The position of the total brightness measured within the
     distribution of randomly placed upperlimit measurements in units of
     the distribution’s $\sigma$ or standard deviation.  See *note
     Quantifying measurement limits:: and *note Upper-limit settings::
     for a complete explanation.

‘--upperlimitquantile’
     The position of the total brightness measured within the
     distribution of randomly placed upperlimit measurements as a
     quantile (value between 0 or 1).  See *note Quantifying measurement
     limits:: and *note Upper-limit settings:: for a complete
     explanation.  If the object is brighter than the brightest randomly
     placed profile, a value of ‘inf’ is returned.  If it is less than
     the minimum, a value of ‘-inf’ is reported.

‘--upperlimitskew’
     This column contains the non-parametric skew of the
     $\sigma$-clipped random distribution that was used to estimate the
     upper-limit magnitude.  Taking $\mu$ as the mean, $\nu$ as the
     median and $\sigma$ as the standard deviation, the traditional
     definition of skewness is defined as: $(\mu-\nu)/\sigma$.

     This can be a good measure to see how much you can trust the random
     measurements, or in other words, how accurately the regions with
     signal have been masked/detected.  If the skewness is strong (and
     to the positive), then you can tell that you have a lot of
     undetected signal in the dataset, and therefore that the
     upper-limit measurement (and other measurements) are not reliable.

‘--riverave’
     [Clumps] The average brightness of the river pixels around this
     clump.  River pixels were defined in Akhlaghi and Ichikawa 2015.
     In short they are the pixels immediately outside of the clumps.
     This value is used internally to find the brightness (or magnitude)
     and signal to noise ratio of the clumps.  It can generally also be
     used as a scale to gauge the base (ambient) flux surrounding the
     clump.  In case there was no river pixels, then this column will
     have the value of the Sky under the clump.  So note that this value
     is _not_ sky subtracted.

‘--rivernum’
     [Clumps] The number of river pixels around this clump, see
     ‘--riverave’.

‘-n’
‘--sn’
     The Signal to noise ratio (S/N) of all clumps or objects.  See
     Akhlaghi and Ichikawa (2015) for the exact equations used.

‘--sky’
     The sky flux (per pixel) value under this object or clump.  This is
     actually the mean value of all the pixels in the sky image that lie
     on the same position as the object or clump.

‘--std’
     The sky value standard deviation (per pixel) for this clump or
     object.  This is the square root of the mean variance under the
     object, or the root mean square.

‘-C’
‘--numclumps’
     [Objects] The number of clumps in this object.

‘-a’
‘--area’
     The raw area (number of pixels/voxels) in any clump or object
     independent of what pixel it lies over (if it is NaN/blank or
     unused for example).

‘--areaxy’
     Similar to ‘--area’, when the clump or object is projected onto the
     first two dimensions.  This is only available for 3-dimensional
     datasets.  When working with Integral Field Unit (IFU) datasets,
     this projection onto the first two dimensions would be a
     narrow-band image.

‘--clumpsarea’
     [Objects] The total area of all the clumps in this object.

‘--weightarea’
     The area (number of pixels) used in the flux weighted position
     calculations.

‘--geoarea’
     The area of all the pixels labeled with an object or clump.  Note
     that unlike ‘--area’, pixel values are completely ignored in this
     column.  For example, if a pixel value is blank, it won’t be
     counted in ‘--area’, but will be counted here.

‘--geoareaxy’
     Similar to ‘--geoarea’, when the clump or object is projected onto
     the first two dimensions.  This is only available for 3-dimensional
     datasets.  When working with Integral Field Unit (IFU) datasets,
     this projection onto the first two dimensions would be a
     narrow-band image.

‘-A’
‘--semimajor’
     The pixel-value weighted root mean square (RMS) along the
     semi-major axis of the profile (assuming it is an ellipse) in units
     of pixels.  See *note Measuring elliptical parameters::.

‘-B’
‘--semiminor’
     The pixel-value weighted root mean square (RMS) along the
     semi-minor axis of the profile (assuming it is an ellipse) in units
     of pixels.  See *note Measuring elliptical parameters::.

‘--axisratio’
     The pixel-value weighted axis ratio (semi-minor/semi-major) of the
     object or clump.

‘-p’
‘--positionangle’
     The pixel-value weighted angle of the semi-major axis with the
     first FITS axis in degrees.  See *note Measuring elliptical
     parameters::.

‘--geosemimajor’
     The geometric (ignoring pixel values) root mean square (RMS) along
     the semi-major axis of the profile, assuming it is an ellipse, in
     units of pixels.

‘--geosemiminor’
     The geometric (ignoring pixel values) root mean square (RMS) along
     the semi-minor axis of the profile, assuming it is an ellipse, in
     units of pixels.

‘--geoaxisratio’
     The geometric (ignoring pixel values) axis ratio of the profile,
     assuming it is an ellipse.

‘--geopositionangle’
     The geometric (ignoring pixel values) angle of the semi-major axis
     with the first FITS axis in degrees.

   Above, all of MakeCatalog’s single-valued measurements were listed.
As mentioned in the start of this section, MakeCatalog can also do
multi-valued measurements per label.  Currently the only such
measurement is the creation of spectra from 3D data cubes as discussed
below:

‘--spectrum’
     Generate a spectrum (measurement along the first two FITS
     dimensions) for each label when the input dataset is a 3D data
     cube.  With this option, a seprate table/spectrum will be generated
     for every label.  If the output is a FITS file, each label’s
     spectrum will be written into an extension of that file with a
     standard name of ‘SPECTRUM_NN’ (the label will be replaced with
     ‘NN’).  If the output is a plain text file, each label’s spectrum
     will be written into a separate file with the suffix ‘spec-NN.txt’.
     See *note MakeCatalog output:: for more on specifying MakeCatalog’s
     output file.

     The spectra will contain one row for every slice (third FITS
     dimension) of the cube.  Since the physical nature of the third
     dimension is different, two types of spectra (along with their
     errors) are measured: 1) Sum of values in each slice that only have
     the requested label.  2) Sum of values on the 2D projection of the
     whole label (the area of this projection can be requested with the
     ‘--areaxy’ column above).

     Labels can overlap when they are projected onto the first two FITS
     dimensions (the spatial domain).  To help separate them,
     MakeCatalog does a third measurement on each slice: the area, sum
     of values and error of all pixels that belong to other labels but
     overlap with the 2D projection.  This can be used to see how
     reliable the emission line measurement is (on the projected
     spectra) and also if multiple lines (labeled regions) belong to the
     same physical object.

‘--inbetweenints’
     Output will contain one row for all integers between 1 and the
     largest label in the input (irrespective of their existance in the
     input image).  By default, MakeCatalog’s output will only contain
     rows with integers that actually corresponded to atleast one pixel
     in the input dataset.

     For example if the input’s only labeled pixel values are 11 and 13,
     MakeCatalog’s default output will only have two rows.  If you use
     this option, it will have 13 rows and all the columns corresponding
     to integer identifiers that didn’t correspond to any pixel will be
     0 or NaN (depending on context).


File: gnuastro.info,  Node: MakeCatalog output,  Prev: MakeCatalog measurements,  Up: Invoking astmkcatalog

7.4.5.4 MakeCatalog output
..........................

After it has completed all the requested measurements (see *note
MakeCatalog measurements::), MakeCatalog will store its measurements in
table(s).  If an output filename is given (see ‘--output’ in *note Input
output options::), the format of the table will be deduced from the
name.  When it isn’t given, the input name will be appended with a
‘_cat’ suffix (see *note Automatic output::) and its format will be
determined from the ‘--tableformat’ option, which is also discussed in
*note Input output options::.  ‘--tableformat’ is also necessary when
the requested output name is a FITS table (recall that FITS can accept
ASCII and binary tables, see *note Table::).

   By default (when ‘--spectrum’ isn’t called) only a single
catalog/table will be created for “objects”, however, if ‘--clumpscat’
is called, a secondary catalog/table will also be created.  For more on
“objects” and “clumps”, see *note Segment::.  In short, if you only have
one set of labeled images, you don’t have to worry about clumps (they
are deactivated by default).

   When ‘--spectrum’ is called, it is not mandatory to specify any
single-valued measurement columns.  In this case, the output will only
be the spectra of each labeled region.  See the description of
‘--spectrum’ in *note MakeCatalog measurements::.

   The full list of MakeCatalog’s output options are elaborated below.

‘-C’
‘--clumpscat’
     Do measurements on clumps and produce a second catalog (only
     devoted to clumps).  When this option is given, MakeCatalog will
     also look for a secondary labeled dataset (identifying
     substructure) and produce a catalog from that.  For more on the
     definition on “clumps”, see *note Segment::.

     When the output is a FITS file, the objects and clumps
     catalogs/tables will be stored as multiple extensions of one FITS
     file.  You can use *note Table:: to inspect the column meta-data
     and contents in this case.  However, in plain text format (see
     *note Gnuastro text table format::), it is only possible to keep
     one table per file.  Therefore, if the output is a text file, two
     output files will be created, ending in ‘_o.txt’ (for objects) and
     ‘_c.txt’ (for clumps).

‘--noclumpsort’
     Don’t sort the clumps catalog based on object ID (only relevant
     with ‘--clumpscat’).  This option will benefit the performance(1)
     of MakeCatalog when it is run on multiple threads _and_ the
     position of the rows in the clumps catalog is irrelevant (for
     example you just want the number-counts).

     MakeCatalog does all its measurements on each _object_
     independently and in parallel.  As a result, while it is writing
     the measurements on each object’s clumps, it doesn’t know how many
     clumps there were in previous objects.  Each thread will just fetch
     the first available row and write the information of clumps (in
     order) starting from that row.  After all the measurements are
     done, by default (when this option isn’t called), MakeCatalog will
     reorder/permute the clumps catalog to have both the object and
     clump ID in an ascending order.

     If you would like to order the catalog later (when its a plain text
     file), you can run the following command to sort the rows by object
     ID (and clump ID within each object), assuming they are
     respectively the first and second columns:

          $ awk '!/^#/' out_c.txt | sort -g -k1,1 -k2,2

‘--sfmagnsigma=FLT’
     Value to multiply with the median standard deviation (from a
     ‘MEDSTD’ keyword in the Sky standard deviation image) for
     estimating the surface brightness limit.  Note that the surface
     brightness limit is only reported when a standard deviation image
     is read, in other words a column using it is requested (for example
     ‘--sn’) or ‘--forcereadstd’ is called.

     This value is a per-pixel value, not per object/clump and is not
     found over an area or aperture, like the common $5\sigma$ values
     that are commonly reported as a measure of depth or the upper-limit
     measurements (see *note Quantifying measurement limits::).

‘--sfmagarea=FLT’
     Area (in arcseconds squared) to convert the per-pixel estimation of
     ‘--sfmagnsigma’ in the comments section of the output tables.  Note
     that the surface brightness limit is only reported when a standard
     deviation image is read, in other words a column using it is
     requested (for example ‘--sn’) or ‘--forcereadstd’ is called.

     Note that this is just a unit conversion using the World Coordinate
     System (WCS) information in the input’s header.  It does not
     actually do any measurements on this area.  For random measurements
     on any area, please use the upper-limit columns of MakeCatalog (see
     the discussion on upper-limit measurements in *note Quantifying
     measurement limits::).

   ---------- Footnotes ----------

   (1) The performance boost due to ‘--noclumpsort’ can only be felt
when there are a huge number of objects.  Therefore, by default the
output is sorted to avoid miss-understandings or bugs in the user’s
scripts when the user forgets to sort the outputs.


File: gnuastro.info,  Node: Match,  Prev: MakeCatalog,  Up: Data analysis

7.5 Match
=========

Data can come come from different telescopes, filters, software and even
different configurations for a single software.  As a result, one of the
primary things to do after generating catalogs from each of these
sources (for example with *note MakeCatalog::), is to find which sources
in one catalog correspond to which in the other(s).  In other words, to
‘match’ the two catalogs with each other.

   Gnuastro’s Match program is in charge of such operations.  The
nearest objects in the two catalogs, within the given aperture, will be
found and given as output.  The aperture can be a circle or an ellipse
with any orientation.

* Menu:

* Invoking astmatch::           Inputs, outputs and options of Match


File: gnuastro.info,  Node: Invoking astmatch,  Prev: Match,  Up: Match

7.5.1 Invoking Match
--------------------

When given two catalogs, Match finds the rows that are nearest to each
other within an input aperture.  The executable name is ‘astmatch’ with
the following general template

     $ astmatch [OPTION ...] input-1 input-2

One line examples:

     ## 1D wavelength match (within 5 angstroms) of the two inputs.
     ## The wavelengths are in the 5th and 10th columns respectively.
     $ astmatch --aperture=5e-10 --ccol1=5 --ccol2=10 in1.fits in2.txt

     ## Match the two catalogs with a circular aperture of width 2.
     ## (Units same as given positional columns).
     ## (By default two columns are given for `--ccol1' and `--ccol2',
     ##  The number of values to these determines the dimensions).
     $ astmatch --aperture=2 input1.txt input2.fits

     ## Similar to before, but the output is created by merging various
     ## columns from the two inputs: columns 1, RA, DEC from the first
     ## input, followed by all columns starting with `MAG' and the `BRG'
     ## column from second input and finally the 10th from first input.
     $ astmatch --aperture=2 input1.txt input2.fits                   \
                --outcols=a1,aRA,aDEC,b/^MAG/,bBRG,a10

     ## Match the two catalogs within an elliptical aperture of 1 and 2
     ## arcseconds along RA and Dec respectively.
     $ astmatch --aperture=1/3600,2/3600 in1.fits in2.txt

     ## Match the RA and DEC columns of the first input with the RA_D
     ## and DEC_D columns of the second within a 0.5 arcseconds aperture.
     $ astmatch --ccol1=RA,DEC --ccol2=RA_D,DEC_D --aperture=0.5/3600  \
                in1.fits in2.fits

     ## Match in 3D (RA, Dec and Wavelength).
     $ astmatch --ccol1=2,3,4 --ccol2=2,3,4 -a0.5/3600,0.5/3600,5e-10 \
                in1.fits in2.txt

   Match will find the rows that are nearest to each other in two
catalogs (given some coordinate columns).  Therefore two catalogs are
necessary for input.  However, they don’t necessarily have to be files:
1) the first catalog can also come from the standard input (for example
a pipe, see *note Standard input::); 2) when only one point is needed,
you can use the ‘--coord’ option to avoid creating a file for the second
catalog.  When the inputs are files, they can be plain text tables or
FITS tables, for more see *note Tables::.

   Match follows the same basic behavior of all Gnuastro programs as
fully described in *note Common program behavior::.  If the first input
is a FITS file, the common ‘--hdu’ option (see *note Input output
options::) should be used to identify the extension.  When the second
input is FITS, the extension must be specified with ‘--hdu2’.

   When ‘--quiet’ is not called, Match will print the number of matches
found in standard output (on the command-line).  When matches are found,
by default, the output file(s) will be the re-arranged input tables such
that the rows match each other: both output tables will have the same
number of rows which are matched with each other.  If ‘--outcols’ is
called, the output is a single table with rows chosen from either of the
two inputs in any order.  If the ‘--logasoutput’ option is called, the
output will be a single table with the contents of the log file, see
below.  If no matches are found, the columns of the output table(s) will
have zero rows (with proper meta-data).

   If no output file name is given with the ‘--output’ option, then
automatic output *note Automatic output:: will be used to determine the
output name(s).  Depending on ‘--tableformat’ (see *note Input output
options::), the output will then be a (possibly multi-extension) FITS
file or (possibly two) plain text file(s).  When the output is a FITS
file, the default re-arranged inputs will be two extensions of the
output FITS file.  With ‘--outcols’ and ‘--logasoutput’, the FITS output
will be a single table (in one extension).

   When the ‘--log’ option is called (see *note Operating mode
options::), and there was a match, Match will also create a file named
‘astmatch.fits’ (or ‘astmatch.txt’, depending on ‘--tableformat’, see
*note Input output options::) in the directory it is run in.  This log
table will have three columns.  The first and second columns show the
matching row/record number (counting from 1) of the first and second
input catalogs respectively.  The third column is the distance between
the two matched positions.  The units of the distance are the same as
the given coordinates (given the possible ellipticity, see description
of ‘--aperture’ below).  When ‘--logasoutput’ is called, no log file
(with a fixed name) will be created.  In this case, the output file
(possibly given by the ‘--output’ option) will have the contents of this
log file.

*‘--log’ isn’t thread-safe*: As described above, when ‘--logasoutput’ is
not called, the Log file has a fixed name for all calls to Match.
Therefore if a separate log is requested in two simultaneous calls to
Match in the same directory, Match will try to write to the same file.
This will cause problems like unreasonable log file, undefined behavior,
or a crash.

‘-H STR’
‘--hdu2=STR’
     The extension/HDU of the second input if it is a FITS file.  When
     it isn’t a FITS file, this option’s value is ignored.  For the
     first input, the common option ‘--hdu’ must be used.

‘--outcols=STR’
     Columns (from both inputs) to write into a single matched table
     output.  The value to ‘--outcols’ must be a comma-separated list of
     strings.  The first character of each string specifies the input
     catalog: ‘a’ for the first and ‘b’ for the second.  The rest of the
     characters of the string will be directly used to identify the
     proper column(s) in the respective table.  See *note Selecting
     table columns:: for how columns can be specified in Gnuastro.

     For example the output of ‘--outcols=a1,bRA,bDEC’ will have three
     columns: the first column of the first input, along with the ‘RA’
     and ‘DEC’ columns of the second input.

     If the string after ‘a’ or ‘b’ is ‘_all’, then all the columns of
     the respective input file will be written in the output.  For
     example the command below will print all the input columns from the
     first catalog along with the 5th column from the second:

          $ astmatch a.fits b.fits --outcols=a_all,b5

     ‘_all’ can be used multiple times, possibly on both inputs.  Tip:
     if an input’s column is called ‘_all’ (an unlikely name!)  and you
     don’t want all the columns from that table the output, use its
     column number to avoid confusion.

     Another example is given in the one-line examples above.  Compared
     to the default case (where two tables with all their columns) are
     saved separately, using this option is much faster: it will only
     read and re-arrange the necessary columns and it will write a
     single output table.  Combined with regular expressions in large
     tables, this can be a very powerful and convenient way to merge
     various tables into one.

     When ‘--coord’ is given, no second catalog will be read.  The
     second catalog will be created internally based on the values given
     to ‘--coord’.  So column names aren’t defined and you can only
     request integer column numbers that are less than the number of
     coordinates given to ‘--coord’.  For example if you want to find
     the row matching RA of 1.2345 and Dec of 6.7890, then you should
     use ‘--coord=1.2345,6.7890’.  But when using ‘--outcols’, you can’t
     give ‘bRA’, or ‘b25’.

‘-l’
‘--logasoutput’
     The output file will have the contents of the log file: indexes in
     the two catalogs that match with each other along with their
     distance.  See description above.  When this option is called, a
     log file called ‘astmatch.txt’ will not be created.  With this
     option, the default output behavior (two tables containing the
     re-arranged inputs) will be

‘--notmatched’
     Write the non-matching rows into the outputs, not the matched ones.
     Note that with this option, the two output tables will not
     necessarily have the same number of rows.  Therefore, this option
     cannot be called with ‘--outcols’.  ‘--outcols’ prints mixed
     columns from both inputs, so they must all have the same number of
     elements and must correspond to each other.

‘-c INT/STR[,INT/STR]’
‘--ccol1=INT/STR[,INT/STR]’
     The coordinate columns of the first input.  The number of
     dimensions for the match is determined by the number of
     comma-separated values given to this option.  The values can be the
     column number (counting from 1), exact column name or a regular
     expression.  For more, see *note Selecting table columns::.  See
     the one-line examples above for some usages of this option.

‘-C INT/STR[,INT/STR]’
‘--ccol2=INT/STR[,INT/STR]’
     The coordinate columns of the second input.  See the example in
     ‘--ccol1’ for more.

‘-d FLT[,FLT]’
‘--coord=FLT[,FLT]’
     Manually specify the coordinates to match against the given
     catalog.  With this option, Match will not look for a second input
     file/table and will directly use the coordinates given to this
     option.

     When this option is called, the output changes in the following
     ways: 1) when ‘--outcols’ is specified, for the second input, it
     can only accept integer numbers that are less than the number of
     values given to this option, see description of that option for
     more.  2) By default (when ‘--outcols’ isn’t used), only the
     matching row of the first table will be output (a single file), not
     two separate files (one for each table).

     This option is good when you have a (large) catalog and only want
     to match a single coordinate to it (for example to find the nearest
     catalog entry to your desired point).  With this option, you can
     write the coordinates on the command-line and thus avoid the need
     to make a single-row file.

‘-a FLT[,FLT[,FLT]]’
‘--aperture=FLT[,FLT[,FLT]]’
     Parameters of the aperture for matching.  The values given to this
     option can be fractions, for example when the position columns are
     in units of degrees, ‘1/3600’ can be used to ask for one arcsecond.
     The interpretation of the values depends on the requested
     dimensions (determined from ‘--ccol1’ and ‘--ccol2’) and how many
     values are given to this option.

     When multiple objects are found within the aperture, the match is
     defined as the nearest one.  In a multi-dimensional dataset, when
     the aperture is a general ellipse or ellipsoid (and not a circle or
     sphere), the distance is calculated in the elliptical space along
     the major axis.  For the defintion of this distance, see $r_{el}$
     in *note Defining an ellipse and ellipsoid::.

     1D match
          The aperture/interval can only take one value: half of the
          interval around each point (maximum distance from each point).

     2D match
          In a 2D match, the aperture can be a circle, an ellipse
          aligned in the axes or an ellipse with a rotated major axis.
          To simply the usage, you can determine the shape based on the
          number of free parameters for each.

          1 number
               For example ‘--aperture=2’.  The aperture will be a
               circle of the given radius.  The value will be in the
               same units as the columns in ‘--ccol1’ and ‘--ccol2’).

          2 numbers
               For example ‘--aperture=3,4e-10’.  The aperture will be
               an ellipse (if the two numbers are different) with the
               respective value along each dimension.  The numbers are
               in units of the first and second axis.  In the example
               above, the semi-axis value along the first axis will be 3
               (in units of the first coordinate) and along the second
               axis will be $4\times10^{-10}$ (in units of the second
               coordinate).  Such values can happen if you are comparing
               catalogs of a spectra for example.  If more than one
               object exists in the aperture, the nearest will be found
               along the major axis as described in *note Defining an
               ellipse and ellipsoid::.

          3 numbers
               For example ‘--aperture=2,0.6,30’.  The aperture will be
               an ellipse (if the second value is not 1).  The first
               number is the semi-major axis, the second is the axis
               ratio and the third is the position angle (in degrees).
               If multiple matches are found within the ellipse, the
               distance (to find the nearest) is calculated along the
               major axis in the elliptical space, see *note Defining an
               ellipse and ellipsoid::.

     3D match
          The aperture (matching volume) can be a sphere, an ellipsoid
          aligned on the three axises or a genenral ellipsoid rotated in
          any direction.  To simplifythe usage, the shape can be
          determined based on the number of values given to this option.

          1 number
               For example ‘--aperture=3’.  The matching volume will be
               a sphere of the given radius.  The value is in the same
               units as the input coordinates.

          3 numbers
               For example ‘--aperture=4,5,6e-10’.  The aperture will be
               a general ellipsoid with the respective extent along each
               dimension.  The numbers must be in the same units as each
               axis.  This is very similar to the two number case of 2D
               inputs.  See there for more.

          6 numbers
               For example ‘--aperture=4,0.5,0.6,10,20,30’.  The numbers
               represent the full general ellipsoid definition (in any
               orientation).  For the definition of a general ellipsoid,
               see *note Defining an ellipse and ellipsoid::.  The first
               number is the semi-major axis.  The second and third are
               the two axis ratios.  The last three are the three Euler
               angles in units of degrees in the ZXZ order as fully
               described in *note Defining an ellipse and ellipsoid::.


File: gnuastro.info,  Node: Modeling and fittings,  Next: High-level calculations,  Prev: Data analysis,  Up: Top

8 Modeling and fitting
**********************

In order to fully understand observations after initial analysis on the
image, it is very important to compare them with the existing models to
be able to further understand both the models and the data.  The tools
in this chapter create model galaxies and will provide 2D fittings to be
able to understand the detections.

* Menu:

* MakeProfiles::                Making mock galaxies and stars.
* MakeNoise::                   Make (add) noise to an image.


File: gnuastro.info,  Node: MakeProfiles,  Next: MakeNoise,  Prev: Modeling and fittings,  Up: Modeling and fittings

8.1 MakeProfiles
================

MakeProfiles will create mock astronomical profiles from a catalog,
either individually or together in one output image.  In data analysis,
making a mock image can act like a calibration tool, through which you
can test how successfully your detection technique is able to detect a
known set of objects.  There are commonly two aspects to detecting: the
detection of the fainter parts of bright objects (which in the case of
galaxies fade into the noise very slowly) or the complete detection of
an over-all faint object.  Making mock galaxies is the most accurate
(and idealistic) way these two aspects of a detection algorithm can be
tested.  You also need mock profiles in fitting known functional
profiles with observations.

   MakeProfiles was initially built for extra galactic studies, so
currently the only astronomical objects it can produce are stars and
galaxies.  We welcome the simulation of any other astronomical object.
The general outline of the steps that MakeProfiles takes are the
following:

  1. Build the full profile out to its truncation radius in a possibly
     over-sampled array.

  2. Multiply all the elements by a fixed constant so its total
     magnitude equals the desired total magnitude.

  3. If ‘--individual’ is called, save the array for each profile to a
     FITS file.

  4. If ‘--nomerged’ is not called, add the overlapping pixels of all
     the created profiles to the output image and abort.

   Using input values, MakeProfiles adds the World Coordinate System
(WCS) headers of the FITS standard to all its outputs (except PSF
images!).  For a simple test on a set of mock galaxies in one image,
there is no need for the third step or the WCS information.

   However in complicated simulations like weak lensing simulations,
where each galaxy undergoes various types of individual transformations
based on their position, those transformations can be applied to the
different individual images with other programs.  After all the
transformations are applied, using the WCS information in each
individual profile image, they can be merged into one output image for
convolution and adding noise.

* Menu:

* Modeling basics::             Astronomical modeling basics.
* If convolving afterwards::    Considerations for convolving later.
* Flux Brightness and magnitude::  About these measures of energy.
* Profile magnitude::           Definition of total profile magnitude.
* Invoking astmkprof::          Inputs and Options for MakeProfiles.


File: gnuastro.info,  Node: Modeling basics,  Next: If convolving afterwards,  Prev: MakeProfiles,  Up: MakeProfiles

8.1.1 Modeling basics
---------------------

In the subsections below, first a review of some very basic information
and concepts behind modeling a real astronomical image is given.  You
can skip this subsection if you are already sufficiently familiar with
these concepts.

* Menu:

* Defining an ellipse and ellipsoid::  Definition of these important shapes.
* PSF::                         Radial profiles for the PSF.
* Stars::                       Making mock star profiles.
* Galaxies::                    Radial profiles for galaxies.
* Sampling from a function::    Sample a function on a pixelated canvas.
* Oversampling::                Oversampling the model.


File: gnuastro.info,  Node: Defining an ellipse and ellipsoid,  Next: PSF,  Prev: Modeling basics,  Up: Modeling basics

8.1.1.1 Defining an ellipse and ellipsoid
.........................................

The PSF, see *note PSF::, and galaxy radial profiles are generally
defined on an ellipse.  Therefore, in this section we’ll start defining
an ellipse on a pixelated 2D surface.  Labeling the major axis of an
ellipse $a$, and its minor axis with $b$, the _axis ratio_ is defined
as: $q\equiv b/a$.  The major axis of an ellipse can be aligned in any
direction, therefore the angle of the major axis with respect to the
horizontal axis of the image is defined to be the _position angle_ of
the ellipse and in this book, we show it with $\theta$.

   Our aim is to put a radial profile of any functional form $f(r)$ over
an ellipse.  Hence we need to associate a radius/distance to every point
in space.  Let’s define the radial distance $r_{el}$ as the distance on
the major axis to the center of an ellipse which is located at $i_c$ and
$j_c$ (in other words $r_{el}\equiv{a}$).  We want to find $r_{el}$ of a
point located at $(i,j)$ (in the image coordinate system) from the
center of the ellipse with axis ratio $q$ and position angle $\theta$.
First the coordinate system is rotated(1) by $\theta$ to get the new
rotated coordinates of that point $(i_r,j_r)$:

           $$i_r(i,j)=+(i_c-i)\cos\theta+(j_c-j)\sin\theta$$
           $$j_r(i,j)=-(i_c-i)\sin\theta+(j_c-j)\cos\theta$$

Recall that an ellipse is defined by $(i_r/a)^2+(j_r/b)^2=1$ and that we
defined $r_{el}\equiv{a}$.  Hence, multiplying all elements of the
ellipse definition with $r_{el}^2$ we get the elliptical distance at
this point point located: $r_{el}=\sqrt{i_r^2+(j_r/q)^2}$.  To place the
radial profiles explained below over an ellipse, $f(r_{el})$ is
calculated based on the functional radial profile desired.

   An ellipse in 3D, or an ellipsoid
(https://en.wikipedia.org/wiki/Ellipsoid), can be defined following
similar principles as before.  Labeling the major (largest) axis length
as $a$, the second and third (in a right-handed coordinate system) axis
lengths can be labeled as $b$ and $c$.  Hence we have two axis ratios:
$q_1\equiv{b/a}$ and $q_2\equiv{c/a}$.  The orientation of the ellipsoid
can be defined from the orientation of its major axis.  There are many
ways to define 3D orientation and order matters.  So to be clear, here
we use the ZXZ (or $Z_1X_2Z_3$) proper Euler angles
(https://en.wikipedia.org/wiki/Euler_angles) to define the 3D
orientation.  In short, when a point is rotated in this order, we first
rotate it around the Z axis (third axis) by $\alpha$, then about the
(rotated) X axis by $\beta$ and finally about the (rotated) Z axis by
$\gamma$.

   Following the discussion in *note Merging multiple warpings::, we can
define the full rotation with the following matrix multiplication.
However, here we are rotating the coordinates, not the point.
Therefore, both the rotation angles and rotation order are reversed.  We
are also not using homogeneous coordinates (see *note Warping basics::)
since we aren’t concerned with translation in this context:

              $$\left[\matrix{i_r\cr j_r\cr k_r}\right] =
\left[\matrix{cos\gamma&sin\gamma&0\cr -sin\gamma&cos\gamma&0\cr 0&0&1}\right]
\left[\matrix{1&0&0\cr 0&cos\beta&sin\beta\cr 0&-sin\beta&cos\beta }\right]
\left[\matrix{cos\alpha&sin\alpha&0\cr -sin\alpha&cos\alpha&0\cr 0&0&1}\right]
           \left[\matrix{i_c-i\cr j_c-j\cr k_c-k}\right] $$

Recall that an ellipsoid can be characterized with
$(i_r/a)^2+(j_r/b)^2+(k_r/c)^2=1$, so similar to before
($r_{el}\equiv{a}$), we can find the ellipsoidal radius at pixel
$(i,j,k)$ as: $r_{el}=\sqrt{i_r^2+(j_r/q_1)^2+(k_r/q_2)^2}$.

   MakeProfiles builds the profile starting from the nearest element
(pixel in an image) in the dataset to the profile center.  The profile
value is calculated for that central pixel using monte carlo
integration, see *note Sampling from a function::.  The next pixel is
the next nearest neighbor to the central pixel as defined by $r_{el}$.
This process goes on until the profile is fully built upto the
truncation radius.  This is done fairly efficiently using a breadth
first parsing strategy(2) which is implemented through an ordered linked
list.

   Using this approach, we build the profile by expanding the
circumference.  Not one more extra pixel has to be checked (the
calculation of $r_{el}$ from above is not cheap in CPU terms).  Another
consequence of this strategy is that extending MakeProfiles to three
dimensions becomes very simple: only the neighbors of each pixel have to
be changed.  Everything else after that (when the pixel index and its
radial profile have entered the linked list) is the same, no matter the
number of dimensions we are dealing with.

   ---------- Footnotes ----------

   (1) Do not confuse the signs of $sin$ with the rotation matrix
defined in *note Warping basics::.  In that equation, the point is
rotated, here the coordinates are rotated and the point is fixed.

   (2) <http://en.wikipedia.org/wiki/Breadth-first_search>


File: gnuastro.info,  Node: PSF,  Next: Stars,  Prev: Defining an ellipse and ellipsoid,  Up: Modeling basics

8.1.1.2 Point spread function
.............................

Assume we have a ‘point’ source, or a source that is far smaller than
the maximum resolution (a pixel).  When we take an image of it, it will
‘spread’ over an area.  To quantify that spread, we can define a
‘function’.  This is how the point spread function or the PSF of an
image is defined.  This ‘spread’ can have various causes, for example in
ground based astronomy, due to the atmosphere.  In practice we can never
surpass the ‘spread’ due to the diffraction of the lens aperture.
Various other effects can also be quantified through a PSF. For example,
the simple fact that we are sampling in a discrete space, namely the
pixels, also produces a very small ‘spread’ in the image.

   Convolution is the mathematical process by which we can apply a
‘spread’ to an image, or in other words blur the image, see *note
Convolution process::.  The Brightness of an object should remain
unchanged after convolution, see *note Flux Brightness and magnitude::.
Therefore, it is important that the sum of all the pixels of the PSF be
unity.  The PSF image also has to have an odd number of pixels on its
sides so one pixel can be defined as the center.  In MakeProfiles, the
PSF can be set by the two methods explained below.

Parametric functions
     A known mathematical function is used to make the PSF. In this
     case, only the parameters to define the functions are necessary and
     MakeProfiles will make a PSF based on the given parameters for each
     function.  In both cases, the center of the profile has to be
     exactly in the middle of the central pixel of the PSF (which is
     automatically done by MakeProfiles).  When talking about the PSF,
     usually, the full width at half maximum or FWHM is used as a scale
     of the width of the PSF.

     ‘Gaussian’
          In the older papers, and to a lesser extent even today, some
          researchers use the 2D Gaussian function to approximate the
          PSF of ground based images.  In its most general form, a
          Gaussian function can be written as:

      $$f(r)=a \exp \left( -(x-\mu)^2 \over 2\sigma^2 \right)+d$$

          Since the center of the profile is pre-defined, $\mu$ and $d$
          are constrained.  $a$ can also be found because the function
          has to be normalized.  So the only important parameter for
          MakeProfiles is the $\sigma$.  In the Gaussian function we
          have this relation between the FWHM and $\sigma$:

      $$\rm{FWHM}_g=2\sqrt{2\ln{2}}\sigma \approx 2.35482\sigma$$

     ‘Moffat’
          The Gaussian profile is much sharper than the images taken
          from stars on photographic plates or CCDs.  Therefore in 1969,
          Moffat proposed this functional form for the image of stars:

  $$f(r)=a \left[ 1+\left( r\over \alpha \right)^2 \right]^{-\beta}$$

          Again, $a$ is constrained by the normalization, therefore two
          parameters define the shape of the Moffat function: $\alpha$
          and $\beta$.  The radial parameter is $\alpha$ which is
          related to the FWHM by

              $$\rm{FWHM}_m=2\alpha\sqrt{2^{1/\beta}-1}$$

          Comparing with the PSF predicted from atmospheric turbulence
          theory with a Moffat function, Trujillo et al.(1)  claim that
          $\beta$ should be 4.765.  They also show how the Moffat PSF
          contains the Gaussian PSF as a limiting case when
          $\beta\to\infty$.

An input FITS image
     An input image file can also be specified to be used as a PSF. If
     the sum of its pixels are not equal to 1, the pixels will be
     multiplied by a fraction so the sum does become 1.

   While the Gaussian is only dependent on the FWHM, the Moffat function
is also dependent on $\beta$.  Comparing these two functions with a
fixed FWHM gives the following results:

   • Within the FWHM, the functions don’t have significant differences.
   • For a fixed FWHM, as $\beta$ increases, the Moffat function becomes
     sharper.
   • The Gaussian function is much sharper than the Moffat functions,
     even when $\beta$ is large.

   ---------- Footnotes ----------

   (1) Trujillo, I., J. A. L. Aguerri, J. Cepa, and C. M. Gutierrez
(2001).  “The effects of seeing on Sérsic profiles - II. The Moffat
PSF”.  In: MNRAS 328, pp.  977—985.


File: gnuastro.info,  Node: Stars,  Next: Galaxies,  Prev: PSF,  Up: Modeling basics

8.1.1.3 Stars
.............

In MakeProfiles, stars are generally considered to be a point source.
This is usually the case for extra galactic studies, were nearby stars
are also in the field.  Since a star is only a point source, we assume
that it only fills one pixel prior to convolution.  In fact, exactly for
this reason, in astronomical images the light profiles of stars are one
of the best methods to understand the shape of the PSF and a very large
fraction of scientific research is preformed by assuming the shapes of
stars to be the PSF of the image.


File: gnuastro.info,  Node: Galaxies,  Next: Sampling from a function,  Prev: Stars,  Up: Modeling basics

8.1.1.4 Galaxies
................

Today, most practitioners agree that the flux of galaxies can be modeled
with one or a few generalized de Vaucouleur’s (or Sérsic) profiles.

$$I(r) = I_e \exp \left ( -b_n \left[ \left( r \over r_e \right)^{1/n} -1 \right] \right )$$

   Gérard de Vaucouleurs (1918-1995) was first to show in 1948 that this
function best fits the galaxy light profiles, with the only difference
that he held $n$ fixed to a value of 4.  20 years later in 1968, J. L.
Sérsic showed that $n$ can have a variety of values and does not
necessarily need to be 4.  This profile depends on the effective radius
($r_e$) which is defined as the radius which contains half of the
profile brightness (see *note Profile magnitude::).  $I_e$ is the flux
at the effective radius.  The Sérsic index $n$ is used to define the
concentration of the profile within $r_e$ and $b_n$ is a constant
dependent on $n$.  MacArthur et al.(1)  show that for $n>0.35$, $b_n$
can be accurately approximated using this equation:

$$b_n=2n - {1\over 3} + {4\over 405n} + {46\over 25515n^2} + {131\over 1148175n^3}-{2194697\over 30690717750n^4}$$

   ---------- Footnotes ----------

   (1) MacArthur, L. A., S. Courteau, and J. A. Holtzman (2003).
“Structure of Disk-dominated Galaxies.  I. Bulge/Disk Parameters,
Simulations, and Secular Evolution”.  In: ApJ 582, pp.  689—722.


File: gnuastro.info,  Node: Sampling from a function,  Next: Oversampling,  Prev: Galaxies,  Up: Modeling basics

8.1.1.5 Sampling from a function
................................

A pixel is the ultimate level of accuracy to gather data, we can’t get
any more accurate in one image, this is known as sampling in signal
processing.  However, the mathematical profiles which describe our
models have infinite accuracy.  Over a large fraction of the area of
astrophysically interesting profiles (for example galaxies or PSFs), the
variation of the profile over the area of one pixel is not too
significant.  In such cases, the elliptical radius ($r_{el}$ of the
center of the pixel can be assigned as the final value of the pixel, see
*note Defining an ellipse and ellipsoid::).

   As you approach their center, some galaxies become very sharp (their
value significantly changes over one pixel’s area).  This sharpness
increases with smaller effective radius and larger Sérsic values.  Thus
rendering the central value extremely inaccurate.  The first method that
comes to mind for solving this problem is integration.  The functional
form of the profile can be integrated over the pixel area in a 2D
integration process.  However, unfortunately numerical integration
techniques also have their limitations and when such sharp profiles are
needed they can become extremely inaccurate.

   The most accurate method of sampling a continuous profile on a
discrete space is by choosing a large number of random points within the
boundaries of the pixel and taking their average value (or Monte Carlo
integration).  This is also, generally speaking, what happens in
practice with the photons on the pixel.  The number of random points can
be set with ‘--numrandom’.

   Unfortunately, repeating this Monte Carlo process would be extremely
time and CPU consuming if it is to be applied to every pixel.  In order
to not loose too much accuracy, in MakeProfiles, the profile is built
using both methods explained below.  The building of the profile begins
from its central pixel and continues (radially) outwards.  Monte Carlo
integration is first applied (which yields $F_r$), then the central
pixel value ($F_c$) is calculated on the same pixel.  If the fractional
difference ($|F_r-F_c|/F_r$) is lower than a given tolerance level
(specified with ‘--tolerance’) MakeProfiles will stop using Monte Carlo
integration and only use the central pixel value.

   The ordering of the pixels in this inside-out construction is based
on $r=\sqrt{(i_c-i)^2+(j_c-j)^2}$, not $r_{el}$, see *note Defining an
ellipse and ellipsoid::.  When the axis ratios are large (near one) this
is fine.  But when they are small and the object is highly elliptical,
it might seem more reasonable to follow $r_{el}$ not $r$.  The problem
is that the gradient is stronger in pixels with smaller $r$ (and larger
$r_{el}$) than those with smaller $r_{el}$.  In other words, the
gradient is strongest along the minor axis.  So if the next pixel is
chosen based on $r_{el}$, the tolerance level will be reached sooner and
lots of pixels with large fractional differences will be missed.

   Monte Carlo integration uses a random number of points.  Thus, every
time you run it, by default, you will get a different distribution of
points to sample within the pixel.  In the case of large profiles, this
will result in a slight difference of the pixels which use Monte Carlo
integration each time MakeProfiles is run.  To have a deterministic
result, you have to fix the random number generator properties which is
used to build the random distribution.  This can be done by setting the
‘GSL_RNG_TYPE’ and ‘GSL_RNG_SEED’ environment variables and calling
MakeProfiles with the ‘--envseed’ option.  To learn more about the
process of generating random numbers, see *note Generating random
numbers::.

   The seed values are fixed for every profile: with ‘--envseed’, all
the profiles have the same seed and without it, each will get a
different seed using the system clock (which is accurate to within one
microsecond).  The same seed will be used to generate a random number
for all the sub-pixel positions of all the profiles.  So in the former,
the sub-pixel points checked for all the pixels undergoing Monte carlo
integration in all profiles will be identical.  In other words, the
sub-pixel points in the first (closest to the center) pixel of all the
profiles will be identical with each other.  All the second pixels
studied for all the profiles will also receive an identical (different
from the first pixel) set of sub-pixel points and so on.  As long as the
number of random points used is large enough or the profiles are not
identical, this should not cause any systematic bias.


File: gnuastro.info,  Node: Oversampling,  Prev: Sampling from a function,  Up: Modeling basics

8.1.1.6 Oversampling
....................

The steps explained in *note Sampling from a function:: do give an
accurate representation of a profile prior to convolution.  However, in
an actual observation, the image is first convolved with or blurred by
the atmospheric and instrument PSF in a continuous space and then it is
sampled on the discrete pixels of the camera.

   In order to more accurately simulate this process, the unconvolved
image and the PSF are created on a finer pixel grid.  In other words,
the output image is a certain odd-integer multiple of the desired size,
we can call this ‘oversampling’.  The user can specify this multiple as
a command-line option.  The reason this has to be an odd number is that
the PSF has to be centered on the center of its image.  An image with an
even number of pixels on each side does not have a central pixel.

   The image can then be convolved with the PSF (which should also be
oversampled on the same scale).  Finally, image can be sub-sampled to
get to the initial desired pixel size of the output image.  After this,
mock noise can be added as explained in the next section.  This is
because unlike the PSF, the noise occurs in each output pixel, not on a
continuous space like all the prior steps.


File: gnuastro.info,  Node: If convolving afterwards,  Next: Flux Brightness and magnitude,  Prev: Modeling basics,  Up: MakeProfiles

8.1.2 If convolving afterwards
------------------------------

In case you want to convolve the image later with a given point spread
function, make sure to use a larger image size.  After convolution, the
profiles become larger and a profile that is normally completely outside
of the image might fall within it.

   On one axis, if you want your final (convolved) image to be $m$
pixels and your PSF is $2n+1$ pixels wide, then when calling
MakeProfiles, set the axis size to $m+2n$, not $m$.  You also have to
shift all the pixel positions of the profile centers on the that axis by
$n$ pixels to the positive.

   After convolution, you can crop the outer $n$ pixels with the section
crop box specification of Crop: ‘--section=n:*-n,n:*-n’ assuming your
PSF is a square, see *note Crop section syntax::.  This will also remove
all discrete Fourier transform artifacts (blurred sides) from the final
image.  To facilitate this shift, MakeProfiles has the options
‘--xshift’, ‘--yshift’ and ‘--prepforconv’, see *note Invoking
astmkprof::.


File: gnuastro.info,  Node: Flux Brightness and magnitude,  Next: Profile magnitude,  Prev: If convolving afterwards,  Up: MakeProfiles

8.1.3 Flux Brightness and magnitude
-----------------------------------

Astronomical data pixels are usually in units of counts(1) or electrons
or either one divided by seconds.  To convert from the counts to
electrons, you will need to know the instrument gain.  In any case, they
can be directly converted to energy or energy/time using the basic
hardware (telescope, camera and filter) information.  We will continue
the discussion assuming the pixels are in units of energy/time.

   The _brightness_ of an object is defined as its total detected energy
per time.  This is simply the sum of the pixels that are associated with
that detection by our detection tool for example *note NoiseChisel::(2).
The _flux_ of an object is in units of energy/time/area and for a
detected object, it is defined as its brightness divided by the area
used to collect the light from the source or the telescope aperture (for
example in $cm^2$)(3).  Knowing the flux ($f$) and distance to the
object ($r$), we can calculate its _luminosity_: $L=4{\pi}r^2f$.
Therefore, flux and luminosity are intrinsic properties of the object,
while brightness depends on our detecting tools (hardware and software).
Here we will not be discussing luminosity, but brightness.  However,
since luminosity is the astrophysically interesting quantity, we also
defined it here to avoid possible confusion between these two terms
because they both have the same units.

   Images of astronomical objects span over a very large range of
brightness.  With the Sun (as the brightest object) being roughly
$2.5^{60}=10^{24}$ times brighter than the faintest galaxies we can
currently detect.  Therefore discussing brightness will be very hard,
and astronomers have chosen to use a logarithmic scale to talk about the
brightness of astronomical objects.  But the logarithm can only be
usable with a unit-less and always positive value.  Fortunately
brightness is always positive and to remove the units we divide the
brightness of the object ($B$) by a reference brightness ($B_r$).  We
then define the resulting logarithmic scale as $magnitude$ through the
following relation(4)

          $$m-m_r=-2.5\log_{10} \left( B \over B_r \right)$$

$m$ is defined as the magnitude of the object and $m_r$ is the
pre-defined magnitude of the reference brightness.  One particularly
easy condition is when $B_r=1$.  This will allow us to summarize all the
hardware specific parameters discussed above into one number as the
reference magnitude which is commonly known as the Zero-point(5)
magnitude.

   ---------- Footnotes ----------

   (1) Counts are also known as analog to digital units (ADU).

   (2) If further processing is done, for example the Kron or Petrosian
radii are calculated, then the detected area is not sufficient and the
total area that was within the respective radius must be used.

   (3) For a full object that spans over several pixels, the telescope
area should be used to find the flux.  However, sometimes, only the
brightness per pixel is desired.  In such cases this book also _loosely_
uses the term flux.  This is only approximately accurate however, since
while all the pixels have a fixed area, the pixel size can vary with
camera on the telescope.

   (4) The $-2.5$ factor in the definition of magnitudes is a legacy of
the our ancient colleagues and in particular Hipparchus of Nicaea
(190-120 BC).

   (5) When $B=Br=1$, the right side of the magnitude definition will be
zero.  Hence the name, “zero-point”.


File: gnuastro.info,  Node: Profile magnitude,  Next: Invoking astmkprof,  Prev: Flux Brightness and magnitude,  Up: MakeProfiles

8.1.4 Profile magnitude
-----------------------

To find the profile brightness or its magnitude, (see *note Flux
Brightness and magnitude::), it is customary to use the 2D integration
of the flux to infinity.  However, in MakeProfiles we do not follow this
idealistic approach and apply a more realistic method to find the total
brightness or magnitude: the sum of all the pixels belonging to a
profile within its predefined truncation radius.  Note that if the
truncation radius is not large enough, this can be significantly
different from the total integrated light to infinity.

   An integration to infinity is not a realistic condition because no
galaxy extends indefinitely (important for high Sérsic index profiles),
pixelation can also cause a significant difference between the actual
total pixel sum value of the profile and that of integration to
infinity, especially in small and high Sérsic index profiles.  To be
safe, you can specify a large enough truncation radius for such compact
high Sérsic index profiles.

   If oversampling is used then the brightness is calculated using the
over-sampled image, see *note Oversampling:: which is much more
accurate.  The profile is first built in an array completely bounding it
with a normalization constant of unity (see *note Galaxies::).  Taking
$B$ to be the desired brightness and $S$ to be the sum of the pixels in
the created profile, every pixel is then multiplied by $B/S$ so the sum
is exactly $B$.

   If the ‘--individual’ option is called, this same array is written to
a FITS file.  If not, only the overlapping pixels of this array and the
output image are kept and added to the output array.


File: gnuastro.info,  Node: Invoking astmkprof,  Prev: Profile magnitude,  Up: MakeProfiles

8.1.5 Invoking MakeProfiles
---------------------------

MakeProfiles will make any number of profiles specified in a catalog
either individually or in one image.  The executable name is ‘astmkprof’
with the following general template

     $ astmkprof [OPTION ...] [Catalog]

One line examples:

     ## Make an image with profiles in catalog.txt (with default size):
     $ astmkprof catalog.txt

     ## Make the profiles in catalog.txt over image.fits:
     $ astmkprof --background=image.fits catalog.txt

     ## Make a Moffat PSF with FWHM 3pix, beta=2.8, truncation=5
     $ astmkprof --kernel=moffat,2.8,5 --oversample=1

     ## Make profiles in catalog, using RA and Dec in the given column:
     $ astmkprof --ccol=RA_CENTER --ccol=DEC_CENTER --mode=wcs catalog.txt

     ## Make a 1500x1500 merged image (oversampled 500x500) image along
     ## with an individual image for all the profiles in catalog:
     $ astmkprof --individual --oversample 3 --mergedsize=500,500 cat.txt

The parameters of the mock profiles can either be given through a
catalog (which stores the parameters of many mock profiles, see *note
MakeProfiles catalog::), or the ‘--kernel’ option (see *note
MakeProfiles output dataset::).  The catalog can be in the FITS ASCII,
FITS binary format, or plain text formats (see *note Tables::).  A plain
text catalog can also be provided using the Standard input (see *note
Standard input::).  The columns related to each parameter can be
determined both by number, or by match/search criteria using the column
names, units, or comments, with the options ending in ‘col’, see below.

   Without any file given to the ‘--background’ option, MakeProfiles
will make a zero-valued image and build the profiles on that (its size
and main WCS parameters can also be defined through the options
described in *note MakeProfiles output dataset::).  Besides the
main/merged image containing all the profiles in the catalog, it is also
possible to build individual images for each profile (only enclosing one
full profile to its truncation radius) with the ‘--individual’ option.

   If an image is given to the ‘--background’ option, the pixels of that
image are used as the background value for every pixel.  The flux value
of each profile pixel will be added to the pixel in that background
value.  In this case, the values to all options relating to the output
size and WCS will be ignored if specified (for example ‘--oversample’,
‘--mergedsize’, and ‘--prepforconv’) on the command-line or in the
configuration files.

   The sections below discuss the options specific to MakeProfiles based
on context: the input catalog settings which can have many rows for
different profiles are discussed in *note MakeProfiles catalog::, in
*note MakeProfiles profile settings::, we discuss how you can set
general profile settings (that are the same for all the profiles in the
catalog).  Finally *note MakeProfiles output dataset:: and *note
MakeProfiles log file:: discuss the outputs of MakeProfiles and how you
can configure them.  Besides these, MakeProfiles also supports all the
common Gnuastro program options that are discussed in *note Common
options::, so please flip through them is well for a more comfortable
usage.

   When building 3D profiles, there are more degrees of freedom.  Hence,
more columns are necessary and all the values related to dimensions (for
example size of dataset in each dimension and the WCS properties) must
also have 3 values.  To allow having an independent set of default
values for creating 3D profiles, MakeProfiles also installs a
‘astmkprof-3d.conf’ configuration file (see *note Configuration
files::).  You can use this for default 3D profile values.  For example,
if you installed Gnuastro with the prefix ‘/usr/local’ (the default
location, see *note Installation directory::), you can benefit from this
configuration file by running MakeProfiles like the example below.  As
with all configuration files, if you want to customize a given option,
call it before the configuration file.

     $ astmkprof --config=/usr/local/etc/astmkprof-3d.conf catalog.txt

   To further simplify the process, you can define a shell alias in any
startup file (for example ‘~/.bashrc’, see *note Installation
directory::).  Assuming that you installed Gnuastro in ‘/usr/local’, you
can add this line to the startup file (you may put it all in one line,
it is broken into two lines here for fitting within page limits).

     alias astmkprof-3d="astmkprof --config=/usr/local/etc/astmkprof-3d.conf"

Using this alias, you can call MakeProfiles with the name ‘astmkprof-3d’
(instead of ‘astmkprof’).  It will automatically load the 3D specific
configuration file first, and then parse any other arguments, options or
configuration files.  You can change the default values in this 3D
configuration file by calling them on the command-line as you do with
‘astmkprof’(1).

   Please see *note Sufi simulates a detection:: for a very complete
tutorial explaining how one could use MakeProfiles in conjunction with
other Gnuastro’s programs to make a complete simulated image of a mock
galaxy.

* Menu:

* MakeProfiles catalog::        Required catalog properties.
* MakeProfiles profile settings::  Configuration parameters for all profiles.
* MakeProfiles output dataset::  The canvas/dataset to build profiles over.
* MakeProfiles log file::       A description of the optional log file.

   ---------- Footnotes ----------

   (1) Recall that for single-invocation options, the last command-line
invocation takes precedence over all previous invocations (including
those in the 3D configuration file).  See the description of ‘--config’
in *note Operating mode options::.


File: gnuastro.info,  Node: MakeProfiles catalog,  Next: MakeProfiles profile settings,  Prev: Invoking astmkprof,  Up: Invoking astmkprof

8.1.5.1 MakeProfiles catalog
............................

The catalog containing information about each profile can be in the FITS
ASCII, FITS binary, or plain text formats (see *note Tables::).  The
latter can also be provided using standard input (see *note Standard
input::).  Its columns can be ordered in any desired manner.  You can
specify which columns belong to which parameters using the set of
options discussed below.  For example through the ‘--rcol’ and ‘--tcol’
options, you can specify the column that contains the radial parameter
for each profile and its truncation respectively.  See *note Selecting
table columns:: for a thorough discussion on the values to these
options.

   The value for the profile center in the catalog (the ‘--ccol’ option)
can be a floating point number so the profile center can be on any
sub-pixel position.  Note that pixel positions in the FITS standard
start from 1 and an integer is the pixel center.  So a 2D image actually
starts from the position (0.5, 0.5), which is the bottom-left corner of
the first pixel.  When a ‘--background’ image with WCS information is
provided or you specify the WCS parameters with the respective options,
you may also use RA and Dec to identify the center of each profile (see
the ‘--mode’ option below).

   In MakeProfiles, profile centers do not have to be in (overlap with)
the final image.  Even if only one pixel of the profile within the
truncation radius overlaps with the final image size, the profile is
built and included in the final image image.  Profiles that are
completely out of the image will not be created (unless you explicitly
ask for it with the ‘--individual’ option).  You can use the output log
file (created with ‘--log’ to see which profiles were within the image,
see *note Common options::.

   If PSF profiles (Moffat or Gaussian, see *note PSF::) are in the
catalog and the profiles are to be built in one image (when
‘--individual’ is not used), it is assumed they are the PSF(s) you want
to convolve your created image with.  So by default, they will not be
built in the output image but as separate files.  The sum of pixels of
these separate files will also be set to unity (1) so you are ready to
convolve, see *note Convolution process::.  As a summary, the position
and magnitude of PSF profile will be ignored.  This behavior can be
disabled with the ‘--psfinimg’ option.  If you want to create all the
profiles separately (with ‘--individual’) and you want the sum of the
PSF profile pixels to be unity, you have to set their magnitudes in the
catalog to the zero-point magnitude and be sure that the central
positions of the profiles don’t have any fractional part (the PSF center
has to be in the center of the pixel).

   The list of options directly related to the input catalog columns is
shown below.

‘--ccol=STR/INT’
     Center coordinate column for each dimension.  This option must be
     called two times to define the center coordinates in an image.  For
     example ‘--ccol=RA’ and ‘--ccol=DEC’ (along with ‘--mode=wcs’) will
     inform MakeProfiles to look into the catalog columns named ‘RA’ and
     ‘DEC’ for the Right Ascension and Declination of the profile
     centers.

‘--fcol=INT/STR’
     The functional form of the profile with one of the values below
     depending on the desired profile.  The column can contain either
     the numeric codes (for example ‘‘1’’) or string characters (for
     example ‘‘sersic’’).  The numeric codes are easier to use in
     scripts which generate catalogs with hundreds or thousands of
     profiles.

     The string format can be easier when the catalog is to be
     written/checked by hand/eye before running MakeProfiles.  It is
     much more readable and provides a level of documentation.  All
     Gnuastro’s recognized table formats (see *note Recognized table
     formats::) accept string type columns.  To have string columns in a
     plain text table/catalog, see *note Gnuastro text table format::.

        • Sérsic profile with ‘‘sersic’’ or ‘‘1’’.

        • Moffat profile with ‘‘moffat’’ or ‘‘2’’.

        • Gaussian profile with ‘‘gaussian’’ or ‘‘3’’.

        • Point source with ‘‘point’’ or ‘‘4’’.

        • Flat profile with ‘‘flat’’ or ‘‘5’’.

        • Circumference profile with ‘‘circum’’ or ‘‘6’’.  A fixed value
          will be used for all pixels less than or equal to the
          truncation radius ($r_t$) and greater than $r_t-w$ ($w$ is the
          value to the ‘--circumwidth’).

        • Radial distance profile with ‘‘distance’’ or ‘‘7’’.  At the
          lowest level, each pixel only has an elliptical radial
          distance given the profile’s shape and orientation (see *note
          Defining an ellipse and ellipsoid::).  When this profile is
          chosen, the pixel’s elliptical radial distance from the
          profile center is written as its value.  For this profile, the
          value in the magnitude column (‘--mcol’) will be ignored.

          You can use this for checks or as a first approximation to
          define your own higher-level radial function.  In the latter
          case, just note that the central values are going to be
          incorrect (see *note Sampling from a function::).

‘--rcol=STR/INT’
     The radius parameter of the profiles.  Effective radius ($r_e$) if
     Sérsic, FWHM if Moffat or Gaussian.

‘--ncol=STR/INT’
     The Sérsic index ($n$) or Moffat $\beta$.

‘--pcol=STR/INT’
     The position angle (in degrees) of the profiles relative to the
     first FITS axis (horizontal when viewed in SAO ds9).  When building
     a 3D profile, this is the first Euler angle: first rotation of the
     ellipsoid major axis from the first FITS axis (rotating about the
     third axis).  See *note Defining an ellipse and ellipsoid::.

‘--p2col=STR/INT’
     Second Euler angle (in degrees) when building a 3D ellipsoid.  This
     is the second rotation of the ellipsoid major axis (following
     ‘--pcol’) about the (rotated) X axis.  See *note Defining an
     ellipse and ellipsoid::.  This column is ignored when building a 2D
     profile.

‘--p3col=STR/INT’
     Third Euler angle (in degrees) when building a 3D ellipsoid.  This
     is the third rotation of the ellipsoid major axis (following
     ‘--pcol’ and ‘--p2col’) about the (rotated) Z axis.  See *note
     Defining an ellipse and ellipsoid::.  This column is ignored when
     building a 2D profile.

‘--qcol=STR/INT’
     The axis ratio of the profiles (minor axis divided by the major
     axis in a 2D ellipse).  When building a 3D ellipse, this is the
     ratio of the major axis to the semi-axis length of the second
     dimension (in a right-handed coordinate system).  See $q1$ in *note
     Defining an ellipse and ellipsoid::.

‘--q2col=STR/INT’
     The ratio of the ellipsoid major axis to the third semi-axis length
     (in a right-handed coordinate system) of a 3D ellipsoid.  See $q1$
     in *note Defining an ellipse and ellipsoid::.  This column is
     ignored when building a 2D profile.

‘--mcol=STR/INT’
     The total pixelated magnitude of the profile within the truncation
     radius, see *note Profile magnitude::.

‘--tcol=STR/INT’
     The truncation radius of this profile.  By default it is in units
     of the radial parameter of the profile (the value in the ‘--rcol’
     of the catalog).  If ‘--tunitinp’ is given, this value is
     interpreted in units of pixels (prior to oversampling) irrespective
     of the profile.


File: gnuastro.info,  Node: MakeProfiles profile settings,  Next: MakeProfiles output dataset,  Prev: MakeProfiles catalog,  Up: Invoking astmkprof

8.1.5.2 MakeProfiles profile settings
.....................................

The profile parameters that differ between each created profile are
specified through the columns in the input catalog and described in
*note MakeProfiles catalog::.  Besides those there are general settings
for some profiles that don’t differ between one profile and another,
they are a property of the general process.  For example how many random
points to use in the monte-carlo integration, this value is fixed for
all the profiles.  The options described in this section are for
configuring such properties.

‘--mode=STR’
     Interpret the center position columns (‘--ccol’ in *note
     MakeProfiles catalog::) in image or WCS coordinates.  This option
     thus accepts only two values: ‘img’ and ‘wcs’.  It is mandatory
     when a catalog is being used as input.

‘-r’
‘--numrandom’
     The number of random points used in the central regions of the
     profile, see *note Sampling from a function::.

‘-e’
‘--envseed’
     Use the value to the ‘GSL_RNG_SEED’ environment variable to
     generate the random Monte Carlo sampling distribution, see *note
     Sampling from a function:: and *note Generating random numbers::.

‘-t FLT’
‘--tolerance=FLT’
     The tolerance to switch from Monte Carlo integration to the central
     pixel value, see *note Sampling from a function::.

‘-p’
‘--tunitinp’
     The truncation column of the catalog is in units of pixels.  By
     default, the truncation column is considered to be in units of the
     radial parameters of the profile (‘--rcol’).  Read it as
     ‘t-unit-in-p’ for ‘truncation unit in pixels’.

‘-f’
‘--mforflatpix’
     When making fixed value profiles (flat and circumference, see
     ‘‘--fcol’’), don’t use the value in the column specified by
     ‘‘--mcol’’ as the magnitude.  Instead use it as the exact value
     that all the pixels of these profiles should have.  This option is
     irrelevant for other types of profiles.  This option is very useful
     for creating masks, or labeled regions in an image.  Any integer,
     or floating point value can used in this column with this option,
     including ‘NaN’ (or ‘‘nan’’, or ‘‘NAN’’, case is irrelevant), and
     infinities (‘inf’, ‘-inf’, or ‘+inf’).

     For example, with this option if you set the value in the magnitude
     column (‘--mcol’) to ‘NaN’, you can create an elliptical or
     circular mask over an image (which can be given as the argument),
     see *note Blank pixels::.  Another useful application of this
     option is to create labeled elliptical or circular apertures in an
     image.  To do this, set the value in the magnitude column to the
     label you want for this profile.  This labeled image can then be
     used in combination with NoiseChisel’s output (see *note
     NoiseChisel output::) to do aperture photometry with MakeCatalog
     (see *note MakeCatalog::).

     Alternatively, if you want to mark regions of the image (for
     example with an elliptical circumference) and you don’t want to use
     NaN values (as explained above) for some technical reason, you can
     get the minimum or maximum value in the image (1) using Arithmetic
     (see *note Arithmetic::), then use that value in the magnitude
     column along with this option for all the profiles.

     Please note that when using MakeProfiles on an already existing
     image, you have to set ‘‘--oversample=1’’.  Otherwise all the
     profiles will be scaled up based on the oversampling scale in your
     configuration files (see *note Configuration files::) unless you
     have accounted for oversampling in your catalog.

‘--mcolisbrightness’
     The value given in the “magnitude column” (specified by ‘--mcol’,
     see *note MakeProfiles catalog::) must be interpreted as
     brightness, not magnitude.  The zeropoint magnitude (value to the
     ‘--zeropoint’ option) is ignored and the given value must have the
     same units as the input dataset’s pixels.

     Recall that the total profile magnitude or brightness that is
     specified with in the ‘--mcol’ column of the input catalog is not
     an integration to infinity, but the actual sum of pixels in the
     profile (until the desired truncation radius).  See *note Profile
     magnitude:: for more on this point.

‘--magatpeak’
     The magnitude column in the catalog (see *note MakeProfiles
     catalog::) will be used to find the brightness only for the peak
     profile pixel, not the full profile.  Note that this is the flux of
     the profile’s peak pixel in the final output of MakeProfiles.  So
     beware of the oversampling, see *note Oversampling::.

     This option can be useful if you want to check a mock profile’s
     total magnitude at various truncation radii.  Without this option,
     no matter what the truncation radius is, the total magnitude will
     be the same as that given in the catalog.  But with this option,
     the total magnitude will become brighter as you increase the
     truncation radius.

     In sharper profiles, sometimes the accuracy of measuring the peak
     profile flux is more than the overall object brightness.  In such
     cases, with this option, the final profile will be built such that
     its peak has the given magnitude, not the total profile.

     *CAUTION:* If you want to use this option for comparing with
     observations, please note that MakeProfiles does not do
     convolution.  Unless you have de-convolved your data, your images
     are convolved with the instrument and atmospheric PSF, see *note
     PSF::.  Particularly in sharper profiles, the flux in the peak
     pixel is strongly decreased after convolution.  Also note that in
     such cases, besides de-convolution, you will have to set
     ‘--oversample=1’ otherwise after resampling your profile with Warp
     (see *note Warp::), the peak flux will be different.

‘-X INT,INT’
‘--shift=INT,INT’
     Shift all the profiles and enlarge the image along each dimension.
     To better understand this option, please see $n$ in *note If
     convolving afterwards::.  This is useful when you want to convolve
     the image afterwards.  If you are using an external PSF, be sure to
     oversample it to the same scale used for creating the mock images.
     If a background image is specified, any possible value to this
     option is ignored.

‘-c’
‘--prepforconv’
     Shift all the profiles and enlarge the image based on half the
     width of the first Moffat or Gaussian profile in the catalog,
     considering any possible oversampling see *note If convolving
     afterwards::.  ‘--prepforconv’ is only checked and possibly
     activated if ‘--xshift’ and ‘--yshift’ are both zero (after reading
     the command-line and configuration files).  If a background image
     is specified, any possible value to this option is ignored.

‘-z FLT’
‘--zeropoint=FLT’
     The zero-point magnitude of the image.

‘-w FLT’
‘--circumwidth=FLT’
     The width of the circumference if the profile is to be an
     elliptical circumference or annulus.  See the explanations for this
     type of profile in ‘--fcol’.

‘-R’
‘--replace’
     Do not add the pixels of each profile over the background (possibly
     crowded by other profiles), replace them.  By default, when two
     profiles overlap, the final pixel value is the sum of all the
     profiles that overlap on that pixel.  When this option is given,
     the pixels are not added but replaced by the newer profile’s pixel
     and any value under it is lost.

     When order matters, make sure to use this function with
     ‘‘--numthreads=1’’.  When multiple threads are used, the separate
     profiles are built asynchronously and not in order.  Since order
     does not matter in an addition, this causes no problems by default
     but has to be considered when this option is given.  Using multiple
     threads is no problem if the profiles are to be used as a mask with
     a blank or fixed value (see ‘‘--mforflatpix’’) since all their
     pixel values are the same.

     Note that only non-zero pixels are replaced.  With radial profiles
     (for example Sérsic or Moffat) only values above zero will be part
     of the profile.  However, when using flat profiles with the
     ‘‘--mforflatpix’’ option, you should be careful not to give a ‘0.0’
     value as the flat profile’s pixel value.

   ---------- Footnotes ----------

   (1) The minimum will give a better result, because the maximum can be
too high compared to most pixels in the image, making it harder to
display.


File: gnuastro.info,  Node: MakeProfiles output dataset,  Next: MakeProfiles log file,  Prev: MakeProfiles profile settings,  Up: Invoking astmkprof

8.1.5.3 MakeProfiles output dataset
...................................

MakeProfiles takes an input catalog uses basic properties that are
defined there to build a dataset, for example a 2D image containing the
profiles in the catalog.  In *note MakeProfiles catalog:: and *note
MakeProfiles profile settings::, the catalog and profile settings were
discussed.  The options of this section, allow you to configure the
output dataset (or the canvas that will host the built profiles).

‘-k STR’
‘--background=STR’
     A background image FITS file to build the profiles on.  The
     extension that contains the image should be specified with the
     ‘--backhdu’ option, see below.  When a background image is
     specified, it will be used to derive all the information about the
     output image.  Hence, the following options will be ignored:
     ‘--mergedsize’, ‘--oversample’, ‘--crpix’, ‘--crval’ (generally,
     all other WCS related parameters) and the output’s data type (see
     ‘--type’ in *note Input output options::).

     The image will act like a canvas to build the profiles on: profile
     pixel values will be summed with the background image pixel values.
     With the ‘--replace’ option you can disable this behavior and
     replace the profile pixels with the background pixels.  If you want
     to use all the image information above, except for the pixel values
     (you want to have a blank canvas to build the profiles on, based on
     an input image), you can call ‘--clearcanvas’, to set all the input
     image’s pixels to zero before starting to build the profiles over
     it (this is done in memory after reading the input, so nothing will
     happen to your input file).

‘-B STR/INT’
‘--backhdu=STR/INT’
     The header data unit (HDU) of the file given to ‘--background’.

‘-C’
‘--clearcanvas’
     When an input image is specified (with the ‘--background’ option,
     set all its pixels to 0.0 immediately after reading it into memory.
     Effectively, this will allow you to use all its properties
     (described under the ‘--background’ option), without having to
     worry about the pixel values.

     ‘--clearcanvas’ can come in handy in many situations, for example
     if you want to create a labeled image (segmentation map) for
     creating a catalog (see *note MakeCatalog::).  In other cases, you
     might have modeled the objects in an image and want to create them
     on the same frame, but without the original pixel values.

‘-E STR/INT,FLT[,FLT,[...]]’
‘--kernel=STR/INT,FLT[,FLT,[...]]’
     Only build one kernel profile with the parameters given as the
     values to this option.  The different values must be separated by a
     comma (<,>).  The first value identifies the radial function of the
     profile, either through a string or through a number (see
     description of ‘--fcol’ in *note MakeProfiles catalog::).  Each
     radial profile needs a different total number of parameters: Sérsic
     and Moffat functions need 3 parameters: radial, Sérsic index or
     Moffat $\beta$, and truncation radius.  The Gaussian function needs
     two parameters: radial and truncation radius.  The point function
     doesn’t need any parameters and flat and circumference profiles
     just need one parameter (truncation radius).

     The PSF or kernel is a unique (and highly constrained) type of
     profile: the sum of its pixels must be one, its center must be the
     center of the central pixel (in an image with an odd number of
     pixels on each side), and commonly it is circular, so its axis
     ratio and position angle are one and zero respectively.  Kernels
     are commonly necessary for various data analysis and data
     manipulation steps (for example see *note Convolve::, and *note
     NoiseChisel::.  Because of this it is inconvenient to define a
     catalog with one row and many zero valued columns (for all the
     non-necessary parameters).  Hence, with this option, it is possible
     to create a kernel with MakeProfiles without the need to create a
     catalog.  Here are some examples:

     ‘--kernel=moffat,3,2.8,5’
          A Moffat kernel with FWHM of 3 pixels, $\beta=2.8$ which is
          truncated at 5 times the FWHM.

     ‘--kernel=gaussian,2,3’
          A circular Gaussian kernel with FWHM of 2 pixels and truncated
          at 3 times the FWHM.

     This option may also be used to create a 3D kernel.  To do that,
     two small modifications are necessary: add a ‘-3d’ (or ‘-3D’) to
     the profile name (for example ‘moffat-3d’) and add a number
     (axis-ratio along the third dimension) to the end of the parameters
     for all profiles except ‘point’.  The main reason behind providing
     an axis ratio in the third dimension is that in 3D astronomical
     datasets, commonly the third dimension doesn’t have the same nature
     (units/sampling) as the first and second.

     For example in IFU datacubes, the first and second dimensions are
     angularpositions (like RA and Dec) but the third is in units of
     Angstroms for wavelength.  Because of this different nature (which
     also affects theprocessing), it may be necessary for the kernel to
     have a different extent in that direction.

     If the 3rd dimension axis ratio is equal to $1.0$, then the kernel
     will be a spheroid.  If its smaller than $1.0$, the kernel will be
     button-shaped: extended less in the third dimension.  However, when
     it islarger than $1.0$, the kernel will be bullet-shaped: extended
     more in the third dimension.  In the latter case, the radial
     parameter will correspond to the length along the 3rd dimension.
     For example, let’s have a look at the two examples above but in 3D:

     ‘--kernel=moffat-3d,3,2.8,5,0.5’
          An ellipsoid Moffat kernel with FWHM of 3 pixels, $\beta=2.8$
          which is truncated at 5 times the FWHM. The ellipsoid is
          circular in the first two dimensions, but in the third
          dimension its extent is half the first two.

     ‘--kernel=gaussian-3d,2,3,1’
          A spherical Gaussian kernel with FWHM of 2 pixels and
          truncated at 3 times the FWHM.

     Ofcourse, if a specific kernel is needed that doesn’t fit the
     constraints imposed by this option, you can always use a catalog to
     define any arbitrary kernel.  Just call the ‘--individual’ and
     ‘--nomerged’ options to make sure that it is built as a separate
     file (individually) and no “merged” image of the input profiles is
     created.

‘-x INT,INT’
‘--mergedsize=INT,INT’
     The number of pixels along each axis of the output, in FITS order.
     This is before over-sampling.  For example if you call MakeProfiles
     with ‘--mergedsize=100,150 --oversample=5’ (assuming no shift due
     for later convolution), then the final image size along the first
     axis will be 500 by 750 pixels.  Fractions are acceptable as values
     for each dimension, however, they must reduce to an integer, so
     ‘--mergedsize=150/3,300/3’ is acceptable but
     ‘--mergedsize=150/4,300/4’ is not.

     When viewing a FITS image in DS9, the first FITS dimension is in
     the horizontal direction and the second is vertical.  As an
     example, the image created with the example above will have 500
     pixels horizontally and 750 pixels vertically.

     If a background image is specified, this option is ignored.

‘-s INT’
‘--oversample=INT’
     The scale to over-sample the profiles and final image.  If not an
     odd number, will be added by one, see *note Oversampling::.  Note
     that this ‘--oversample’ will remain active even if an input image
     is specified.  If your input catalog is based on the background
     image, be sure to set ‘--oversample=1’.

‘--psfinimg’
     Build the possibly existing PSF profiles (Moffat or Gaussian) in
     the catalog into the final image.  By default they are built
     separately so you can convolve your images with them, thus their
     magnitude and positions are ignored.  With this option, they will
     be built in the final image like every other galaxy profile.  To
     have a final PSF in your image, make a point profile where you want
     the PSF and after convolution it will be the PSF.

‘-i’
‘--individual’
     If this option is called, each profile is created in a separate
     FITS file within the same directory as the output and the row
     number of the profile (starting from zero) in the name.  The file
     for each row’s profile will be in the same directory as the final
     combined image of all the profiles and will have the final image’s
     name as a suffix.  So for example if the final combined image is
     named ‘./out/fromcatalog.fits’, then the first profile that will be
     created with this option will be named ‘./out/0_fromcatalog.fits’.

     Since each image only has one full profile out to the truncation
     radius the profile is centered and so, only the sub-pixel position
     of the profile center is important for the outputs of this option.
     The output will have an odd number of pixels.  If there is no
     oversampling, the central pixel will contain the profile center.
     If the value to ‘--oversample’ is larger than unity, then the
     profile center is on any of the central ‘--oversample’’d pixels
     depending on the fractional value of the profile center.

     If the fractional value is larger than half, it is on the bottom
     half of the central region.  This is due to the FITS definition of
     a real number position: The center of a pixel has fractional value
     $0.00$ so each pixel contains these fractions: .5 – .75 – .00
     (pixel center) – .25 – .5.

‘-m’
‘--nomerged’
     Don’t make a merged image.  By default after making the profiles,
     they are added to a final image with side lengths specified by
     ‘--mergedsize’ if they overlap with it.

The options below can be used to define the world coordinate system
(WCS) properties of the MakeProfiles outputs.  The option names are
deliberately chosen to be the same as the FITS standard WCS keywords.
See Section 8 of Pence et al [2010]
(https://doi.org/10.1051/0004-6361/201015362) for a short introduction
to WCS in the FITS standard(1).

   If you look into the headers of a FITS image with WCS for example you
will see all these names but in uppercase and with numbers to represent
the dimensions, for example ‘CRPIX1’ and ‘PC2_1’.  You can see the FITS
headers with Gnuastro’s *note Fits:: program using a command like this:
‘$ astfits -p image.fits’.

   If the values given to any of these options does not correspond to
the number of dimensions in the output dataset, then no WCS information
will be added.

‘--crpix=FLT,FLT’
     The pixel coordinates of the WCS reference point.  Fractions are
     acceptable for the values of this option.

‘--crval=FLT,FLT’
     The WCS coordinates of the Reference point.  Fractions are
     acceptable for the values of this option.

‘--cdelt=FLT,FLT’
     The resolution (size of one data-unit or pixel in WCS units) of the
     non-oversampled dataset.  Fractions are acceptable for the values
     of this option.

‘--pc=FLT,FLT,FLT,FLT’
     The PC matrix of the WCS rotation, see the FITS standard (link
     above) to better understand the PC matrix.

‘--cunit=STR,STR’
     The units of each WCS axis, for example ‘deg’.  Note that these
     values are part of the FITS standard (link above).  MakeProfiles
     won’t complain if you use non-standard values, but later usage of
     them might cause trouble.

‘--ctype=STR,STR’
     The type of each WCS axis, for example ‘RA---TAN’ and ‘DEC--TAN’.
     Note that these values are part of the FITS standard (link above).
     MakeProfiles won’t complain if you use non-standard values, but
     later usage of them might cause trouble.

   ---------- Footnotes ----------

   (1) The world coordinate standard in FITS is a very beautiful and
powerful concept to link/associate datasets with the outside world
(other datasets).  The description in the FITS standard (link above)
only touches the tip of the ice-burg.  To learn more please see Greisen
and Calabretta [2002] (https://doi.org/10.1051/0004-6361:20021326),
Calabretta and Greisen [2002]
(https://doi.org/10.1051/0004-6361:20021327), Greisen et al.  [2006]
(https://doi.org/10.1051/0004-6361:20053818), and Calabretta et al.
(http://www.atnf.csiro.au/people/mcalabre/WCS/dcs_20040422.pdf)


File: gnuastro.info,  Node: MakeProfiles log file,  Prev: MakeProfiles output dataset,  Up: Invoking astmkprof

8.1.5.4 MakeProfiles log file
.............................

Besides the final merged dataset of all the profiles, or the individual
datasets (see *note MakeProfiles output dataset::), if the ‘--log’
option is called MakeProfiles will also create a log file in the current
directory (where you run MockProfiles).  See *note Common options:: for
a full description of ‘--log’ and other options that are shared between
all Gnuastro programs.  The values for each column are explained in the
first few commented lines of the log file (starting with ‘#’ character).
Here is a more complete description.

   • An ID (row number of profile in input catalog).

   • The total magnitude of the profile in the output dataset.  When the
     profile does not completely overlap with the output dataset, this
     will be different from your input magnitude.

   • The number of pixels (in the oversampled image) which used Monte
     Carlo integration and not the central pixel value, see *note
     Sampling from a function::.

   • The fraction of flux in the Monte Carlo integrated pixels.

   • If an individual image was created, this column will have a value
     of ‘1’, otherwise it will have a value of ‘0’.


File: gnuastro.info,  Node: MakeNoise,  Prev: MakeProfiles,  Up: Modeling and fittings

8.2 MakeNoise
=============

Real data are always buried in noise, therefore to finalize a simulation
of real data (for example to test our observational algorithms) it is
essential to add noise to the mock profiles created with MakeProfiles,
see *note MakeProfiles::.  Below, the general principles and concepts to
help understand how noise is quantified is discussed.  MakeNoise options
and argument are then discussed in *note Invoking astmknoise::.

* Menu:

* Noise basics::                Noise concepts and definitions.
* Invoking astmknoise::         Options and arguments to MakeNoise.


File: gnuastro.info,  Node: Noise basics,  Next: Invoking astmknoise,  Prev: MakeNoise,  Up: MakeNoise

8.2.1 Noise basics
------------------

Deep astronomical images, like those used in extragalactic studies,
seriously suffer from noise in the data.  Generally speaking, the
sources of noise in an astronomical image are photon counting noise and
Instrumental noise which are discussed in *note Photon counting noise::
and *note Instrumental noise::.  This review finishes with *note
Generating random numbers:: which is a short introduction on how random
numbers are generated.  We will see that while software random number
generators are not perfect, they allow us to obtain a reproducible
series of random numbers through setting the random number generator
function and seed value.  Therefore in this section, we’ll also discuss
how you can set these two parameters in Gnuastro’s programs (including
MakeNoise).

* Menu:

* Photon counting noise::       Poisson noise
* Instrumental noise::          Readout, dark current and other sources.
* Final noised pixel value::    How the final noised value is calculated.
* Generating random numbers::   How random numbers are generated.


File: gnuastro.info,  Node: Photon counting noise,  Next: Instrumental noise,  Prev: Noise basics,  Up: Noise basics

8.2.1.1 Photon counting noise
.............................

With the very accurate electronics used in today’s detectors, photon
counting noise(1) is the most significant source of uncertainty in most
datasets.  To understand this noise (error in counting), we need to take
a closer look at how a distribution produced by counting can be modeled
as a parametric function.

   Counting is an inherently discrete operation, which can only produce
positive (including zero) integer outputs.  For example we can’t count
$3.2$ or $-2$ of anything.  We only count $0$, $1$, $2$, $3$ and so on.
The distribution of values, as a result of counting efforts is formally
known as the Poisson distribution
(https://en.wikipedia.org/wiki/Poisson_distribution).  It is associated
to Siméon Denis Poisson, because he discussed it while working on the
number of wrongful convictions in court cases in his 1837 book(2).

   Let’s take $\lambda$ to represent the expected mean count of
something.  Furthermore, let’s take $k$ to represent the result of one
particular counting attempt.  The probability density function of
getting $k$ counts (in each attempt, given the expected/mean count of
$\lambda$) can be written as:

$$f(k)={\lambda^k \over k!} e^{-\lambda},\quad k\in {0, 1, 2, 3, \dots }$$

   Because the Poisson distribution is only applicable to positive
values (note the factorial operator, which only applies to non-negative
integers), naturally it is very skewed when $\lambda$ is near zero.  One
qualitative way to understand this behavior is that there simply aren’t
enough integers smaller than $\lambda$, than integers that are larger
than it.  Therefore to accommodate all possibilities/counts, it has to
be strongly skewed when $\lambda$ is small.

   As $\lambda$ becomes larger, the distribution becomes more and more
symmetric.  A very useful property of the Poisson distribution is that
the mean value is also its variance.  When $\lambda$ is very large, say
$\lambda>1000$, then the Normal (Gaussian) distribution
(https://en.wikipedia.org/wiki/Normal_distribution), is an excellent
approximation of the Poisson distribution with mean $\mu=\lambda$ and
standard deviation $\sigma=\sqrt{\lambda}$.  In other words, a Poisson
distribution (with a sufficiently large $\lambda$) is simply a Gaussian
that only has one free parameter ($\mu=\lambda$ and
$\sigma=\sqrt{\lambda}$), instead of the two parameters (independent
$\mu$ and $\sigma$) that it originally has.

   In real situations, the photons/flux from our targets are added to a
certain background flux (observationally, the _Sky_ value).  The Sky
value is defined to be the average flux of a region in the dataset with
no targets.  Its physical origin can be the brightness of the atmosphere
(for ground-based instruments), possible stray light within the imaging
instrument, the average flux of undetected targets, etc.  The Sky value
is thus an ideal definition, because in real datasets, what lies deep in
the noise (far lower than the detection limit) is never known(3).  To
account for all of these, the sky value is defined to be the average
count/value of the undetected regions in the image.  In a mock
image/dataset, we have the luxury of setting the background (Sky) value.

   In each element of the dataset (pixel in an image), the flux is the
sum of contributions from various sources (after convolution by the PSF,
see *note PSF::).  Let’s name the convolved sum of possibly overlapping
objects, $I_{nn}$.  $nn$ representing ‘no noise’.  For now, let’s assume
the background ($B$) is constant and sufficiently high for the Poisson
distribution to be approximated by a Gaussian.  Then the flux after
adding noise is a random value taken from a Gaussian distribution with
the following mean ($\mu$) and standard deviation ($\sigma$):

            $$\mu=B+I_{nn}, \quad \sigma=\sqrt{B+I_{nn}}$$

   Since this type of noise is inherent in the objects we study, it is
usually measured on the same scale as the astronomical objects, namely
the magnitude system, see *note Flux Brightness and magnitude::.  It is
then internally converted to the flux scale for further processing.

   ---------- Footnotes ----------

   (1) In practice, we are actually counting the electrons that are
produced by each photon, not the actual photons.

   (2) [From Wikipedia] Poisson’s result was also derived in a previous
study by Abraham de Moivre in 1711.  Therefore some people suggest it
should rightly be called the de Moivre distribution.

   (3) In a real image, a relatively large number of very faint objects
can been fully buried in the noise and never detected.  These undetected
objects will bias the background measurement to slightly larger values.
Our best approximation is thus to simply assume they are uniform, and
consider their average effect.  See Figure 1 (a.1 and a.2) and Section
2.2 in Akhlaghi and Ichikawa [2015] (https://arxiv.org/abs/1505.01664).


File: gnuastro.info,  Node: Instrumental noise,  Next: Final noised pixel value,  Prev: Photon counting noise,  Up: Noise basics

8.2.1.2 Instrumental noise
..........................

While taking images with a camera, a dark current is fed to the pixels,
the variation of the value of this dark current over the pixels, also
adds to the final image noise.  Another source of noise is the readout
noise that is produced by the electronics in the detector.
Specifically, the parts that attempt to digitize the voltage produced by
the photo-electrons in the analog to digital converter.  With the
current generation of instruments, this source of noise is not as
significant as the noise due to the background Sky discussed in *note
Photon counting noise::.

   Let $C$ represent the combined standard deviation of all these
instrumental sources of noise.  When only this source of noise is
present, the noised pixel value would be a random value chosen from a
Gaussian distribution with

            $$\mu=I_{nn}, \quad \sigma=\sqrt{C^2+I_{nn}}$$

   This type of noise is independent of the signal in the dataset, it is
only determined by the instrument.  So the flux scale (and not magnitude
scale) is most commonly used for this type of noise.  In practice, this
value is usually reported in analog-to-digital units or ADUs, not flux
or electron counts.  The gain value of the device can be used to convert
between these two, see *note Flux Brightness and magnitude::.


File: gnuastro.info,  Node: Final noised pixel value,  Next: Generating random numbers,  Prev: Instrumental noise,  Up: Noise basics

8.2.1.3 Final noised pixel value
................................

Based on the discussions in *note Photon counting noise:: and *note
Instrumental noise::, depending on the values you specify for $B$ and
$C$ from the above, the final noised value for each pixel is a random
value chosen from a Gaussian distribution with

          $$\mu=B+I_{nn}, \quad \sigma=\sqrt{C^2+B+I_{nn}}$$


File: gnuastro.info,  Node: Generating random numbers,  Prev: Final noised pixel value,  Up: Noise basics

8.2.1.4 Generating random numbers
.................................

As discussed above, to generate noise we need to make random samples of
a particular distribution.  So it is important to understand some
general concepts regarding the generation of random numbers.  For a very
complete and nice introduction we strongly advise reading Donald Knuth’s
“The art of computer programming”, volume 2, chapter 3(1).  Quoting from
the GNU Scientific Library manual, “If you don’t own it, you should stop
reading right now, run to the nearest bookstore, and buy it”(2)!

   Using only software, we can only produce what is called a
psuedo-random sequence of numbers.  A true random number generator is a
hardware (let’s assume we have made sure it has no systematic biases),
for example throwing dice or flipping coins (which have remained from
the ancient times).  More modern hardware methods use atmospheric noise,
thermal noise or other types of external electromagnetic or quantum
phenomena.  All pseudo-random number generators (software) require a
seed to be the basis of the generation.  The advantage of having a seed
is that if you specify the same seed for multiple runs, you will get an
identical sequence of random numbers which allows you to reproduce the
same final noised image.

   The programs in GNU Astronomy Utilities (for example MakeNoise or
MakeProfiles) use the GNU Scientific Library (GSL) to generate random
numbers.  GSL allows the user to set the random number generator through
environment variables, see *note Installation directory:: for an
introduction to environment variables.  In the chapter titled “Random
Number Generation” they have fully explained the various random number
generators that are available (there are a lot of them!).  Through the
two environment variables ‘GSL_RNG_TYPE’ and ‘GSL_RNG_SEED’ you can
specify the generator and its seed respectively.

   If you don’t specify a value for ‘GSL_RNG_TYPE’, GSL will use its
default random number generator type.  The default type is sufficient
for most general applications.  If no value is given for the
‘GSL_RNG_SEED’ environment variable and you have asked Gnuastro to read
the seed from the environment (through the ‘--envseed’ option), then GSL
will use the default value of each generator to give identical outputs.
If you don’t explicitly tell Gnuastro programs to read the seed value
from the environment variable, then they will use the system time
(accurate to within a microsecond) to generate (apparently random)
seeds.  In this manner, every time you run the program, you will get a
different random number distribution.

   There are two ways you can specify values for these environment
variables.  You can call them on the same command-line for example:

     $ GSL_RNG_TYPE="taus" GSL_RNG_SEED=345 astmknoise input.fits

In this manner the values will only be used for this particular
execution of MakeNoise.  Alternatively, you can define them for the full
period of your terminal session or script length, using the shell’s
‘export’ command with the two separate commands below (for a script
remove the ‘$’ signs):

     $ export GSL_RNG_TYPE="taus"
     $ export GSL_RNG_SEED=345

The subsequent programs which use GSL’s random number generators will
hence forth use these values in this session of the terminal you are
running or while executing this script.  In case you want to set fixed
values for these parameters every time you use the GSL random number
generator, you can add these two lines to your ‘.bashrc’ startup
script(3), see *note Installation directory::.

*NOTE:* If the two environment variables ‘GSL_RNG_TYPE’ and
‘GSL_RNG_SEED’ are defined, GSL will report them by default, even if you
don’t use the ‘--envseed’ option.  For example you can see the top few
lines of the output of MakeProfiles:

     $ export GSL_RNG_TYPE="taus"
     $ export GSL_RNG_SEED=345
     $ astmkprof -s1 --kernel=gaussian,2,5 --envseed
     GSL_RNG_TYPE=taus
     GSL_RNG_SEED=345
     MakeProfiles A.B started on DDD MMM DD HH:MM:SS YYYY
       - Building one gaussian kernel
       - Random number generator (RNG) type: ranlxs1
       - RNG seed for all profiles: 345
       ---- ./kernel.fits created.
     MakeProfiles finished in 0.111271 seconds

The first two output lines (showing the names of the environment
variables) are printed by GSL before MakeProfiles actually starts
generating random numbers.  The Gnuastro programs will report the values
they use independently, you should check them for the final values used.
For example if ‘--envseed’ is not given, ‘GSL_RNG_SEED’ will not be used
and the last line shown above will not be printed.  In the case of
MakeProfiles, each profile will get its own seed value.

   ---------- Footnotes ----------

   (1) Knuth, Donald.  1998.  The art of computer programming.
Addison–Wesley.  ISBN 0-201-89684-2

   (2) For students, running to the library might be more affordable!

   (3) Don’t forget that if you are going to give your scripts (that use
the GSL random number generator) to others you have to make sure you
also tell them to set these environment variable separately.  So for
scripts, it is best to keep all such variable definitions within the
script, even if they are within your ‘.bashrc’.


File: gnuastro.info,  Node: Invoking astmknoise,  Prev: Noise basics,  Up: MakeNoise

8.2.2 Invoking MakeNoise
------------------------

MakeNoise will add noise to an existing image.  The executable name is
‘astmknoise’ with the following general template

     $ astmknoise [OPTION ...] InputImage.fits

One line examples:

     ## Add noise with a standard deviation of 100 to image:
     $ astmknoise --sigma=100 image.fits

     ## Add noise to input image assuming a background magnitude (with
     ## zeropoint magnitude of 0) and a certain instrumental noise:
     $ astmknoise --background=-10 -z0 --instrumental=20 mockimage.fits

If actual processing is to be done, the input image is a mandatory
argument.  The full list of options common to all the programs in
Gnuastro can be seen in *note Common options::.  The type (see *note
Numeric data types::) of the output can be specified with the ‘--type’
option, see *note Input output options::.  The header of the output FITS
file keeps all the parameters that were influential in making it.  This
is done for future reproducibility.

‘-s FLT’
‘--sigma=FLT’
     The total noise sigma in the same units as the pixel values.  With
     this option, the ‘--background’, ‘--zeropoint’ and ‘--instrumental’
     will be ignored.  With this option, the noise will be independent
     of the pixel values (which is not realistic, see *note Photon
     counting noise::).  Hence it is only useful if you are working on
     low surface brightness regions where the change in pixel value (and
     thus real noise) is insignificant.

‘-b FLT’
‘--background=FLT’
     The background pixel value for the image in units of magnitudes,
     see *note Photon counting noise:: and *note Flux Brightness and
     magnitude::.

‘-z FLT’
‘--zeropoint=FLT’
     The zeropoint magnitude used to convert the value of ‘--background’
     (in units of magnitude) to flux, see *note Flux Brightness and
     magnitude::.

‘-i FLT’
‘--instrumental=FLT’
     The instrumental noise which is in units of flux, see *note
     Instrumental noise::.

‘-e’
‘--envseed’
     Use the ‘GSL_RNG_SEED’ environment variable for the seed used in
     the random number generator, see *note Generating random numbers::.
     With this option, the output image noise is always going to be
     identical (or reproducible).

‘-d’
‘--doubletype’
     Save the output in the double precision floating point format that
     was used internally.  This option will be most useful if the input
     images were of integer types.


File: gnuastro.info,  Node: High-level calculations,  Next: Library,  Prev: Modeling and fittings,  Up: Top

9 High-level calculations
*************************

After the reduction of raw data (for example with the programs in *note
Data manipulation::) you will have reduced images/data ready for
processing/analyzing (for example with the programs in *note Data
analysis::).  But the processed/analyzed data (or catalogs) are still
not enough to derive any scientific result.  Even higher-level analysis
is still needed to convert the observed magnitudes, sizes or volumes
into physical quantities that we associate with each catalog entry or
detected object which is the purpose of the tools in this section.

* Menu:

* CosmicCalculator::            Calculate cosmological variables


File: gnuastro.info,  Node: CosmicCalculator,  Prev: High-level calculations,  Up: High-level calculations

9.1 CosmicCalculator
====================

To derive higher-level information regarding our sources in
extra-galactic astronomy, cosmological calculations are necessary.  In
Gnuastro, CosmicCalculator is in charge of such calculations.  Before
discussing how CosmicCalculator is called and operates (in *note
Invoking astcosmiccal::), it is important to provide a rough but mostly
self sufficient review of the basics and the equations used in the
analysis.  In *note Distance on a 2D curved space:: the basic idea of
understanding distances in a curved and expanding 2D universe (which we
can visualize) are reviewed.  Having solidified the concepts there, in
*note Extending distance concepts to 3D::, the formalism is extended to
the 3D universe we are trying to study in our research.

   The focus here is obtaining a physical insight into these equations
(mainly for the use in real observational studies).  There are many
books thoroughly deriving and proving all the equations with all
possible initial conditions and assumptions for any abstract universe,
interested readers can study those books.

* Menu:

* Distance on a 2D curved space::  Distances in 2D for simplicity
* Extending distance concepts to 3D::  Going to 3D (our real universe).
* Invoking astcosmiccal::       How to run CosmicCalculator


File: gnuastro.info,  Node: Distance on a 2D curved space,  Next: Extending distance concepts to 3D,  Prev: CosmicCalculator,  Up: CosmicCalculator

9.1.1 Distance on a 2D curved space
-----------------------------------

The observations to date (for example the Planck 2015 results), have not
measured(1) the presence of significant curvature in the universe.
However to be generic (and allow its measurement if it does in fact
exist), it is very important to create a framework that allows non-zero
uniform curvature.  However, this section is not intended to be a fully
thorough and mathematically complete derivation of these concepts.
There are many references available for such reviews that go deep into
the abstract mathematical proofs.  The emphasis here is on visualization
of the concepts for a beginner.

   As 3D beings, it is difficult for us to mentally create (visualize) a
picture of the curvature of a 3D volume.  Hence, here we will assume a
2D surface/space and discuss distances on that 2D surface when it is
flat and when it is curved.  Once the concepts have been
created/visualized here, we will extend them, in *note Extending
distance concepts to 3D::, to a real 3D spatial _slice_ of the Universe
we live in and hope to study.

   To be more understandable (actively discuss from an observer’s point
of view) let’s assume there’s an imaginary 2D creature living on the 2D
space (which _might_ be curved in 3D). Here, we will be working with
this creature in its efforts to analyze distances in its 2D universe.
The start of the analysis might seem too mundane, but since it is
difficult to imagine a 3D curved space, it is important to review all
the very basic concepts thoroughly for an easy transition to a universe
that is more difficult to visualize (a curved 3D space embedded in 4D).

   To start, let’s assume a static (not expanding or shrinking), flat 2D
surface similar to *note Figure 9.1: flatplane. and that the 2D creature
is observing its universe from point $A$.  One of the most basic ways to
parameterize this space is through the Cartesian coordinates ($x$, $y$).
In *note Figure 9.1: flatplane, the basic axes of these two coordinates
are plotted.  An infinitesimal change in the direction of each axis is
written as $dx$ and $dy$.  For each point, the infinitesimal changes are
parallel with the respective axes and are not shown for clarity.
Another very useful way of parameterizing this space is through polar
coordinates.  For each point, we define a radius ($r$) and angle
($\phi$) from a fixed (but arbitrary) reference axis.  In *note Figure
9.1: flatplane. the infinitesimal changes for each polar coordinate are
plotted for a random point and a dashed circle is shown for all points
with the same radius.

 [image src="gnuastro-figures/flatplane.png" text="../gnuastro-figures//flatplane.eps" ]

Figure 9.1: Two dimensional Cartesian and polar coordinates on a flat
plane.

   Assuming an object is placed at a certain position, which can be
parameterized as $(x,y)$, or $(r,\phi)$, a general infinitesimal change
in its position will place it in the coordinates $(x+dx,y+dy)$ and
$(r+dr,\phi+d\phi)$.  The distance (on the flat 2D surface) that is
covered by this infinitesimal change in the static universe ($ds_s$, the
subscript signifies the static nature of this universe) can be written
as:

                  $$ds_s=dx^2+dy^2=dr^2+r^2d\phi^2$$

   The main question is this: how can the 2D creature incorporate the
(possible) curvature in its universe when it’s calculating distances?
The universe that it lives in might equally be a curved surface like
*note Figure 9.2: sphereandplane.  The answer to this question but for a
3D being (us) is the whole purpose to this discussion.  Here, we want to
give the 2D creature (and later, ourselves) the tools to measure
distances if the space (that hosts the objects) is curved.

   *note Figure 9.2: sphereandplane. assumes a spherical shell with
radius $R$ as the curved 2D plane for simplicity.  The 2D plane is
tangent to the spherical shell and only touches it at $A$.  This idea
will be generalized later.  The first step in measuring the distance in
a curved space is to imagine a third dimension along the $z$ axis as
shown in *note Figure 9.2: sphereandplane.  For simplicity, the $z$ axis
is assumed to pass through the center of the spherical shell.  Our
imaginary 2D creature cannot visualize the third dimension or a curved
2D surface within it, so the remainder of this discussion is purely
abstract for it (similar to us having difficulty in visualizing a 3D
curved space in 4D). But since we are 3D creatures, we have the
advantage of visualizing the following steps.  Fortunately the 2D
creature is already familiar with our mathematical constructs, so it can
follow our reasoning.

   With the third axis added, a generic infinitesimal change over _the
full_ 3D space corresponds to the distance:

            $$ds_s^2=dx^2+dy^2+dz^2=dr^2+r^2d\phi^2+dz^2.$$

 [image src="gnuastro-figures/sphereandplane.png" text="../gnuastro-figures//sphereandplane.eps" ]

Figure 9.2: 2D spherical shell (centered on $O$) and flat plane (light
gray) tangent to it at point $A$.

   It is very important to recognize that this change of distance is for
_any_ point in the 3D space, not just those changes that occur on the 2D
spherical shell of *note Figure 9.2: sphereandplane.  Recall that our 2D
friend can only do measurements on the 2D surfaces, not the full 3D
space.  So we have to constrain this general change to any change on the
2D spherical shell.  To do that, let’s look at the arbitrary point $P$
on the 2D spherical shell.  Its image ($P'$) on the flat plain is also
displayed.  From the dark gray triangle, we see that

$$\sin\theta={r\over R},\quad\cos\theta={R-z\over R}.$$These relations allow the 2D creature to find the value of $z$ (an abstract dimension for it) as a function of r (distance on a flat 2D plane, which it can visualize) and thus eliminate $z$.
   From $\sin^2\theta+\cos^2\theta=1$, we get $z^2-2Rz+r^2=0$ and
solving for $z$, we find:

           $$z=R\left(1\pm\sqrt{1-{r^2\over R^2}}\right).$$

   The $\pm$ can be understood from *note Figure 9.2: sphereandplane.:
For each $r$, there are two points on the sphere, one in the upper
hemisphere and one in the lower hemisphere.  An infinitesimal change in
$r$, will create the following infinitesimal change in $z$:

                    $$dz={\mp r\over R}\left(1\over
\sqrt{1-{r^2/R^2}}\right)dr.$$Using the positive signed equation instead of $dz$ in the $ds_s^2$ equation above, we get:

             $$ds_s^2={dr^2\over 1-r^2/R^2}+r^2d\phi^2.$$

   The derivation above was done for a spherical shell of radius $R$ as
a curved 2D surface.  To generalize it to any surface, we can define
$K=1/R^2$ as the curvature parameter.  Then the general infinitesimal
change in a static universe can be written as:

               $$ds_s^2={dr^2\over 1-Kr^2}+r^2d\phi^2.$$

   Therefore, when $K>0$ (and curvature is the same everywhere), we have
a finite universe, where $r$ cannot become larger than $R$ as in *note
Figure 9.2: sphereandplane.  When $K=0$, we have a flat plane (*note
Figure 9.1: flatplane.) and a negative $K$ will correspond to an
imaginary $R$.  The latter two cases may be infinite in area (which is
not a simple concept, but mathematically can be modeled with $r$
extending infinitely), or finite-area (like a cylinder is flat
everywhere with $ds_s^2={dx^2 + dy^2}$, but finite in one direction in
size).

   A very important issue that can be discussed now (while we are still
in 2D and can actually visualize things) is that $\overrightarrow{r}$ is
tangent to the curved space at the observer’s position.  In other words,
it is on the gray flat surface of *note Figure 9.2: sphereandplane, even
when the universe if curved: $\overrightarrow{r}=P'-A$.  Therefore for
the point $P$ on a curved space, the raw coordinate $r$ is the distance
to $P'$, not $P$.  The distance to the point $P$ (at a specific
coordinate $r$ on the flat plane) over the curved surface (thick line in
*note Figure 9.2: sphereandplane.) is called the _proper distance_ and
is displayed with $l$.  For the specific example of *note Figure 9.2:
sphereandplane, the proper distance can be calculated with: $l=R\theta$
($\theta$ is in radians).  Using the $\sin\theta$ relation found above,
we can find $l$ as a function of $r$:

    $$\theta=\sin^{-1}\left({r\over R}\right)\quad\rightarrow\quad
               l(r)=R\sin^{-1}\left({r\over R}\right)$$

   $R$ is just an arbitrary constant and can be directly found from $K$,
so for cleaner equations, it is common practice to set $R=1$, which
gives: $l(r)=\sin^{-1}r$.  Also note that when $R=1$, then $l=\theta$.
Generally, depending on the curvature, in a _static_ universe the proper
distance can be written as a function of the coordinate $r$ as (from now
on we are assuming $R=1$):

               $$l(r)=\sin^{-1}(r)\quad(K>0),\quad\quad
    l(r)=r\quad(K=0),\quad\quad l(r)=\sinh^{-1}(r)\quad(K<0).$$With
   $l$, the infinitesimal change of distance can be written in a more
simpler and abstract form of

                      $$ds_s^2=dl^2+r^2d\phi^2.$$

   Until now, we had assumed a static universe (not changing with time).
But our observations so far appear to indicate that the universe is
expanding (it isn’t static).  Since there is no reason to expect the
observed expansion is unique to our particular position of the universe,
we expect the universe to be expanding at all points with the same rate
at the same time.  Therefore, to add a time dependence to our distance
measurements, we can include a multiplicative scaling factor, which is a
function of time: $a(t)$.  The functional form of $a(t)$ comes from the
cosmology, the physics we assume for it: general relativity, and the
choice of whether the universe is uniform (‘homogeneous’) in density and
curvature or inhomogeneous.  In this section, the functional form of
$a(t)$ is irrelevant, so we can avoid these issues.

   With this scaling factor, the proper distance will also depend on
time.  As the universe expands, the distance between two given points
will shift to larger values.  We thus define a distance measure, or
coordinate, that is independent of time and thus doesn’t ‘move’.  We
call it the _comoving distance_ and display with $\chi$ such that:
$l(r,t)=\chi(r)a(t)$.  We have therefore, shifted the $r$ dependence of
the proper distance we derived above for a static universe to the
comoving distance:

              $$\chi(r)=\sin^{-1}(r)\quad(K>0),\quad\quad
   \chi(r)=r\quad(K=0),\quad\quad \chi(r)=\sinh^{-1}(r)\quad(K<0).$$

   Therefore, $\chi(r)$ is the proper distance to an object at a
specific reference time: $t=t_r$ (the $r$ subscript signifies
“reference”) when $a(t_r)=1$.  At any arbitrary moment ($t\neq{t_r}$)
before or after $t_r$, the proper distance to the object can be scaled
with $a(t)$.

   Measuring the change of distance in a time-dependent (expanding)
universe only makes sense if we can add up space and time(2).  But we
can only add bits of space and time together if we measure them in the
same units: with a conversion constant (similar to how 1000 is used to
convert a kilometer into meters).  Experimentally, we find strong
support for the hypothesis that this conversion constant is the speed of
light (or gravitational waves(3)) in a vacuum.  This speed is postulated
to be constant(4) and is almost always written as $c$.  We can thus
parameterize the change in distance on an expanding 2D surface as

  $$ds^2=c^2dt^2-a^2(t)ds_s^2 = c^2dt^2-a^2(t)(d\chi^2+r^2d\phi^2).$$

   ---------- Footnotes ----------

   (1) The observations are interpreted under the assumption of uniform
curvature.  For a relativistic alternative to dark energy (and maybe
also some part of dark matter), non-uniform curvature may be even be
more critical, but that is beyond the scope of this brief explanation.

   (2) In other words, making our space-time consistent with Minkowski
space-time geometry.  In this geometry, different observers at a given
point (event) in space-time split up space-time into ‘space’ and ‘time’
in different ways, just like people at the same spatial position can
make different choices of splitting up a map into ‘left–right’ and
‘up–down’.  This model is well supported by twentieth and twenty-first
century observations.

   (3) The speed of gravitational waves was recently found to be very
similar to that of light in vacuum, see arXiv:1710.05834
(https://arxiv.org/abs/1710.05834).

   (4) In _natural units_, speed is measured in units of the speed of
light in vacuum.


File: gnuastro.info,  Node: Extending distance concepts to 3D,  Next: Invoking astcosmiccal,  Prev: Distance on a 2D curved space,  Up: CosmicCalculator

9.1.2 Extending distance concepts to 3D
---------------------------------------

The concepts of *note Distance on a 2D curved space:: are here extended
to a 3D space that _might_ be curved.  We can start with the generic
infinitesimal distance in a static 3D universe, but this time in
spherical coordinates instead of polar coordinates.  $\theta$ is shown
in *note Figure 9.2: sphereandplane, but here we are 3D beings,
positioned on $O$ (the center of the sphere) and the point $O$ is
tangent to a 4D-sphere.  In our 3D space, a generic infinitesimal
displacement will correspond to the following distance in spherical
coordinates:

 $$ds_s^2=dx^2+dy^2+dz^2=dr^2+r^2(d\theta^2+\sin^2{\theta}d\phi^2).$$

   Like the 2D creature before, we now have to assume an abstract
dimension which we cannot visualize easily.  Let’s call the fourth
dimension $w$, then the general change in coordinates in the _full_ four
dimensional space will be:

      $$ds_s^2=dr^2+r^2(d\theta^2+\sin^2{\theta}d\phi^2)+dw^2.$$

But we can only work on a 3D curved space, so following exactly the same
steps and conventions as our 2D friend, we arrive at:

  $$ds_s^2={dr^2\over 1-Kr^2}+r^2(d\theta^2+\sin^2{\theta}d\phi^2).$$

In a non-static universe (with a scale factor a(t)), the distance can be
written as:

$$ds^2=c^2dt^2-a^2(t)[d\chi^2+r^2(d\theta^2+\sin^2{\theta}d\phi^2)].$$


File: gnuastro.info,  Node: Invoking astcosmiccal,  Prev: Extending distance concepts to 3D,  Up: CosmicCalculator

9.1.3 Invoking CosmicCalculator
-------------------------------

CosmicCalculator will calculate cosmological variables based on the
input parameters.  The executable name is ‘astcosmiccal’ with the
following general template

     $ astcosmiccal [OPTION...] ...

One line examples:

     ## Print basic cosmological properties at redshift 2.5:
     $ astcosmiccal -z2.5

     ## Only print Comoving volume over 4pi stradian to z (Mpc^3):
     $ astcosmiccal --redshift=0.8 --volume

     ## Print redshift and age of universe when Lyman-alpha line is
     ## at 6000 angstrom (another way to specify redshift).
     $ astcosmiccal --obsline=lyalpha,6000 --age

     ## Print luminosity distance, angular diameter distance and age
     ## of universe in one row at redshift 0.4
     $ astcosmiccal -z0.4 -LAg

     ## Assume Lambda and matter density of 0.7 and 0.3 and print
     ## basic cosmological parameters for redshift 2.1:
     $ astcosmiccal -l0.7 -m0.3 -z2.1

   The input parameters (for example current matter density, etc) can be
given as command-line options or in the configuration files, see *note
Configuration files::.  For a definition of the different parameters,
please see the sections prior to this.  If no redshift is given,
CosmicCalculator will just print its input parameters and abort.  For a
full list of the input options, please see *note CosmicCalculator input
options::.

   When only a redshift is given, CosmicCalculator will print all
calculations (one per line) with some explanations before each.  This
can be good when you want a general feeling of the conditions at a
specific redshift.  Alternatively, if any specific calculations are
requested, only the requested values will be calculated and printed with
one character space between them.  In this case, no description will be
printed.  See *note CosmicCalculator specific calculations:: for the
full list of these options along with some explanations how when/how
they can be useful.

* Menu:

* CosmicCalculator input options::  Options to specify input conditions.
* CosmicCalculator specific calculations::  Requesting specific outputs.


File: gnuastro.info,  Node: CosmicCalculator input options,  Next: CosmicCalculator specific calculations,  Prev: Invoking astcosmiccal,  Up: Invoking astcosmiccal

9.1.3.1 CosmicCalculator input options
......................................

The inputs to CosmicCalculator can be specified with the following
options:

‘-z FLT’
‘--redshift=FLT’
     The redshift of interest.  This cannot be called with ‘--obsline’.

‘-H FLT’
‘--H0=FLT’
     Current expansion rate (in km sec$^{-1}$ Mpc$^{-1}$).

‘-l FLT’
‘--olambda=FLT’
     Cosmological constant density divided by the critical density in
     the current Universe ($\Omega_{\Lambda,0}$).

‘-m FLT’
‘--omatter=FLT’
     Matter (including massive neutrinos) density divided by the
     critical density in the current Universe ($\Omega_{m,0}$).

‘-r FLT’
‘--oradiation=FLT’
     Radiation density divided by the critical density in the current
     Universe ($\Omega_{r,0}$).

‘--listlines’
     List the pre-defined spectral line wavelengths and names on
     standard output, then abort CosmicCalculator.  When this option is
     given, other operations on the command-line will be ignored.  This
     is convenient when you forget the specific name of the spectral
     line used within Gnuastro.

     These names can be used with the options that deal with spectral
     lines, for example ‘--obsline’ (below) and ‘--lineatz’ (*note
     CosmicCalculator specific calculations::).

     The format of the output list is a two-column table, with
     Gnuastro’s text table format (see *note Gnuastro text table
     format::).  Therefore, if you are only looking for lines in a
     specific range, you can pipe the output into Gnuastro’s table
     program and use its ‘--range’ option on the ‘wavelength’ (first)
     column.  For example, if you only want to see the lines between
     4000 and 6000 Angstroms, you can run this command:

          $ astcosmiccal --listlines \
                         | asttable --range=wavelength,4000,6000

     And if you want to use the list later and have it as a table in a
     file, you can easily add the ‘--output’ (or ‘-o’) option to the
     ‘asttable’ command, and specify the filename, for example
     ‘--output=lines.fits’ or ‘--output=lines.txt’.

‘-O STR/FLT,FLT’
‘--obsline=STR/FLT,FLT’
     Find the redshift to use in next steps based on the rest-frame and
     observed wavelengths of a line.  Wavelengths are assumed to be in
     Angstroms.  The first argument identifies the line.  It can be one
     of the standard names below, or any rest-frame wavelength in
     Angstroms.  The second argument is the observed wavelength of that
     line.  For example ‘--obsline=lyalpha,6000’ is the same as
     ‘--obsline=1215.64,6000’.

     The pre-defined names are listed below, sorted from red (longer
     wavelength) to blue (shorter wavelength).  You can get this list on
     the command-line with the ‘--listlines’.

     ‘siired’
          [6731Å] SII doublet’s redder line.

     ‘sii’
          [6724Å] SII doublet’s mean center at .

     ‘siiblue’
          [6717Å] SII doublet’s bluer line.

     ‘niired’
          [6584Å] NII doublet’s redder line.

     ‘nii’
          [6566Å] NII doublet’s mean center.

     ‘halpha’
          [6562.8Å] H-$\alpha$ line.

     ‘niiblue’
          [6548Å] NII doublet’s bluer line.

     ‘oiiired’
          [5007Å] OIII doublet’s redder line.

     ‘oiii’
          [4983Å] OIII doublet’s mean center.

     ‘oiiiblue’
          [4959Å] OIII doublet’s bluer line.

     ‘hbeta’
          [4861.36Å] H-$\beta$ line.

     ‘heiired’
          [4686Å] HeII doublet’s redder line.

     ‘hgamma’
          [4340.46Å] H-$\gamma$ line.

     ‘hdelta’
          [4101.74Å] H-$\delta$ line.

     ‘hepsilon’
          [3970.07Å] H-$\epsilon$ line.

     ‘neiii’
          [3869Å] NEIII line.

     ‘oiired’
          [3729Å] OII doublet’s redder line.

     ‘oii’
          [3727.5Å] OII doublet’s mean center.

     ‘oiiblue’
          [3726Å] OII doublet’s bluer line.

     ‘blimit’
          [3646Å] Balmer limit.

     ‘mgiired’
          [2803Å] MgII doublet’s redder line.

     ‘mgii’
          [2799.5Å] MgII doublet’s mean center.

     ‘mgiiblue’
          [2796Å] MgII doublet’s bluer line.

     ‘ciiired’
          [1909Å] CIII doublet’s redder line.

     ‘ciii’
          [1908Å] CIII doublet’s mean center.

     ‘ciiiblue’
          [1907Å] CIII doublet’s bluer line.

     ‘heiiblue’
          [1640Å] HeII doublet’s bluer line.

     ‘lyalpha’
          [1215.67Å] Lyman-$\alpha$ line.

     ‘lylimit’
          [912Å] Lyman limit.


File: gnuastro.info,  Node: CosmicCalculator specific calculations,  Prev: CosmicCalculator input options,  Up: Invoking astcosmiccal

9.1.3.2 CosmicCalculator specific calculations
..............................................

By default, when no specific calculations are requested,
CosmicCalculator will print a complete set of all its calculators (one
line for each calculation, see *note Invoking astcosmiccal::).  The full
list of calculations can be useful when you don’t want any specific
value, but just a general view.  In other contexts (for example in a
batch script or during a discussion), you know exactly what you want and
don’t want to be distracted by all the extra information.

   You can use any number of the options described below in any order.
When any of these options are requested, CosmicCalculator’s output will
just be a single line with a single space between the (possibly)
multiple values.  In the example below, only the tangential distance
along one arcsecond (in kpc), absolute magnitude conversion, and age of
the universe at redshift 2 are printed (recall that you can merge short
options together, see *note Options::).

     $ astcosmiccal -z2 -sag
     8.585046 44.819248 3.289979

   Here is one example of using this feature in scripts: by adding the
following two lines in a script to keep/use the comoving volume with
varying redshifts:

     z=3.12
     vol=$(astcosmiccal --redshift=$z --volume)

In a script, this operation might be necessary for a large number of
objects (several of galaxies in a catalog for example).  So the fact
that all the other default calculations are ignored will also help you
get to your result faster.

   If you are indeed dealing with many (for example thousands) of
redshifts, using CosmicCalculator is not the best/fastest solution.
Because it has to go through all the configuration files and
preparations for each invocation.  To get the best efficiency (least
overhead), we recommend using Gnuastro’s cosmology library (see *note
Cosmology library::).  CosmicCalculator also calls the library functions
defined there for its calculations, so you get the same result with no
overhead.  Gnuastro also has libraries for easily reading tables into a
C program, see *note Table input output::.  Afterwards, you can easily
build and run your C program for the particular processing with *note
BuildProgram::.

   If you just want to inspect the value of a variable visually, the
description (which comes with units) might be more useful.  In such
cases, the following command might be better.  The other calculations
will also be done, but they are so fast that you will not notice on
modern computers (the time it takes your eye to focus on the result is
usually longer than the processing: a fraction of a second).

     $ astcosmiccal --redshift=0.832 | grep volume

   The full list of CosmicCalculator’s specific calculations is present
below.  In case you have forgot the units, you can use the ‘--help’
option which has the units along with a short description.

‘-e’
‘--usedredshift’
     The redshift that was used in this run.  In many cases this is the
     main input parameter to CosmicCalculator, but it is useful in
     others.  For example in combination with ‘--obsline’ (where you
     give an observed and rest-frame wavelength and would like to know
     the redshift), or if you want to run CosmicCalculator in a loop
     while changing the redshift and you want to keep the redshift
     value.

‘-G’
‘--agenow’
     The current age of the universe (given the input parameters) in Ga
     (Giga annum, or billion years).

‘-C’
‘--criticaldensitynow’
     The current critical density (given the input parameters) in grams
     per centimeter-cube ($g/cm^3$).

‘-d’
‘--properdistance’
     The proper distance (at current time) to object at the given
     redshift in Megaparsecs (Mpc).  See *note Distance on a 2D curved
     space:: for a description of the proper distance.

‘-A’
‘--angulardimdist’
     The angular diameter distance to object at given redshift in
     Megaparsecs (Mpc).

‘-s’
‘--arcsectandist’
     The tangential distance covered by 1 arcseconds at the given
     redshift in kiloparsecs (Kpc).  This can be useful when trying to
     estimate the resolution or pixel scale of an instrument (usually in
     units of arcseconds) at a given redshift.

‘-L’
‘--luminositydist’
     The luminosity distance to object at given redshift in Megaparsecs
     (Mpc).

‘-u’
‘--distancemodulus’
     The distance modulus at given redshift.

‘-a’
‘--absmagconv’
     The conversion factor (addition) to absolute magnitude.  Note that
     this is practically the distance modulus added with
     $-2.5\log{(1+z)}$ for the desired redshift based on the input
     parameters.  Once the apparent magnitude and redshift of an object
     is known, this value may be added with the apparent magnitude to
     give the object’s absolute magnitude.

‘-g’
‘--age’
     Age of the universe at given redshift in Ga (Giga annum, or billion
     years).

‘-b’
‘--lookbacktime’
     The look-back time to given redshift in Ga (Giga annum, or billion
     years).  The look-back time at a given redshift is defined as the
     current age of the universe (‘--agenow’) subtracted by the age of
     the universe at the given redshift.

‘-c’
‘--criticaldensity’
     The critical density at given redshift in grams per centimeter-cube
     ($g/cm^3$).

‘-v’
‘--onlyvolume’
     The comoving volume in Megaparsecs cube (Mpc$^3$) until the desired
     redshift based on the input parameters.

‘-i STR/FLT’
‘--lineatz=STR/FLT’
     The wavelength of the specified line at the redshift given to
     CosmicCalculator.  The line can be specified either by its name or
     directly as a number (its wavelength).  To get the list of
     pre-defined names for the lines and their wavelength, you can use
     the ‘--listlines’ option, see *note CosmicCalculator input
     options::.  In the former case (when a name is given), the returned
     number is in units of Angstroms.  In the latter (when a number is
     given), the returned value is the same units of the input number
     (assuming its a wavelength).


File: gnuastro.info,  Node: Library,  Next: Developing,  Prev: High-level calculations,  Up: Top

10 Library
**********

Each program in Gnuastro that was discussed in the prior chapters (or
any program in general) is a collection of functions that is compiled
into one executable file which can communicate directly with the outside
world.  The outside world in this context is the operating system.  By
communication, we mean that control is directly passed to a program from
the operating system with a (possible) set of inputs and after it is
finished, the program will pass control back to the operating system.
For programs written in C and C++, the unique ‘main’ function is in
charge of this communication.

   Similar to a program, a library is also a collection of functions
that is compiled into one executable file.  However, unlike programs,
libraries don’t have a ‘main’ function.  Therefore they can’t
communicate directly with the outside world.  This gives you the chance
to write your own ‘main’ function and call library functions from within
it.  After compiling your program into a binary executable, you just
have to _link_ it to the library and you are ready to run (execute) your
program.  In this way, you can use Gnuastro at a much lower-level, and
in combination with other libraries on your system, you can
significantly boost your creativity.

   This chapter starts with a basic introduction to libraries and how
you can use them in *note Review of library fundamentals::.  The
separate functions in the Gnuastro library are then introduced
(classified by context) in *note Gnuastro library::.  If you end up
routinely using a fixed set of library functions, with a well-defined
input and output, it will be much more beneficial if you define a
program for the job.  Therefore, in its *note Version controlled
source::, Gnuastro comes with the *note The TEMPLATE program:: to easily
define your own programs(s).

* Menu:

* Review of library fundamentals::  Guide on libraries and linking.
* BuildProgram::                Link and run source files with this library.
* Gnuastro library::            Description of all library functions.
* Library demo programs::       Demonstration for using libraries.


File: gnuastro.info,  Node: Review of library fundamentals,  Next: BuildProgram,  Prev: Library,  Up: Library

10.1 Review of library fundamentals
===================================

Gnuastro’s libraries are written in the C programming language.  In
*note Why C::, we have thoroughly discussed the reasons behind this
choice.  C was actually created to write Unix, thus understanding the
way C works can greatly help in effectively using programs and libraries
in all Unix-like operating systems.  Therefore, in the following
subsections some important aspects of C, as it relates to libraries (and
thus programs that depend on them) on Unix are reviewed.  First we will
discuss header files in *note Headers:: and then go onto *note
Linking::.  This section finishes with *note Summary and example on
libraries::.  If you are already familiar with these concepts, please
skip this section and go directly to *note Gnuastro library::.

   In theory, a full operating system (or any software) can be written
as one function.  Such a software would not need any headers or linking
(that are discussed in the subsections below).  However, writing that
single function and maintaining it (adding new features, fixing bugs,
documentation, etc) would be a programmer or scientist’s worst
nightmare!  Furthermore, all the hard work that went into creating it
cannot be reused in other software: every other programmer or scientist
would have to re-invent the wheel.  The ultimate purpose behind
libraries (which come with headers and have to be linked) is to address
this problem and increase modularity: “the degree to which a system’s
components may be separated and recombined” (from Wikipedia).  The more
modular the source code of a program or library, the easier maintaining
it will be, and all the hard work that went into creating it can be
reused for a wider range of problems.

* Menu:

* Headers::                     Header files included in source.
* Linking::                     Linking the compiled source files into one.
* Summary and example on libraries::  A summary and example on using libraries.


File: gnuastro.info,  Node: Headers,  Next: Linking,  Prev: Review of library fundamentals,  Up: Review of library fundamentals

10.1.1 Headers
--------------

C source code is read from top to bottom in the source file, therefore
program components (for example variables, data structures and
functions) should all be _defined_ or _declared_ closer to the top of
the source file: before they are used.  _Defining_ something in C or C++
is jargon for providing its full details.  _Declaring_ it, on the
other-hand, is jargon for only providing the minimum information needed
for the compiler to pass it temporarily and fill in the detailed
definition later.

   For a function, the _declaration_ only contains the inputs and their
data-types along with the output’s type(1).  The _definition_ adds to
the declaration by including the exact details of what operations are
done to the inputs to generate the output.  As an example, take this
simple summation function:

     double
     sum(double a, double b)
     {
       return a + b;
     }
What you see above is the _definition_ of this function: it shows you
(and the compiler) exactly what it does to the two ‘double’ type inputs
and that the output also has a ‘double’ type.  Note that a function’s
internal operations are rarely so simple and short, it can be
arbitrarily long and complicated.  This unreasonably short and simple
function was chosen here for ease of reading.  The declaration for this
function is:

     double
     sum(double a, double b);

You can think of a function’s declaration as a building’s address in the
city, and the definition as the building’s complete blueprints.  When
the compiler confronts a call to a function during its processing, it
doesn’t need to know anything about how the inputs are processed to
generate the output.  Just as the postman doesn’t need to know the inner
structure of a building when delivering the mail.  The declaration
(address) is enough.  Therefore by _declaring_ the functions once at the
start of the source files, we don’t have to worry about _defining_ them
after they are used.

   Even for a simple real-world operation (not a simple summation like
above!), you will soon need many functions (for example, some for
reading/preparing the inputs, some for the processing, and some for
preparing the output).  Although it is technically possible, managing
all the necessary functions in one file is not easy and is contrary to
the modularity principle (see *note Review of library fundamentals::),
for example the functions for preparing the input can be usable in your
other projects with a different processing.  Therefore, as we will see
later (in *note Linking::), the functions don’t necessarily need to be
defined in the source file where they are used.  As long as their
definitions are ultimately linked to the final executable, everything
will be fine.  For now, it is just important to remember that the
functions that are called within one source file must be declared within
the source file (declarations are mandatory), but not necessarily
defined there.

   In the spirit of modularity, it is common to define contextually
similar functions in one source file.  For example, in Gnuastro,
functions that calculate the median, mean and other statistical
functions are defined in ‘lib/statistics.c’, while functions that deal
directly with FITS files are defined in ‘lib/fits.c’.

   Keeping the definition of similar functions in a separate file
greatly helps their management and modularity, but this fact alone
doesn’t make things much easier for the caller’s source code: recall
that while definitions are optional, declarations are mandatory.  So if
this was all, the caller would have to manually copy and paste
(_include_) all the declarations from the various source files into the
file they are working on now.  To address this problem, programmers have
adopted the header file convention: the header file of a source code
contains all the declarations that a caller would need to be able to use
any of its functions.  For example, in Gnuastro, ‘lib/statistics.c’
(file containing function definitions) comes with
‘lib/gnuastro/statistics.h’ (only containing function declarations).

   The discussion above was mainly focused on functions, however, there
are many more programming constructs such as pre-processor macros and
data structures.  Like functions, they also need to be known to the
compiler when it confronts a call to them.  So the header file also
contains their definitions or declarations when they are necessary for
the functions.

   Pre-processor macros (or macros for short) are replaced with their
defined value by the pre-processor before compilation.  Conventionally
they are written only in capital letters to be easily recognized.  It is
just important to understand that the compiler doesn’t see the macros,
it sees their fixed values.  So when a header specifies macros you can
do your programming without worrying about the actual values.  The
standard C types (for example ‘int’, or ‘float’) are very low-level and
basic.  We can collect multiple C types into a _structure_ for a
higher-level way to keep and pass-along data.  See *note Generic data
container:: for some examples of macros and data structures.

   The contents in the header need to be _include_d into the caller’s
source code with a special pre-processor command: ‘#include
<path/to/header.h>’.  As the name suggests, the _pre-processor_ goes
through the source code prior to the processor (or compiler).  One of
its jobs is to include, or merge, the contents of files that are
mentioned with this directive in the source code.  Therefore the
compiler sees a single entity containing the contents of the main file
and all the included files.  This allows you to include many (sometimes
thousands of) declarations into your code with only one line.  Since the
headers are also installed with the library into your system, you don’t
even need to keep a copy of them for each separate program, making
things even more convenient.

   Try opening some of the ‘.c’ files in Gnuastro’s ‘lib/’ directory
with a text editor to check out the include directives at the start of
the file (after the copyright notice).  Let’s take ‘lib/fits.c’ as an
example.  You will notice that Gnuastro’s header files (like
‘gnuastro/fits.h’) are indeed within this directory (the ‘fits.h’ file
is in the ‘gnuastro/’ directory).  You will notice that files like
‘stdio.h’, or ‘string.h’ are not in this directory (or anywhere within
Gnuastro).

   On most systems the basic C header files (like ‘stdio.h’ and
‘string.h’ mentioned above) are located in ‘/usr/include/’(2).  Your
compiler is configured to automatically search that directory (and
possibly others), so you don’t have to explicitly mention these
directories.  Go ahead, look into the ‘/usr/include’ directory and find
‘stdio.h’ for example.  When the necessary header files are not in those
specific libraries, the pre-processor can also search in places other
than the current directory.  You can specify those directories with this
pre-processor option(3):

‘-I DIR’
     “Add the directory ‘DIR’ to the list of directories to be searched
     for header files.  Directories named by ’-I’ are searched before
     the standard system include directories.  If the directory ‘DIR’ is
     a standard system include directory, the option is ignored to
     ensure that the default search order for system directories and the
     special treatment of system headers are not defeated...” (quoted
     from the GNU Compiler Collection manual).  Note that the space
     between <I> and the directory is optional and commonly not used.

   If the pre-processor can’t find the included files, it will abort
with an error.  In fact a common error when building programs that
depend on a library is that the compiler doesn’t not know where a
library’s header is (see *note Known issues::).  So you have to manually
tell the compiler where to look for the library’s headers with the ‘-I’
option.  For a small software with one or two source files, this can be
done manually (see *note Summary and example on libraries::).  However,
to enhance modularity, Gnuastro (and most other bin/libraries) contain
many source files, so the compiler is invoked many times(4).  This makes
manual addition or modification of this option practically impossible.

   To solve this problem, in the GNU build system, there are
conventional environment variables for the various kinds of compiler
options (or flags).  These environment variables are used in every call
to the compiler (they can be empty).  The environment variable used for
the C Pre-Processor (or CPP) is ‘CPPFLAGS’.  By giving ‘CPPFLAGS’ a
value once, you can be sure that each call to the compiler will be
affected.  See *note Known issues:: for an example of how to set this
variable at configure time.

   As described in *note Installation directory::, you can select the
top installation directory of a software using the GNU build system,
when you ‘./configure’ it.  All the separate components will be put in
their separate sub-directory under that, for example the programs,
compiled libraries and library headers will go into ‘$prefix/bin’
(replace ‘$prefix’ with a directory), ‘$prefix/lib’, and
‘$prefix/include’ respectively.  For enhanced modularity, libraries that
contain diverse collections of functions (like GSL, WCSLIB, and
Gnuastro), put their header files in a sub-directory unique to
themselves.  For example all Gnuastro’s header files are installed in
‘$prefix/include/gnuastro’.  In your source code, you need to keep the
library’s sub-directory when including the headers from such libraries,
for example ‘#include <gnuastro/fits.h>’(5).  Not all libraries need to
follow this convention, for example CFITSIO only has one header
(‘fitsio.h’) which is directly installed in ‘$prefix/include’.

   ---------- Footnotes ----------

   (1) Recall that in C, functions only have one output.

   (2) The ‘include/’ directory name is taken from the pre-processor’s
‘#include’ directive, which is also the motivation behind the ‘I’ in the
‘-I’ option to the pre-processor.

   (3) Try running Gnuastro’s ‘make’ and find the directories given to
the compiler with the ‘-I’ option.

   (4) Nearly every command you see being executed after running ‘make’
is one call to the compiler.

   (5) the top ‘$prefix/include’ directory is usually known to the
compiler


File: gnuastro.info,  Node: Linking,  Next: Summary and example on libraries,  Prev: Headers,  Up: Review of library fundamentals

10.1.2 Linking
--------------

To enhance modularity, similar functions are defined in one source file
(with a ‘.c’ suffix, see *note Headers:: for more).  After running
‘make’, each human-readable, ‘.c’ file is translated (or compiled) into
a computer-readable “object” file (ending with ‘.o’).  Note that object
files are also created when building programs, they aren’t particular to
libraries.  Try opening Gnuastro’s ‘lib/’ and ‘bin/progname/’
directories after running ‘make’ to see these object files(1).
Afterwards, the object files are _linked_ together to create an
executable program or a library.

   The object files contain the full definition of the functions in the
respective ‘.c’ file along with a list of any other function (or
generally “symbol”) that is referenced there.  To get a list of those
functions you can use the ‘nm’ program which is part of GNU Binutils.
For example from the top Gnuastro directory, run:

     $ nm bin/arithmetic/arithmetic.o

This will print a list of all the functions (more generally, ‘symbols’)
that were called within ‘bin/arithmetic/arithmetic.c’ along with some
further information (for example a ‘T’ in the second column shows that
this function is actually defined here, ‘U’ says that it is undefined
here).  Try opening the ‘.c’ file to check some of these functions for
your self.  Run ‘info nm’ for more information.

   To recap, the _compiler_ created the separate object files mentioned
above for each ‘.c’ file.  The _linker_ will then combine all the
symbols of the various object files (and libraries) into one program or
library.  In the case of Arithmetic (a program) the contents of the
object files in ‘bin/arithmetic/’ are copied (and re-ordered) into one
final executable file which we can run from the operating system.

   There are two ways to _link_ all the necessary symbols: static and
dynamic/shared.  When the symbols (computer-readable function
definitions in most cases) are copied into the output, it is called
_static_ linking.  When the symbols are kept in their original file and
only a reference to them is kept in the executable, it is called
_dynamic_, or _shared_ linking.

   Let’s have a closer look at the executable to understand this better:
we’ll assume you have built Gnuastro without any customization and
installed Gnuastro into the default ‘/usr/local/’ directory (see *note
Installation directory::).  If you tried the ‘nm’ command on one of
Arithmetic’s object files above, then with the command below you can
confirm that all the functions that were defined in the object file
above (had a ‘T’ in the second column) are also defined in the
‘astarithmetic’ executable:

     $ nm /usr/local/bin/astarithmetic

These symbols/function have been statically linked (copied) in the final
executable.  But you will notice that there are still many undefined
symbols in the executable (those with a ‘U’ in the second column).  One
class of such functions are Gnuastro’s own library functions that start
with ‘‘gal_’’:

     $ nm /usr/local/bin/astarithmetic | grep gal_

   These undefined symbols (functions) are present in another file and
will be linked to the Arithmetic program every time you run it.
Therefore they are known as dynamically _linked_ libraries (2).  As we
saw above, static linking is done when the executable is being built.
However, when a program is dynamically linked to a library, at
build-time, the library’s symbols are only checked with the available
libraries: they are not actually copied into the program’s executable.
Every time you run the program, the (dynamic) linker will be activated
and will try to link the program to the installed library before the
program starts.

   If you want all the libraries to be statically linked to the
executables, you have to tell Libtool (which Gnuastro uses for the
linking) to disable shared libraries at configure time(3):

     $ configure --disable-shared

Try configuring Gnuastro with the command above, then build and install
it (as described in *note Quick start::).  Afterwards, check the ‘gal_’
symbols in the installed Arithmetic executable like before.  You will
see that they are actually copied this time (have a ‘T’ in the second
column).  If the second column doesn’t convince you, look at the
executable file size with the following command:

     $ ls -lh /usr/local/bin/astarithmetic

It should be around 4.2 Megabytes with this static linking.  If you
configure and build Gnuastro again with shared libraries enabled (which
is the default), you will notice that it is roughly 100 Kilobytes!

   This huge difference would have been very significant in the old
days, but with the roughly Terabyte storage drives commonly in use
today, it is negligible.  Fortunately, output file size is not the only
benefit of dynamic linking: since it links to the libraries at run-time
(rather than build-time), you don’t have to re-build a higher-level
program or library when an update comes for one of the lower-level
libraries it depends on.  You just install the new low-level library and
it will automatically be used/linked next time in the programs that use
it.  To be fair, this also creates a few complications(4):

   • Reproducibility: Even though your high-level tool has the same
     version as before, with the updated library, you might not get the
     same results.
   • Broken links: if some functions have been changed or removed in the
     updated library, then the linker will abort with an error at
     run-time.  Therefore you need to re-build your higher-level program
     or library.

   To see a list of all the shared libraries that are needed for a
program or a shared library to run, you can use GNU C library’s ‘ldd’(5)
program, for example:

     $ ldd /usr/local/bin/astarithmetic

   Library file names (in their installation directory) start with a
‘lib’ and their ending (suffix) shows if they are static (‘.a’) or
dynamic (‘.so’), as described below.  The name of the library is in the
middle of these two, for example ‘libgsl.a’ or ‘libgnuastro.a’ (GSL and
Gnuastro’s static libraries), and ‘libgsl.so.23.0.0’ or
‘libgnuastro.so.4.0.0’ (GSL and Gnuastro’s shared library, the numbers
may be different).

   • A static library is known as an archive file and has the ‘.a’
     suffix.  A static library is not an executable file.

   • A shared library ends with the ‘.so.X.Y.Z’ suffix and is
     executable.  The three numbers in the suffix, describe the version
     of the shared library.  Shared library versions are defined to
     allow multiple versions of a shared library simultaneously on a
     system and to help detect possible updates in the library and
     programs that depend on it by the linker.

     It is very important to mention that this version number is
     different from the software version number (see *note Version
     numbering::), so do not confuse the two.  See the “Library
     interface versions” chapter of GNU Libtool for more.

     For each shared library, we also have two symbolic links ending
     with ‘.so.X’ and ‘.so’.  They are automatically set by the
     installer, but you can change them (point them to another version
     of the library) when you have multiple versions of a library on
     your system.

   Libraries that are built with GNU Libtool (including Gnuastro and its
dependencies), build both static and dynamic libraries by default and
install them in ‘prefix/lib/’ directory (for more on ‘prefix’, see *note
Installation directory::).  In this way, programs depending on the
libraries can link with them however they prefer.  See the contents of
‘/usr/local/lib’ with the command below to see both the static and
shared libraries available there, along with their executable nature and
the symbolic links:

     $ ls -l /usr/local/lib/

   To link with a library, the linker needs to know where to find the
library.  _At compilation time_, these locations can be passed to the
linker with two separate options (see *note Summary and example on
libraries:: for an example) as described below.  You can see these
options and their usage in practice while building Gnuastro (after
running ‘make’):

‘-L DIR’
     Will tell the linker to look into ‘DIR’ for the libraries.  For
     example ‘-L/usr/local/lib’, or ‘-L/home/yourname/.local/lib’.  You
     can make multiple calls to this option, so the linker looks into
     several directories at compilation time.  Note that the space
     between <L> and the directory is optional and commonly ignored
     (written as ‘-LDIR’).

‘-lLIBRARY’
     Specify the unique library identifier/name (not containing
     directory or shared/dynamic nature) to be linked with the
     executable.  As discussed above, library file names have fixed
     parts which must not be given to this option.  So ‘-lgsl’ will
     guide the linker to either look for ‘libgsl.a’ or ‘libgsl.so’
     (depending on the type of linking it is suppose to do).  You can
     link many libraries by repeated calls to this option.

     *Very important: * The place of this option on the compiler’s
     command matters.  This is often a source of confusion for
     beginners, so let’s assume you have asked the linker to link with
     library A using this option.  As soon as the linker confronts this
     option, it looks into the list of the undefined symbols it has
     found until that point and does a search in library A for any of
     those symbols.  If any pending undefined symbol is found in library
     A, it is used.  After the search in undefined symbols is complete,
     the contents of library A are completely discarded from the
     linker’s memory.  Therefore, if a later object file or library uses
     an unlinked symbol in library A, the linker will abort after it has
     finished its search in all the input libraries or object files.

     As an example, Gnuastro’s ‘gal_fits_img_read’ function depends on
     the ‘fits_read_pix’ function of CFITSIO (specified with
     ‘-lcfitsio’, which in turn depends on the cURL library, called with
     ‘-lcurl’).  So the proper way to link something that uses this
     function is ‘-lgnuastro -lcfitsio -lcurl’.  If instead, you give:
     ‘-lcfitsio -lgnuastro’ the linker will complain and abort.  To
     avoid such linking complexities when using Gnuastro’s library, we
     recommend using *note BuildProgram::.

   If you have compiled and linked your program with a dynamic library,
then the dynamic linker also needs to know the location of the libraries
after building the program: _every time_ the program is run afterwards.
Therefore, it may happen that you don’t get any errors when
compiling/linking a program, but are unable to run your program because
of a failure to find a library.  This happens because the dynamic linker
hasn’t found the dynamic library _at run time_.

   To find the dynamic libraries at run-time, the linker looks into the
paths, or directories, in the ‘LD_LIBRARY_PATH’ environment variable.
For a discussion on environment variables, especially search paths like
‘LD_LIBRARY_PATH’, and how you can add new directories to them, see
*note Installation directory::.

   ---------- Footnotes ----------

   (1) Gnuastro uses GNU Libtool for portable library creation.  Libtool
will also make a ‘.lo’ file for each ‘.c’ file when building libraries
(‘.lo’ files are human-readable).

   (2) Do not confuse dynamically _linked_ libraries with dynamically
_loaded_ libraries.  The former (that is discussed here) are only loaded
once at the program startup.  However, the latter can be loaded anytime
during the program’s execution, they are also known as plugins.

   (3) Libtool is very common and is commonly used.  Therefore, you can
use this option to configure on most programs using the GNU build system
if you want static linking.

   (4) Both of these can be avoided by joining the mailing lists of the
lower-level libraries and checking the changes in newer versions before
installing them.  Updates that result in such behaviors are generally
heavily emphasized in the release notes.

   (5) If your operating system is not using the GNU C library, you
might need another tool.


File: gnuastro.info,  Node: Summary and example on libraries,  Prev: Linking,  Up: Review of library fundamentals

10.1.3 Summary and example on libraries
---------------------------------------

After the mostly abstract discussions of *note Headers:: and *note
Linking::, we’ll give a small tutorial here.  But before that, let’s
recall the general steps of how your source code is prepared, compiled
and linked to the libraries it depends on so you can run it:

  1. The *pre-processor* includes the header (‘.h’) files into the
     function definition (‘.c’) files, expands pre-processor macros.
     Generally the pre-processor prepares the human-readable source for
     compilation (reviewed in *note Headers::).

  2. The *compiler* will translate (compile) the human-readable contents
     of each source (merged ‘.c’ and the ‘.h’ files, or generally the
     output of the pre-processor) into the computer-readable code of
     ‘.o’ files.

  3. The *linker* will link the called function definitions from various
     compiled files to create one unified object.  When the unified
     product has a ‘main’ function, this function is the product’s only
     entry point, enabling the operating system or user to directly
     interact with it, so the product is a program.  When the product
     doesn’t have a ‘main’ function, the linker’s product is a library
     and its exported functions can be linked to other executables (it
     has many entry points).

   The GNU Compiler Collection (or GCC for short) will do all three
steps.  So as a first example, from Gnuastro’s source, go to
‘tests/lib/’.  This directory contains the library tests, you can use
these as some simple tutorials.  For this demonstration, we will compile
and run the ‘arraymanip.c’.  This small program will call Gnuastro
library for some simple operations on an array (open it and have a
look).  To compile this program, run this command inside the directory
containing it.

     $ gcc arraymanip.c -lgnuastro -lm -o arraymanip

The two ‘-lgnuastro’ and ‘-lm’ options (in this order) tell GCC to first
link with the Gnuastro library and then with C’s math library.  The ‘-o’
option is used to specify the name of the output executable, without it
the output file name will be ‘a.out’ (on most OSs), independent of your
input file name(s).

   If your top Gnuastro installation directory (let’s call it ‘$prefix’,
see *note Installation directory::) is not recognized by GCC, you will
get pre-processor errors for unknown header files.  Once you fix it, you
will get linker errors for undefined functions.  To fix both, you should
run GCC as follows: additionally telling it which directories it can
find Gnuastro’s headers and compiled library (see *note Headers:: and
*note Linking::):

     $ gcc -I$prefix/include -L$prefix/lib arraymanip.c -lgnuastro -lm     \
           -o arraymanip

This single command has done all the pre-processor, compilation and
linker operations.  Therefore no intermediate files (object files in
particular) were created, only a single output executable was created.
You are now ready to run the program with:

     $ ./arraymanip

   The Gnuastro functions called by this program only needed to be
linked with the C math library.  But if your program needs WCS
coordinate transformations, needs to read a FITS file, needs special
math operations (which include its linear algebra operations), or you
want it to run on multiple CPU threads, you also need to add these
libraries in the call to GCC: ‘-lgnuastro -lwcs -lcfitsio -lgsl
-lgslcblas -pthread -lm’.  In *note Gnuastro library::, where each
function is documented, it is mentioned which libraries (if any) must
also be linked when you call a function.  If you feel all these linkings
can be confusing, please consider Gnuastro’s *note BuildProgram::
program.


File: gnuastro.info,  Node: BuildProgram,  Next: Gnuastro library,  Prev: Review of library fundamentals,  Up: Library

10.2 BuildProgram
=================

The number and order of libraries that are necessary for linking a
program with Gnuastro library might be too confusing when you need to
compile a small program for one particular job (with one source file).
BuildProgram will use the information gathered during configuring
Gnuastro and link with all the appropriate libraries on your system.
This will allow you to easily compile, link and run programs that use
Gnuastro’s library with one simple command and not worry about which
libraries to link to, or the linking order.

   BuildProgram uses GNU Libtool to find the necessary libraries to link
against (GNU Libtool is the same program that builds all of Gnuastro’s
libraries and programs when you run ‘make’).  So in the future, if
Gnuastro’s prerequisite libraries change or other libraries are added,
you don’t have to worry, you can just run BuildProgram and internal
linking will be done correctly.

*BuildProgram requires GNU Libtool:* BuildProgram depends on GNU
Libtool, other implementations don’t have some necessary features.  If
GNU Libtool isn’t available at Gnuastro’s configure time, you will get a
notice at the end of the configuration step and BuildProgram will not be
built or installed.  Please see *note Optional dependencies:: for more
information.

* Menu:

* Invoking astbuildprog::       Options and examples for using this program.


File: gnuastro.info,  Node: Invoking astbuildprog,  Prev: BuildProgram,  Up: BuildProgram

10.2.1 Invoking BuildProgram
----------------------------

BuildProgram will compile and link a C source program with Gnuastro’s
library and all its dependencies, greatly facilitating the compilation
and running of small programs that use Gnuastro’s library.  The
executable name is ‘astbuildprog’ with the following general template:

     $ astbuildprog [OPTION...] C_SOURCE_FILE

One line examples:

     ## Compile, link and run `myprogram.c':
     $ astbuildprog myprogram.c

     ## Similar to previous, but with optimization and compiler warnings:
     $ astbuildprog -Wall -O2 myprogram.c

     ## Compile and link `myprogram.c', then run it with `image.fits'
     ## as its argument:
     $ astbuildprog myprogram.c image.fits

     ## Also look in other directories for headers and linking:
     $ astbuildprog -Lother -Iother/dir myprogram.c

     ## Just build (compile and link) `myprogram.c', don't run it:
     $ astbuildprog --onlybuild myprogram.c

   If BuildProgram is to run, it needs a C programming language source
file as input.  By default it will compile and link the given source
into a final executable program and run it.  The built executable name
can be set with the optional ‘--output’ option.  When no output name is
set, BuildProgram will use Gnuastro’s *note Automatic output:: system to
remove the suffix of the input source file (usually ‘.c’) and use the
resulting name as the built program name.

   For the full list of options that BuildProgram shares with other
Gnuastro programs, see *note Common options::.  You may also use
Gnuastro’s *note Configuration files:: to specify other
libraries/headers to use for special directories and not have to type
them in every time.

   The C compiler can be chosen with the ‘--cc’ option, or environment
variables, please see the description of ‘--cc’ for more.  The two
common ‘LDFLAGS’ and ‘CPPFLAGS’ environment variables are also checked
and used in the build by default.  Note that they are placed after the
values to the corresponding options ‘--includedir’ and ‘--linkdir’.
Therefore BuildProgram’s own options take precedence.  Using environment
variables can be disabled with the ‘--noenv’ option.  Just note that
BuildProgram also keeps the important flags in these environment
variables in its configuration file.  Therefore, in many cases, even
though you may needed them to build Gnuastro, you won’t need them in
BuildProgram.

   The first argument is considered to be the C source file that must be
compiled and linked.  Any other arguments (non-option tokens on the
command-line) will be passed onto the program when BuildProgram wants to
run it.  Recall that by default BuildProgram will run the program after
building it.  This behavior can be disabled with the ‘--onlybuild’
option.

   When the ‘--quiet’ option (see *note Operating mode options::) is not
called, BuildPrograms will print the compilation and running commands.
Once your program grows and you break it up into multiple files (which
are much more easily managed with Make), you can use the linking flags
of the non-quiet output in your ‘Makefile’.

‘-c STR’
‘--cc=STR’
     C compiler to use for the compilation, if not given environment
     variables will be used as described in the next paragraph.  If the
     compiler is in your systems’s search path, you can simply give its
     name, for example ‘--cc=gcc’.  If its not in your system’s search
     path, you can give its full path, for example
     ‘--cc=/path/to/your/custom/cc’.

     If this option has no value after parsing the command-line and all
     configuration files (see *note Configuration file precedence::),
     then BuildProgram will look into the following environment
     variables in the given order ‘CC’ and ‘GCC’.  If they are also not
     defined, BuildProgram will ultimately default to the ‘gcc’ command
     which is present in many systems (sometimes as a link to other
     compilers).

‘-I STR’
‘--includedir=STR’
     Directory to search for files that you ‘#include’ in your C
     program.  Note that headers relating to Gnuastro and its
     dependencies don’t need this option.  This is only necessary if you
     want to use other headers.  It may be called multiple times and
     order matters.  This directory will be searched before those of
     Gnuastro’s build and also the system search directories.  See *note
     Headers:: for a thorough introduction.

     From the GNU C Pre-Processor manual: “Add the directory ‘STR’ to
     the list of directories to be searched for header files.
     Directories named by ‘-I’ are searched before the standard system
     include directories.  If the directory ‘STR’ is a standard system
     include directory, the option is ignored to ensure that the default
     search order for system directories and the special treatment of
     system headers are not defeated”.

‘-L STR’
‘--linkdir=STR’
     Directory to search for compiled libraries to link the program
     with.  Note that all the directories that Gnuastro was built with
     will already be used by BuildProgram (GNU Libtool).  This option is
     only necessary if your libraries are in other directories.
     Multiple calls to this option are possible and order matters.  This
     directory will be searched before those of Gnuastro’s build and
     also the system search directories.  See *note Linking:: for a
     thorough introduction.

‘-l STR’
‘--linklib=STR’
     Library to link with your program.  Note that all the libraries
     that Gnuastro was built with will already be linked by BuildProgram
     (GNU Libtool).  This option is only necessary if you want to link
     with other directories.  Multiple calls to this option are possible
     and order matters.  This library will be linked before Gnuastro’s
     library or its dependencies.  See *note Linking:: for a thorough
     introduction.

‘-O INT/STR’
‘--optimize=INT/STR’
     Compiler optimization level: 0 (for no optimization, good
     debugging), 1, 2, 3 (for the highest level of optimizations).  From
     the GNU Compiler Collection (GCC) manual: “Without any optimization
     option, the compiler’s goal is to reduce the cost of compilation
     and to make debugging produce the expected results.  Statements are
     independent: if you stop the program with a break point between
     statements, you can then assign a new value to any variable or
     change the program counter to any other statement in the function
     and get exactly the results you expect from the source code.
     Turning on optimization flags makes the compiler attempt to improve
     the performance and/or code size at the expense of compilation time
     and possibly the ability to debug the program.” Please see your
     compiler’s manual for the full list of acceptable values to this
     option.

‘-g’
‘--debug’
     Emit extra information in the compiled binary for use by a
     debugger.  When calling this option, it is best to explicitly
     disable optimization with ‘-O0’.  To combine both options you can
     run ‘-gO0’ (see *note Options:: for how short options can be merged
     into one).

‘-W STR’
‘--warning=STR’
     Print compiler warnings on command-line during compilation.
     “Warnings are diagnostic messages that report constructions that
     are not inherently erroneous but that are risky or suggest there
     may have been an error.” (from the GCC manual).  It is always
     recommended to compile your programs with warnings enabled.

     All compiler warning options that start with ‘W’ are usable by this
     option in BuildProgram also, see your compiler’s manual for the
     full list.  Some of the most common values to this option are:
     ‘pedantic’ (Warnings related to standard C) and ‘all’ (all issues
     the compiler confronts).

‘-t’
‘--tag=STR’
     The language configuration information.  Libtool can build objects
     and libraries in many languages.  In many cases, it can identify
     the language automatically, but when it doesn’t you can use this
     option to explicitly notify Libtool of the language.  The
     acceptable values are: ‘CC’ for C, ‘CXX’ for C++, ‘GCJ’ for Java,
     ‘F77’ for Fortran 77, ‘FC’ for Fortran, ‘GO’ for Go and ‘RC’ for
     Windows Resource.  Note that the Gnuastro library is not yet fully
     compatible with all these languages.

‘-b’
‘--onlybuild’
     Only build the program, don’t run it.  By default, the built
     program is immediately run afterwards.

‘-d’
‘--deletecompiled’
     Delete the compiled binary file after running it.  This option is
     only relevant when the compiled program is run after being built.
     In other words, it is only relevant when ‘--onlybuild’ is not
     called.  It can be useful when you are busy testing a program or
     just want a fast result and the actual binary/compiled file is not
     of later use.

‘-a STR’
‘--la=STR’
     Use the given ‘.la’ file (Libtool control file) instead of the one
     that was produced from Gnuastro’s configuration results.  The
     Libtool control file keeps all the necessary information for
     building and linking a program with a library built by Libtool.
     The default ‘prefix/lib/libgnuastro.la’ keeps all the information
     necessary to build a program using the Gnuastro library gathered
     during configure time (see *note Installation directory:: for
     prefix).  This option is useful when you prefer to use another
     Libtool control file.

‘-e’
‘--noenv’
     Don’t use environment variables in the build, just use the values
     given to the options.  As described above, environment variables
     like ‘CC’, ‘GCC’, ‘LDFLAGS’, ‘CPPFLAGS’ will be read by default and
     used in the build if they have been defined.


File: gnuastro.info,  Node: Gnuastro library,  Next: Library demo programs,  Prev: BuildProgram,  Up: Library

10.3 Gnuastro library
=====================

Gnuastro library’s programming constructs (function declarations,
macros, data structures, or global variables) are classified by context
into multiple header files (see *note Headers::)(1).  In this section,
the functions in each header will be discussed under a separate
sub-section, which includes the name of the header.  Assuming a function
declaration is in ‘headername.h’, you can include its declaration in
your source code with:

     # include <gnuastro/headername.h>

The names of all constructs in ‘headername.h’ are prefixed with
‘gal_headername_’ (or ‘GAL_HEADERNAME_’ for macros).  The ‘gal_’ prefix
stands for _G_NU _A_stronomy _L_ibrary.

   Gnuastro library functions are compiled into a single file which can
be linked on the command-line with the ‘-lgnuastro’ option.  See *note
Linking:: and *note Summary and example on libraries:: for an
introduction on linking and some fully working examples of the
libraries.

   Gnuastro’s library is a high-level library which depends on lower
level libraries for some operations (see *note Dependencies::).
Therefore if at least one of Gnuastro’s functions in your program use
functions from the dependencies, you will also need to link those
dependencies after linking with Gnuastro.  See *note BuildProgram:: for
a convenient way to deal with the dependencies.  BuildProgram will take
care of the libraries to link with your program (which uses the Gnuastro
library), and can even run the built program afterwards.  Therefore it
allows you to conveniently focus on your exciting science/research when
using Gnuastro’s libraries.

*Libraries are still under heavy development: * Gnuastro was initially
created to be a collection of command-line programs.  However, as the
programs and their the shared functions grew, internal (not installed)
libraries were added.  Since the 0.2 release, the libraries are
install-able.  Hence the libraries are currently under heavy development
and will significantly evolve between releases and will become more
mature and stable in due time.  It will stabilize with the removal of
this notice.  Check the ‘NEWS’ file for interface changes.  If you use
the Info version of this manual (see *note Info::), you don’t have to
worry: the documentation will correspond to your installed version.

* Menu:

* Configuration information::   General information about library config.
* Multithreaded programming::   Tools for easy multi-threaded operations.
* Library data types::          Definitions and functions for types.
* Pointers::                    Wrappers for easy working with pointers.**
* Library blank values::        Blank values and functions to deal with them.
* Library data container::      General data container in Gnuastro.
* Dimensions::                  Dealing with coordinates and dimensions.
* Linked lists::                Various types of linked lists.
* Array input output::          Reading and writing images or cubes.
* Table input output::          Reading and writing table columns.
* FITS files::                  Working with FITS data.
* File input output::           Reading and writing to various file formats.
* World Coordinate System::     Dealing with the world coordinate system.
* Arithmetic on datasets::      Arithmetic operations on a dataset.
* Tessellation library::        Functions for working on tiles.
* Bounding box::                Finding the bounding box.
* Polygons::                    Working with the vertices of a polygon.
* Qsort functions::             Helper functions for Qsort.
* Permutations::                Re-order (or permute) the values in a dataset.
* Matching::                    Matching catalogs based on position.
* Statistical operations::      Functions for basic statistics.
* Binary datasets::             Datasets that can only have values of 0 or 1.
* Labeled datasets::            Working with Segmented/labeled datasets.
* Convolution functions::       Library functions to do convolution.
* Interpolation::               Interpolate (over blank values possibly).
* Git wrappers::                Wrappers for functions in libgit2.
* Unit conversion library (units.h)::  Convert between units.
* Spectral lines library::      Functions for operating on Spectral lines.
* Cosmology library::           Cosmological calculations.

   ---------- Footnotes ----------

   (1) Within Gnuastro’s source, all installed ‘.h’ files in
‘lib/gnuastro/’ are accompanied by a ‘.c’ file in ‘/lib/’.


File: gnuastro.info,  Node: Configuration information,  Next: Multithreaded programming,  Prev: Gnuastro library,  Up: Gnuastro library

10.3.1 Configuration information (‘config.h’)
---------------------------------------------

The ‘gnuastro/config.h’ header contains information about the full
Gnuastro installation on your system.  Gnuastro developers should note
that this is the only header that is not available within Gnuastro, it
is only available to a Gnuastro library user _after_ installation.
Within Gnuastro, ‘config.h’ (which is included in every Gnuastro ‘.c’
file, see *note Coding conventions::) has more than enough information
about the overall Gnuastro installation.

 -- Macro: GAL_CONFIG_VERSION
     This macro can be used as a string literal(1) containing the
     version of Gnuastro that is being used.  See *note Version
     numbering:: for the version formats.  For example:

          printf("Gnuastro version: %s\n", GAL_CONFIG_VERSION);

     or

          char *gnuastro_version=GAL_CONFIG_VERSION;

 -- Macro: GAL_CONFIG_HAVE_LIBGIT2
     Libgit2 is an optional dependency of Gnuastro (see *note Optional
     dependencies::).  When it is installed and detected at configure
     time, this macro will have a value of ‘1’ (one).  Otherwise, it
     will have a value of ‘0’ (zero).  Gnuastro also comes with some
     wrappers to make it easier to use libgit2 (see *note Git
     wrappers::).

 -- Macro: GAL_CONFIG_HAVE_FITS_IS_REENTRANT
     This macro will have a value of 1 when the CFITSIO of the host
     system has the ‘fits_is_reentrant’ function (available from CFITSIO
     version 3.30).  This function is used to see if CFITSIO was
     configured to read a FITS file simultaneously on different threads.

 -- Macro: GAL_CONFIG_HAVE_WCSLIB_VERSION
     WCSLIB is the reference library for world coordinate system
     transformation (see *note WCSLIB:: and *note World Coordinate
     System::).  However, only more recent versions of WCSLIB also
     provide its version number.  If the WCSLIB that is installed on the
     system provides its version (through the possibly existing
     ‘wcslib_version’ function), this macro will have a value of one,
     otherwise it will have a value of zero.

 -- Macro: GAL_CONFIG_HAVE_PTHREAD_BARRIER
     The POSIX threads standard define barriers as an optional
     requirement.  Therefore, some operating systems choose to not
     include it.  As one of the ‘./configure’ step checks, Gnuastro we
     check if your system has this POSIX thread barriers.  If so, this
     macro will have a value of ‘1’, otherwise it will have a value of
     ‘0’.  see *note Implementation of pthread_barrier:: for more.

 -- Macro: GAL_CONFIG_SIZEOF_LONG
 -- Macro: GAL_CONFIG_SIZEOF_SIZE_T
     The size of (number of bytes in) the system’s ‘long’ and ‘size_t’
     types.  Their values are commonly either 4 or 8 for 32-bit and
     64-bit systems.  You can also get this value with the expression
     ‘‘sizeof size_t’’ for example without having to include this
     header.

   ---------- Footnotes ----------

   (1) <https://en.wikipedia.org/wiki/String_literal>


File: gnuastro.info,  Node: Multithreaded programming,  Next: Library data types,  Prev: Configuration information,  Up: Gnuastro library

10.3.2 Multithreaded programming (‘threads.h’)
----------------------------------------------

In recent years, newer CPUs don’t have significantly higher frequencies
any more.  However, CPUs are being manufactured with more cores,
enabling more than one operation (thread) at each instant.  This can be
very useful to speed up many aspects of processing and in particular
image processing.

   Most of the programs in Gnuastro utilize multi-threaded programming
for the CPU intensive processing steps.  This can potentially lead to a
significant decrease in the running time of a program, see *note A note
on threads::.  In terms of reading the code, you don’t need to know
anything about multi-threaded programming.  You can simply follow the
case where only one thread is to be used.  In these cases, threads are
not used and can be completely ignored.

   When the C language was defined (the K&R’s book was written), using
threads was not common, so C’s threading capabilities aren’t introduced
there.  Gnuastro uses POSIX threads for multi-threaded programming,
defined in the ‘pthread.h’ system wide header.  There are various
resources for learning to use POSIX threads.  An excellent tutorial
(https://computing.llnl.gov/tutorials/pthreads/) is provided by the
Lawrence Livermore National Laboratory, with abundant figures to better
understand the concepts, it is a very good start.  The book ‘Advanced
programming in the Unix environment’(1), by Richard Stevens and Stephen
Rago, Addison-Wesley, 2013 (Third edition) also has two chapters
explaining the POSIX thread constructs which can be very helpful.

   An alternative to POSIX threads was OpenMP, but POSIX threads are low
level, allowing much more control, while being easier to understand, see
*note Why C::.  All the situations where threads are used in Gnuastro
currently are completely independent with no need of coordination
between the threads.  Such problems are known as “embarrassingly
parallel” problems.  They are some of the simplest problems to solve
with threads and are also the ones that benefit most from them, see the
LLNL introduction(2).

   One very useful POSIX thread concept is ‘pthread_barrier’.
Unfortunately, it is only an optional feature in the POSIX standard, so
some operating systems don’t include it.  Therefore in *note
Implementation of pthread_barrier::, we introduce our own
implementation.  This is a rather technical section only necessary for
more technical readers and you can safely ignore it.  Following that, we
describe the helper functions in this header that can greatly simplify
writing a multi-threaded program, see *note Gnuastro's thread related
functions:: for more.

* Menu:

* Implementation of pthread_barrier::  Some systems don’t have pthread_barrier
* Gnuastro's thread related functions::  Functions for managing threads.

   ---------- Footnotes ----------

   (1) Don’t let the title scare you!  The two chapters on
Multi-threaded programming are very self-sufficient and don’t need any
more knowledge than K&R.

   (2) <https://computing.llnl.gov/tutorials/parallel_comp/>


File: gnuastro.info,  Node: Implementation of pthread_barrier,  Next: Gnuastro's thread related functions,  Prev: Multithreaded programming,  Up: Multithreaded programming

10.3.2.1 Implementation of ‘pthread_barrier’
............................................

One optional feature of the POSIX Threads standard is the
‘pthread_barrier’ concept.  It is a very useful high-level construct
that allows for independent threads to “wait” behind a “barrier” for the
rest after they finish.  Barriers can thus greatly simplify the code in
a multi-threaded program, so they are heavily used in Gnuastro.
However, since its an optional feature in the POSIX standard, some
operating systems don’t include it.  So to make Gnuastro portable, we
have written our own implementation of those ‘pthread_barrier’
functions.

   At ‘./configure’ time, Gnuastro will check if ‘pthread_barrier’
constructs are available on your system or not.  If ‘pthread_barrier’ is
not available, our internal implementation will be compiled into the
Gnuastro library and the definitions and declarations below will be
usable in your code with ‘#include <gnuastro/threads.h>’.

 -- Type: pthread_barrierattr_t
     Type to specify the attributes of a POSIX threads barrier.

 -- Type: pthread_barrier_t
     Structure defining the POSIX threads barrier.

 -- Function:
          int
          pthread_barrier_init (pthread_barrier_t ‘*b’,
          pthread_barrierattr_t ‘*attr’, unsigned int ‘limit’)
     Initialize the barrier ‘b’, with the attributes ‘attr’ and total
     ‘limit’ (a number of) threads that must wait behind it.  This
     function must be called before spinning off threads.

 -- Function:
          int
          pthread_barrier_wait (pthread_barrier_t ‘*b’)
     This function is called within each thread, just before it is ready
     to return.  Once a thread’s function hits this, it will “wait”
     until all the other functions are also finished.

 -- Function:
          int
          pthread_barrier_destroy (pthread_barrier_t ‘*b’)
     Destroy all the information in the barrier structure.  This should
     be called by the function that spun-off the threads after all the
     threads have finished.

     *Destroy a barrier before re-using it:* It is very important to
     destroy the barrier before (possibly) reusing it.  This destroy
     function not only destroys the internal structures, it also waits
     (in 1 microsecond intervals, so you will not notice!)  until all
     the threads don’t need the barrier structure any more.  If you
     immediately start spinning off new threads with a not-destroyed
     barrier, then the internal structure of the remaining threads will
     get mixed with the new ones and you will get very strange and
     apparently random errors that are extremely hard to debug.

